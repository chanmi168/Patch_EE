{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: work on pred_dann so it adapts to the new architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "from math import sin\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from plotting_tools import *\n",
    "from handy_tools import *\n",
    "from setting import *\n",
    "from evaluate import *\n",
    "\n",
    "from stage3_preprocess import *\n",
    "from stage4_regression import *\n",
    "from dataIO import *\n",
    "from FL_extension.training_util import *\n",
    "from FL_extension.dataset_util import *\n",
    "from FL_extension.evaluation_util import *\n",
    "from FL_extension.models import *\n",
    "from FL_extension.models_CNNlight import *\n",
    "# from unet_extension.training_util import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../data/stage3/win60_overlap90/', output_folder='../../data/stage4_FL/', subject_id='101', training_params_file='training_params_computation.json', use_denoise_scg='True')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='SpO2_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--subject_id', metavar='subject_id', help='subject_id',\n",
    "                    default='101')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "parser.add_argument('--use_denoise_scg', metavar='use_denoise_scg', help='use_denoise_scg',\n",
    "                    default='true')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "# args = parser.parse_args(['--input_folder', '../../data/stage3_FL/win8_overlap75', \n",
    "#                           '--output_folder', '../../data/stage4_FL/',\n",
    "# #                           '--training_params_file', 'training_params_baseline.json',\n",
    "#                           '--training_params_file', 'training_params_dummy.json',\n",
    "#                          ])\n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3/win60_overlap90/', \n",
    "                          '--output_folder', '../../data/stage4_FL/',\n",
    "#                           '--training_params_file', 'training_params_baseline.json',\n",
    "#                           '--training_params_file', 'training_params_dummy.json',\n",
    "#                           '--training_params_file', 'training_params_STFT.json',\n",
    "                          '--training_params_file', 'training_params_computation.json',\n",
    "#                           '--training_params_file', 'training_params_RespiratoryRegression.json',\n",
    "                          '--use_denoise_scg', 'True',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 1==1 :\n",
    "#     print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2023-May-17 02:40:04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_start = datetime.now(tz_NY)\n",
    "print(\"start time:\", datetime_start.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "use_denoise_scg = eval(args.use_denoise_scg)\n",
    "\n",
    "outputdir = outputdir + '{}/'.format(training_params_file.split('.json')[0].split('_')[-1])\n",
    "\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training params and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_names(training_params):\n",
    "    model_out_names = []\n",
    "\n",
    "#     for output_name in training_params['output_names']:\n",
    "    for output_name in training_params['output_names']+['domain']:\n",
    "        for input_name in training_params['input_names']:\n",
    "            model_out_names.append(output_name+'-{}'.format(input_name))\n",
    "    return model_out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modality_dict(training_params):\n",
    "    label_encoder = LabelEncoder()\n",
    "    modality_encoded = label_encoder.fit_transform(training_params['input_names'])\n",
    "#     modality_encoded # array([0, 1])\n",
    "\n",
    "    modality_dict = {}\n",
    "    for i_modality in modality_encoded:\n",
    "        modality_dict[training_params['input_names'][i_modality]] = i_modality\n",
    "#     training_params['modality_dict'] = modality_dict\n",
    "    return modality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_params_computation.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions are: (417, 1, 6000)\n",
      "# of windows: 438\n"
     ]
    }
   ],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in [training_params_list[0]]:\n",
    "    \n",
    "    # 1. store device info\n",
    "    # include device in training_params\n",
    "    if training_params['cuda_i']==-1:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "    training_params['device'] = device\n",
    "    \n",
    "    # 2. check training mode (subject_ind [default] vs. subject_specific)\n",
    "    if 'training_mode' in training_params:\n",
    "        training_mode = training_params['training_mode']\n",
    "    else:\n",
    "        training_params = 'subject_ind'\n",
    "    \n",
    "    # 3. transfer some info from stage3_dict to training_params\n",
    "    stage3_dict = data_loader('stage3_dict', inputdir).item()\n",
    "    training_params['list_signal'] = stage3_dict['list_signal']\n",
    "    training_params['list_feature'] = stage3_dict['list_feature']\n",
    "    training_params['list_output'] = stage3_dict['list_output']\n",
    "    training_params['list_meta'] = stage3_dict['list_meta']\n",
    "    training_params['FS_RESAMPLE_DL'] = stage3_dict['FS_RESAMPLE_DL']\n",
    "    training_params['subject_ids'] = stage3_dict['subject_ids']\n",
    "    training_params['task_ids'] = stage3_dict['task_ids']\n",
    "    training_params['sequence'] = stage3_dict['sequence']\n",
    "    \n",
    "    # 4. get data loaders (TODO: load data into script once to save IO time)\n",
    "    # [change it] first change CV_config so get_loaders prepare the data correctly though\n",
    "    training_params['CV_config'] = {\n",
    "        'subject_id': 113,\n",
    "#         'task_id': 5,\n",
    "    }\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['train'].dataset.data.shape)\n",
    "    print('# of windows:', dataset_sizes['train']+dataset_sizes['val'])\n",
    "    \n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "    del dataloaders\n",
    "    \n",
    "    # 5. get sweep name (is this necessary?)\n",
    "    sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "\n",
    "\n",
    "    # 7. store the ordered_subject_ids, inputdir, and outputdir\n",
    "    training_params['ordered_subject_ids'] = np.asarray(training_params['ordered_subject_ids'])\n",
    "#       \"ordered_subject_ids\": [101, 103, 104, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120, 121, 113],\n",
    "    # [101, 103, 104, 106, 111, 115, 116, 117, 118, 119, 120, 121, 113],\n",
    "    \n",
    "    training_params['inputdir'] = inputdir\n",
    "    training_params['outputdir'] = outputdir\n",
    "\n",
    "\n",
    "# training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_scg(sig, Fs):\n",
    "\n",
    "    sig_analytic = hilbert(sig)\n",
    "    sig_envelope = np.abs(sig_analytic)\n",
    "    sig_filt = get_padded_filt(sig_envelope, lowcutoff=0.1, highcutoff=10, Fs=Fs)\n",
    "\n",
    "    return sig_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on subject 107\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 121\n",
      "data dimensions are: (20, 1, 6000)\n",
      "dataset_sizes:  {'train': 418, 'val': 20}\n",
      "working on subject 117\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 101\n",
      "data dimensions are: (20, 1, 6000)\n",
      "dataset_sizes:  {'train': 418, 'val': 20}\n",
      "working on subject 102\n",
      "data dimensions are: (22, 1, 6000)\n",
      "dataset_sizes:  {'train': 416, 'val': 22}\n",
      "working on subject 103\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 104\n",
      "data dimensions are: (20, 1, 6000)\n",
      "dataset_sizes:  {'train': 418, 'val': 20}\n",
      "working on subject 105\n",
      "data dimensions are: (20, 1, 6000)\n",
      "dataset_sizes:  {'train': 418, 'val': 20}\n",
      "working on subject 106\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 108\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 110\n",
      "data dimensions are: (20, 1, 6000)\n",
      "dataset_sizes:  {'train': 418, 'val': 20}\n",
      "working on subject 111\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 113\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 114\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 115\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 116\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 118\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 119\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n",
      "working on subject 120\n",
      "data dimensions are: (21, 1, 6000)\n",
      "dataset_sizes:  {'train': 417, 'val': 21}\n"
     ]
    }
   ],
   "source": [
    "df_outputlabel_val = pd.DataFrame()\n",
    "df_performance_val = pd.DataFrame()\n",
    "\n",
    "for i_CV, subject_id in enumerate(training_params['ordered_subject_ids']):\n",
    "    print('working on subject', subject_id)\n",
    "    if 'CV_max' in training_params:\n",
    "        if i_CV >= training_params['CV_max']:\n",
    "            continue\n",
    "\n",
    "    training_params['CV_config']['subject_id'] = subject_id\n",
    "\n",
    "    # need to load the data for each LOSO CV\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['val'].dataset.data.shape)\n",
    "    print('dataset_sizes: ', dataset_sizes)\n",
    "\n",
    "    # update the dimension so the model is created correctly\n",
    "    data_dimensions = dataloaders['val'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "\n",
    "\n",
    "    for j in range(dataloaders['val'].dataset.__len__()):\n",
    "                   \n",
    "        HR_label = dataloaders['val'].dataset.label[j,0]\n",
    "        sig = dataloaders['val'].dataset.data[j,0,:]\n",
    "        \n",
    "        if use_denoise_scg:\n",
    "            sig = denoise_scg(sig, Fs=training_params['FS_RESAMPLE_DL'])\n",
    "\n",
    "\n",
    "        xf, yf = get_psd(sig, Fs=training_params['FS_RESAMPLE_DL'])\n",
    "\n",
    "        xf = xf * 60\n",
    "        mask = (xf>=label_range_dict['HR_DL'][0]) & (xf<=label_range_dict['HR_DL'][1])\n",
    "        xf = xf[mask]\n",
    "        yf = yf[mask]\n",
    "\n",
    "        i_max = np.argmax(yf)\n",
    "        HR_est = xf[i_max]\n",
    "        HR_est, HR_label\n",
    "                   \n",
    "        df = pd.DataFrame( \n",
    "            {\n",
    "                'label_est': HR_est,\n",
    "                'label': HR_label,\n",
    "                'CV': subject_id,\n",
    "                'task': 'HR_estimation',\n",
    "                'activity': np.vectorize(tasks_dict_reversed.get)(dataloaders['train'].dataset.meta[i,1])\n",
    "            }, index=[0] )\n",
    "                   \n",
    "        df_outputlabel_val = df_outputlabel_val.append(df)\n",
    "\n",
    "    df_outputlabel_sub = df_outputlabel_val[df_outputlabel_val['CV']==subject_id]\n",
    "    df_performance_val = df_performance_val.append( get_df_performance(df_outputlabel_sub['label'].values, df_outputlabel_sub['label_est'].values, subject_id, 'HR_estimation'), ignore_index=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.260116337254515, 6.162864422844996)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance_val['mae'].mean(), df_performance_val['mae'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.834240727557825, 9.077831237986317)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance_val['rmse'].mean(), df_performance_val['rmse'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(5,7), dpi=150)\n",
    "\n",
    "\n",
    "j = 5\n",
    "\n",
    "sig = dataloaders['val'].dataset.data[j, 0, :]\n",
    "HR_label = dataloaders['val'].dataset.label[j,0]\n",
    "xf, yf = get_psd(sig, Fs=training_params['FS_RESAMPLE_DL'])\n",
    "axes[0].plot(xf, yf)\n",
    "\n",
    "\n",
    "def denoise_scg(sig, Fs):\n",
    "\n",
    "    sig_analytic = hilbert(sig)\n",
    "    sig_envelope = np.abs(sig_analytic)\n",
    "    sig_filt = get_padded_filt(sig_envelope, lowcutoff=0.1, highcutoff=None, Fs=Fs)\n",
    "\n",
    "    return sig_filt\n",
    "\n",
    "\n",
    "\n",
    "# sig_analytic = hilbert(sig)\n",
    "# sig_envelope = np.abs(sig_analytic)\n",
    "# plt.plot(sig)\n",
    "\n",
    "sig_filt = denoise_scg(sig, Fs=training_params['FS_RESAMPLE_DL'])\n",
    "xf, yf = get_psd(sig_filt, Fs=training_params['FS_RESAMPLE_DL'])\n",
    "yf = get_smooth(yf, N=7)\n",
    "\n",
    "axes[1].plot(xf, yf, color='steelblue', linewidth=2)\n",
    "\n",
    "mask = (xf>=1.3) & (xf<=1.7)\n",
    "axes[1].fill_between(xf[mask], yf[mask], alpha=0.8, color='steelblue')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0,5)\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "    ax.set_xlabel('freq (Hz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(sig_envelope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xf = xf * 60\n",
    "# mask = (xf>=label_range_dict['HR_DL'][0]) & (xf<=label_range_dict['HR_DL'][1])\n",
    "# xf = xf[mask]\n",
    "# yf = yf[mask]\n",
    "\n",
    "i_max = np.argmax(yf)\n",
    "HR_est = xf[i_max]\n",
    "HR_est, HR_label\n",
    "\n",
    "\n",
    "# axes[1].plot(amplitude_envelope)\n",
    "# axes[2].plot(signal_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(3,1,figsize=(30,3), dpi=100)\n",
    "# axes[0].plot(signal)\n",
    "# axes[1].plot(amplitude_envelope)\n",
    "# axes[2].plot(signal_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " get_df_performance(df['label'].values, df['label_est'].values, subject_id, 'HR_estimation'), ignore_index=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = df_outputlabel_val['label_est']-df_outputlabel_val['label']\n",
    "np.mean(np.abs(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_outputlabel_val['label_est'].values)\n",
    "plt.plot(df_outputlabel_val['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_df_performance(df_outputlabel_val['label'], df_outputlabel_val['label_est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outputlabel_val['label_est'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['model_out_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['regressor_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "mienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
