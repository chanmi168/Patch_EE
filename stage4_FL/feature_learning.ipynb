{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: work on pred_dann so it adapts to the new architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "from math import sin\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from plotting_tools import *\n",
    "from handy_tools import *\n",
    "from setting import *\n",
    "from evaluate import *\n",
    "\n",
    "from stage3_preprocess import *\n",
    "from stage4_regression import *\n",
    "from dataIO import *\n",
    "from FL_extension.training_util import *\n",
    "from FL_extension.dataset_util import *\n",
    "from FL_extension.evaluation_util import *\n",
    "from FL_extension.models import *\n",
    "from FL_extension.models_CNNlight import *\n",
    "# from unet_extension.training_util import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../data/stage3/win60_overlap90/', output_folder='../../data/stage4_FL/', subject_id='101', training_params_file='training_params_shortWin.json')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='SpO2_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--subject_id', metavar='subject_id', help='subject_id',\n",
    "                    default='101')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "# args = parser.parse_args(['--input_folder', '../../data/stage3_FL/win8_overlap75', \n",
    "#                           '--output_folder', '../../data/stage4_FL/',\n",
    "# #                           '--training_params_file', 'training_params_baseline.json',\n",
    "#                           '--training_params_file', 'training_params_dummy.json',\n",
    "#                          ])\n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3/win60_overlap90/', \n",
    "                          '--output_folder', '../../data/stage4_FL/',\n",
    "#                           '--training_params_file', 'training_params_baseline.json',\n",
    "#                           '--training_params_file', 'training_params_dummy.json',\n",
    "#                           '--training_params_file', 'training_params_STFT.json',\n",
    "                          # '--training_params_file', 'training_params_computation.json',\n",
    "                          '--training_params_file', 'training_params_shortWin.json',\n",
    "#                           '--training_params_file', 'training_params_RespiratoryRegression.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 1==1 :\n",
    "#     print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2023-May-16 14:23:23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_start = datetime.now(tz_NY)\n",
    "print(\"start time:\", datetime_start.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "outputdir = outputdir + '{}/'.format(training_params_file.split('.json')[0].split('_')[-1])\n",
    "\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training params and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_names(training_params):\n",
    "    model_out_names = []\n",
    "\n",
    "#     for output_name in training_params['output_names']:\n",
    "    for output_name in training_params['output_names']+['domain']:\n",
    "        for input_name in training_params['input_names']:\n",
    "            model_out_names.append(output_name+'-{}'.format(input_name))\n",
    "    return model_out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modality_dict(training_params):\n",
    "    label_encoder = LabelEncoder()\n",
    "    modality_encoded = label_encoder.fit_transform(training_params['input_names'])\n",
    "#     modality_encoded # array([0, 1])\n",
    "\n",
    "    modality_dict = {}\n",
    "    for i_modality in modality_encoded:\n",
    "        modality_dict[training_params['input_names'][i_modality]] = i_modality\n",
    "#     training_params['modality_dict'] = modality_dict\n",
    "    return modality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor_names(training_params):\n",
    "    training_params['regressor_names'] = []\n",
    "    main_task_name = training_params['output_names'][0]\n",
    "    \n",
    "    for output_name in training_params['output_names']:\n",
    "        if output_name == main_task_name:\n",
    "            training_params['regressor_names'].append(output_name)\n",
    "        else:\n",
    "            for input_name in training_params['input_names']:\n",
    "                training_params['regressor_names'].append(output_name + '-' + input_name)\n",
    "                \n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions are: (417, 2, 6000)\n",
      "# of windows: 438\n"
     ]
    }
   ],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in [training_params_list[0]]:\n",
    "    \n",
    "    # 1. store device info\n",
    "    # include device in training_params\n",
    "    if training_params['cuda_i']==-1:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "    training_params['device'] = device\n",
    "    \n",
    "    # 2. check training mode (subject_ind [default] vs. subject_specific)\n",
    "    if 'training_mode' in training_params:\n",
    "        training_mode = training_params['training_mode']\n",
    "    else:\n",
    "        training_params = 'subject_ind'\n",
    "    \n",
    "    # 3. transfer some info from stage3_dict to training_params\n",
    "    stage3_dict = data_loader('stage3_dict', inputdir).item()\n",
    "    training_params['list_signal'] = stage3_dict['list_signal']\n",
    "    training_params['list_feature'] = stage3_dict['list_feature']\n",
    "    training_params['list_output'] = stage3_dict['list_output']\n",
    "    training_params['list_meta'] = stage3_dict['list_meta']\n",
    "    training_params['FS_RESAMPLE_DL'] = stage3_dict['FS_RESAMPLE_DL']\n",
    "    training_params['subject_ids'] = stage3_dict['subject_ids']\n",
    "    training_params['task_ids'] = stage3_dict['task_ids']\n",
    "    training_params['sequence'] = stage3_dict['sequence']\n",
    "    \n",
    "    # 4. get data loaders (TODO: load data into script once to save IO time)\n",
    "    # [change it] first change CV_config so get_loaders prepare the data correctly though\n",
    "    training_params['CV_config'] = {\n",
    "        'subject_id': 113,\n",
    "#         'task_id': 5,\n",
    "    }\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['train'].dataset.data.shape)\n",
    "    print('# of windows:', dataset_sizes['train']+dataset_sizes['val'])\n",
    "    \n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "    del dataloaders\n",
    "    \n",
    "    # 5. get sweep name (is this necessary?)\n",
    "    sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "    # 6. get the right feature extractor function\n",
    "    if training_params['model_name'] == 'FeatureExtractor_CNN':\n",
    "        training_params['feature_extractor'] = FeatureExtractor_CNN\n",
    "    elif training_params['model_name'] == 'ResNet1D':\n",
    "        training_params['feature_extractor'] = ResNet1D\n",
    "    elif training_params['model_name'] == 'FeatureExtractor_CNN2':\n",
    "        training_params['feature_extractor'] = FeatureExtractor_CNN2\n",
    "    elif 'CNNlight' in training_params['model_name']: # designed for HR estimation\n",
    "        training_params['feature_extractor'] = FeatureExtractor_CNNlight\n",
    "        \n",
    "\n",
    "    # 6. get the right feature extractor function\n",
    "    if training_params['regressor_type'] == 'DominantFreqRegression':\n",
    "        training_params['regressor'] = DominantFreqRegression\n",
    "    elif training_params['regressor_type'] == 'FFTRegression':\n",
    "        training_params['regressor'] = FFTRegression\n",
    "    elif training_params['regressor_type'] == 'RespiratoryRegression':\n",
    "        training_params['regressor'] = RespiratoryRegression\n",
    "\n",
    "    # 7. store the ordered_subject_ids, inputdir, and outputdir\n",
    "    training_params['ordered_subject_ids'] = np.asarray(training_params['ordered_subject_ids'])\n",
    "#       \"ordered_subject_ids\": [101, 103, 104, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120, 121, 113],\n",
    "    # [101, 103, 104, 106, 111, 115, 116, 117, 118, 119, 120, 121, 113],\n",
    "    \n",
    "    training_params['inputdir'] = inputdir\n",
    "    training_params['outputdir'] = outputdir\n",
    "    \n",
    "    # 8. get regressor_names ('domain-ECG_filt', 'HR_patch-ppg_ir_2_cardiac', etc.)\n",
    "    # [change it]\n",
    "    training_params = get_regressor_names(training_params)\n",
    "\n",
    "    # 9. compute output_dim if it's LSTM\n",
    "    # [change it]\n",
    "    if 'LSTM' in training_params['model_name']:\n",
    "        training_params = change_output_dim(training_params)\n",
    "\n",
    "    # 10. get model output data keys ('domain-ECG_filt', 'HR_patch-ppg_ir_2_cardiac', etc., which tells me what data is used for computing the output)\n",
    "    # [change it]\n",
    "    training_params['model_out_names'] = get_model_out_names(training_params)\n",
    "    \n",
    "    # 11. encoder the modality to integers (the first is 0, the second is 1, etc.)\n",
    "    # store the code in modality_dict\n",
    "    training_params['modality_dict'] = get_modality_dict(training_params)\n",
    "\n",
    "    # 12. get freq meta (what is the frequency for each spectral feature, mask it used label_range_dict['HR_DL'])\n",
    "    training_params = update_freq_meta(training_params)\n",
    "\n",
    "# training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params['xf_masked'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['model_out_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['regressor_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir = outputdir+training_params['model_name']+'/'\n",
    "# if not os.path.exists(outputdir):\n",
    "#     os.makedirs(outputdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define trainer, evaler, preder functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_dann\n",
    "evaler = eval_dann\n",
    "preder = pred_dann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some random param\n",
    "## kernel_size = 5 ~ 16*100/300 (16 for 300 Hz sampling rate, 5 for 100 sampling rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['model_name'] == 'ResNet1D':\n",
    "# reference: https://github.com/hsd1503/resnet1d/blob/master/test_physionet.py\n",
    "    training_params['base_filters'] = training_params['channel_n'] # [64] \n",
    "    training_params['in_channels'] = training_params['data_dimensions']\n",
    "#     training_params['increasefilter_gap'] = [training_params['downsample_gap'][0] * 2]\n",
    "    training_params['n_block'] = training_params['n_block_macro'] * training_params['downsample_gap']\n",
    "    training_params['increasefilter_gap'] = training_params['downsample_gap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['data_dimensions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_model_dann\n",
      "using model  CNNlight-DANN\n",
      "feature_out_dim   : 110\n",
      "feature_out_dim   : 110\n",
      "resp_DANN(\n",
      "  (feature_extractors): ModuleDict(\n",
      "    (ECG_filt): FeatureExtractor_CNNlight(\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (ch_pooling): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (scgZ): FeatureExtractor_CNNlight(\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(1, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "            (3): Conv1d(8, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (4): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (5): ReLU()\n",
      "            (6): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (ch_pooling): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (regressors): ModuleDict(\n",
      "    (HR_patch): DominantFreqRegression()\n",
      "  )\n",
      "  (domain_classifier): DomainClassifier(\n",
      "    (fc1): Linear(in_features=110, out_features=50, bias=True)\n",
      "    (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 8, 6000]              48\n",
      "       BatchNorm1d-2              [-1, 8, 6000]              16\n",
      "              ReLU-3              [-1, 8, 6000]               0\n",
      "            Conv1d-4              [-1, 8, 6000]             328\n",
      "       BatchNorm1d-5              [-1, 8, 6000]              16\n",
      "              ReLU-6              [-1, 8, 6000]               0\n",
      "         AvgPool1d-7              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-8              [-1, 8, 3000]               0\n",
      "            Conv1d-9              [-1, 8, 6000]              80\n",
      "      BatchNorm1d-10              [-1, 8, 6000]              16\n",
      "             ReLU-11              [-1, 8, 6000]               0\n",
      "           Conv1d-12              [-1, 8, 6000]             584\n",
      "      BatchNorm1d-13              [-1, 8, 6000]              16\n",
      "             ReLU-14              [-1, 8, 6000]               0\n",
      "        AvgPool1d-15              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-16              [-1, 8, 3000]               0\n",
      "           Conv1d-17              [-1, 8, 6000]             112\n",
      "      BatchNorm1d-18              [-1, 8, 6000]              16\n",
      "             ReLU-19              [-1, 8, 6000]               0\n",
      "           Conv1d-20              [-1, 8, 6000]             840\n",
      "      BatchNorm1d-21              [-1, 8, 6000]              16\n",
      "             ReLU-22              [-1, 8, 6000]               0\n",
      "        AvgPool1d-23              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-24              [-1, 8, 3000]               0\n",
      "           Conv1d-25              [-1, 8, 3000]             200\n",
      "   InceptionBlock-26              [-1, 8, 3000]               0\n",
      "           Conv1d-27              [-1, 8, 3000]             328\n",
      "      BatchNorm1d-28              [-1, 8, 3000]              16\n",
      "             ReLU-29              [-1, 8, 3000]               0\n",
      "           Conv1d-30              [-1, 8, 3000]             328\n",
      "      BatchNorm1d-31              [-1, 8, 3000]              16\n",
      "             ReLU-32              [-1, 8, 3000]               0\n",
      "        AvgPool1d-33              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-34              [-1, 8, 1500]               0\n",
      "           Conv1d-35              [-1, 8, 3000]             584\n",
      "      BatchNorm1d-36              [-1, 8, 3000]              16\n",
      "             ReLU-37              [-1, 8, 3000]               0\n",
      "           Conv1d-38              [-1, 8, 3000]             584\n",
      "      BatchNorm1d-39              [-1, 8, 3000]              16\n",
      "             ReLU-40              [-1, 8, 3000]               0\n",
      "        AvgPool1d-41              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-42              [-1, 8, 1500]               0\n",
      "           Conv1d-43              [-1, 8, 3000]             840\n",
      "      BatchNorm1d-44              [-1, 8, 3000]              16\n",
      "             ReLU-45              [-1, 8, 3000]               0\n",
      "           Conv1d-46              [-1, 8, 3000]             840\n",
      "      BatchNorm1d-47              [-1, 8, 3000]              16\n",
      "             ReLU-48              [-1, 8, 3000]               0\n",
      "        AvgPool1d-49              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-50              [-1, 8, 1500]               0\n",
      "           Conv1d-51              [-1, 8, 1500]             200\n",
      "   InceptionBlock-52              [-1, 8, 1500]               0\n",
      "           Conv1d-53              [-1, 8, 1500]             328\n",
      "      BatchNorm1d-54              [-1, 8, 1500]              16\n",
      "             ReLU-55              [-1, 8, 1500]               0\n",
      "           Conv1d-56              [-1, 8, 1500]             328\n",
      "      BatchNorm1d-57              [-1, 8, 1500]              16\n",
      "             ReLU-58              [-1, 8, 1500]               0\n",
      "        AvgPool1d-59               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-60               [-1, 8, 750]               0\n",
      "           Conv1d-61              [-1, 8, 1500]             584\n",
      "      BatchNorm1d-62              [-1, 8, 1500]              16\n",
      "             ReLU-63              [-1, 8, 1500]               0\n",
      "           Conv1d-64              [-1, 8, 1500]             584\n",
      "      BatchNorm1d-65              [-1, 8, 1500]              16\n",
      "             ReLU-66              [-1, 8, 1500]               0\n",
      "        AvgPool1d-67               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-68               [-1, 8, 750]               0\n",
      "           Conv1d-69              [-1, 8, 1500]             840\n",
      "      BatchNorm1d-70              [-1, 8, 1500]              16\n",
      "             ReLU-71              [-1, 8, 1500]               0\n",
      "           Conv1d-72              [-1, 8, 1500]             840\n",
      "      BatchNorm1d-73              [-1, 8, 1500]              16\n",
      "             ReLU-74              [-1, 8, 1500]               0\n",
      "        AvgPool1d-75               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-76               [-1, 8, 750]               0\n",
      "           Conv1d-77               [-1, 8, 750]             200\n",
      "   InceptionBlock-78               [-1, 8, 750]               0\n",
      "           Conv1d-79               [-1, 1, 750]               9\n",
      "           Conv1d-80               [-1, 1, 750]               9\n",
      "FeatureExtractor_CNNlight-81                  [-1, 110]               0\n",
      "DominantFreqRegression-82                    [-1, 1]               0\n",
      "           Linear-83                   [-1, 50]           5,550\n",
      "          Dropout-84                   [-1, 50]               0\n",
      "             ReLU-85                   [-1, 50]               0\n",
      "           Linear-86                    [-1, 2]             102\n",
      " DomainClassifier-87                    [-1, 2]               0\n",
      "           Conv1d-88              [-1, 8, 6000]              48\n",
      "      BatchNorm1d-89              [-1, 8, 6000]              16\n",
      "             ReLU-90              [-1, 8, 6000]               0\n",
      "           Conv1d-91              [-1, 8, 6000]             328\n",
      "      BatchNorm1d-92              [-1, 8, 6000]              16\n",
      "             ReLU-93              [-1, 8, 6000]               0\n",
      "        AvgPool1d-94              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-95              [-1, 8, 3000]               0\n",
      "           Conv1d-96              [-1, 8, 6000]              80\n",
      "      BatchNorm1d-97              [-1, 8, 6000]              16\n",
      "             ReLU-98              [-1, 8, 6000]               0\n",
      "           Conv1d-99              [-1, 8, 6000]             584\n",
      "     BatchNorm1d-100              [-1, 8, 6000]              16\n",
      "            ReLU-101              [-1, 8, 6000]               0\n",
      "       AvgPool1d-102              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-103              [-1, 8, 3000]               0\n",
      "          Conv1d-104              [-1, 8, 6000]             112\n",
      "     BatchNorm1d-105              [-1, 8, 6000]              16\n",
      "            ReLU-106              [-1, 8, 6000]               0\n",
      "          Conv1d-107              [-1, 8, 6000]             840\n",
      "     BatchNorm1d-108              [-1, 8, 6000]              16\n",
      "            ReLU-109              [-1, 8, 6000]               0\n",
      "       AvgPool1d-110              [-1, 8, 3000]               0\n",
      "MyAvgPool1dPadSame-111              [-1, 8, 3000]               0\n",
      "          Conv1d-112              [-1, 8, 3000]             200\n",
      "  InceptionBlock-113              [-1, 8, 3000]               0\n",
      "          Conv1d-114              [-1, 8, 3000]             328\n",
      "     BatchNorm1d-115              [-1, 8, 3000]              16\n",
      "            ReLU-116              [-1, 8, 3000]               0\n",
      "          Conv1d-117              [-1, 8, 3000]             328\n",
      "     BatchNorm1d-118              [-1, 8, 3000]              16\n",
      "            ReLU-119              [-1, 8, 3000]               0\n",
      "       AvgPool1d-120              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-121              [-1, 8, 1500]               0\n",
      "          Conv1d-122              [-1, 8, 3000]             584\n",
      "     BatchNorm1d-123              [-1, 8, 3000]              16\n",
      "            ReLU-124              [-1, 8, 3000]               0\n",
      "          Conv1d-125              [-1, 8, 3000]             584\n",
      "     BatchNorm1d-126              [-1, 8, 3000]              16\n",
      "            ReLU-127              [-1, 8, 3000]               0\n",
      "       AvgPool1d-128              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-129              [-1, 8, 1500]               0\n",
      "          Conv1d-130              [-1, 8, 3000]             840\n",
      "     BatchNorm1d-131              [-1, 8, 3000]              16\n",
      "            ReLU-132              [-1, 8, 3000]               0\n",
      "          Conv1d-133              [-1, 8, 3000]             840\n",
      "     BatchNorm1d-134              [-1, 8, 3000]              16\n",
      "            ReLU-135              [-1, 8, 3000]               0\n",
      "       AvgPool1d-136              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-137              [-1, 8, 1500]               0\n",
      "          Conv1d-138              [-1, 8, 1500]             200\n",
      "  InceptionBlock-139              [-1, 8, 1500]               0\n",
      "          Conv1d-140              [-1, 8, 1500]             328\n",
      "     BatchNorm1d-141              [-1, 8, 1500]              16\n",
      "            ReLU-142              [-1, 8, 1500]               0\n",
      "          Conv1d-143              [-1, 8, 1500]             328\n",
      "     BatchNorm1d-144              [-1, 8, 1500]              16\n",
      "            ReLU-145              [-1, 8, 1500]               0\n",
      "       AvgPool1d-146               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-147               [-1, 8, 750]               0\n",
      "          Conv1d-148              [-1, 8, 1500]             584\n",
      "     BatchNorm1d-149              [-1, 8, 1500]              16\n",
      "            ReLU-150              [-1, 8, 1500]               0\n",
      "          Conv1d-151              [-1, 8, 1500]             584\n",
      "     BatchNorm1d-152              [-1, 8, 1500]              16\n",
      "            ReLU-153              [-1, 8, 1500]               0\n",
      "       AvgPool1d-154               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-155               [-1, 8, 750]               0\n",
      "          Conv1d-156              [-1, 8, 1500]             840\n",
      "     BatchNorm1d-157              [-1, 8, 1500]              16\n",
      "            ReLU-158              [-1, 8, 1500]               0\n",
      "          Conv1d-159              [-1, 8, 1500]             840\n",
      "     BatchNorm1d-160              [-1, 8, 1500]              16\n",
      "            ReLU-161              [-1, 8, 1500]               0\n",
      "       AvgPool1d-162               [-1, 8, 750]               0\n",
      "MyAvgPool1dPadSame-163               [-1, 8, 750]               0\n",
      "          Conv1d-164               [-1, 8, 750]             200\n",
      "  InceptionBlock-165               [-1, 8, 750]               0\n",
      "          Conv1d-166               [-1, 1, 750]               9\n",
      "          Conv1d-167               [-1, 1, 750]               9\n",
      "FeatureExtractor_CNNlight-168                  [-1, 110]               0\n",
      "DominantFreqRegression-169                    [-1, 1]               0\n",
      "          Linear-170                   [-1, 50]           5,550\n",
      "         Dropout-171                   [-1, 50]               0\n",
      "            ReLU-172                   [-1, 50]               0\n",
      "          Linear-173                    [-1, 2]             102\n",
      "DomainClassifier-174                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 31,116\n",
      "Trainable params: 31,116\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 28.23\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 28.34\n",
      "----------------------------------------------------------------\n",
      "trainable pytorch_total_params: 25446\n",
      "all pytorch_total_params: 25446\n"
     ]
    }
   ],
   "source": [
    "def test_model_lstm(training_params):\n",
    "    print('test_model_lstm')\n",
    "\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    # prepare model\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    model = model.to(device).float()\n",
    "\n",
    "    # prepare data\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    data = torch.from_numpy(data)\n",
    "\n",
    "    feature = dataloaders['val'].dataset.feature[:5,:]\n",
    "    feature = torch.from_numpy(feature)\n",
    "\n",
    "    label = dataloaders['val'].dataset.label[:5,:]\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    data = data.to(device=device, dtype=torch.float)\n",
    "    feature = feature.to(device=device, dtype=torch.float)\n",
    "    label = label.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model inference\n",
    "    out = model(data, feature)\n",
    "\n",
    "    # compute loss\n",
    "    criterion = MultiTaskLoss(training_params)\n",
    "    losses = criterion(out, label)\n",
    "\n",
    "    # check losses\n",
    "    print(losses)\n",
    "    del model\n",
    "\n",
    "    \n",
    "def test_model(training_params):\n",
    "    print('test_model')\n",
    "\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    print(model)\n",
    "    del model\n",
    "    \n",
    "def test_model_dann(training_params):\n",
    "    print('test_model_dann')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_DANN(training_params=training_params)\n",
    "    print(model)\n",
    "\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    \n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('trainable pytorch_total_params:', pytorch_total_params)\n",
    "    \n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print('all pytorch_total_params:', pytorch_total_params)\n",
    "\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "debug_model = True\n",
    "if debug_model==True:\n",
    "    if 'LSTM' in training_params['model_name']:\n",
    "        test_model_lstm(training_params)\n",
    "    elif 'DANN' not in training_params['model_name']:\n",
    "        test_model(training_params)\n",
    "    else:\n",
    "        test_model_dann(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_out_dim   : 110\n",
      "feature_out_dim   : 110\n",
      "ECG_filt pytorch_total_params: 9897\n",
      "scgZ pytorch_total_params: 9897\n",
      "regressors pytorch_total_params: 0\n",
      "domain_classifier pytorch_total_params: 5652\n",
      "trainable pytorch_total_params: 25446\n"
     ]
    }
   ],
   "source": [
    "check_model_params = True\n",
    "check_inference = False\n",
    "\n",
    "if check_model_params:\n",
    "\n",
    "    model = resp_DANN(training_params=training_params)\n",
    "\n",
    "    if 'ECG_filt' in training_params['input_names']:\n",
    "        pytorch_total_params = sum(p.numel() for p in model.feature_extractors.ECG_filt.parameters() if p.requires_grad)\n",
    "        print('ECG_filt pytorch_total_params:', pytorch_total_params)\n",
    "    if 'scgZ' in training_params['input_names']:\n",
    "        pytorch_total_params = sum(p.numel() for p in model.feature_extractors.scgZ.parameters() if p.requires_grad)\n",
    "        print('scgZ pytorch_total_params:', pytorch_total_params)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.regressors.parameters() if p.requires_grad)\n",
    "    print('regressors pytorch_total_params:', pytorch_total_params)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.domain_classifier.parameters() if p.requires_grad)\n",
    "    print('domain_classifier pytorch_total_params:', pytorch_total_params)\n",
    "\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('trainable pytorch_total_params:', pytorch_total_params)\n",
    "    \n",
    "    \n",
    "    if check_inference:\n",
    "        device = torch.device('cpu')\n",
    "        # device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        model = model.to(device).float()\n",
    "\n",
    "        # prepare data\n",
    "        dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "        data = dataloaders['val'].dataset.data[[0],:,:]\n",
    "        data = torch.from_numpy(data)\n",
    "\n",
    "        feature = dataloaders['val'].dataset.feature[[0],:]\n",
    "        feature = torch.from_numpy(feature)\n",
    "\n",
    "        data = data.to(device=device, dtype=torch.float)\n",
    "        feature = feature.to(device=device, dtype=torch.float)\n",
    "\n",
    "        # model inference\n",
    "        # out = model(data, feature)\n",
    "        %timeit -n 1 -r 1000 out = model(data, feature)\n",
    "    \n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " # training_params['input_names']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resp_DANN(training_params=training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.feature_extractors.ECG_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (data, feature, label, meta) in enumerate( dataloaders['train']):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.size(), feature.size(), label.size()\n",
    "# data[[0],0,:].shape\n",
    "# data.size()\n",
    "\n",
    "# data[[0],[1],:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.feature_extractors.scgZ.ch_pooling(data[[0],:,:], feature[[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. set up the hook\n",
    "# activation = {}\n",
    "# def get_activation(name):\n",
    "#     def hook(model, input, output):\n",
    "#         activation[name] = output.detach()\n",
    "#     return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             # model.feature_extractors[input_name].basicblock_list[-1].register_forward_hook(get_activation(input_name+'_layer_last'))\n",
    "# model.feature_extractors.scgZ.ch_pooling.register_forward_hook(get_activation('last_conv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out, deep_feature = model(data[[0],:,:], feature[[0],:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(activation['last_conv'].data.detach().cpu().numpy()[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(deep_feature['ECG_filt'].data.detach().cpu().numpy()[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_feature['ECG_filt'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9897\n",
    "# 25446\n",
    "# 25446\n",
    "# 25446\n",
    "# 25446\n",
    "# 6097\n",
    "# 2645\n",
    "# 49098\n",
    "# 26386\n",
    "# 30346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "## make sure data can pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_flow = False\n",
    "\n",
    "if check_data_flow:\n",
    "\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data = dataloaders['train'].dataset.data[:5,:,:]\n",
    "    # data = torch\n",
    "    data = torch.from_numpy(data)\n",
    "    data = data.to(device=device, dtype=torch.float)\n",
    "\n",
    "    feature = dataloaders['train'].dataset.feature[:5,:]\n",
    "    # data = torch\n",
    "    feature = torch.from_numpy(feature)\n",
    "    feature = feature.to(device=device, dtype=torch.float)\n",
    "\n",
    "    label = dataloaders['train'].dataset.label[:5,:]\n",
    "    # data = torch\n",
    "    label = torch.from_numpy(label)\n",
    "    label = label.to(device=device, dtype=torch.float)\n",
    "\n",
    "    model = resp_DANN(training_params=training_params)\n",
    "    model = model.to(device).float()\n",
    "    output = model(data, feature)\n",
    "\n",
    "    print(data.size(), feature.size(), label.size(), output['domain-ECG_filt'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, val, eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    wandb.login()\n",
    "    os.environ[\"WANDB_DIR\"] = os.path.abspath(outputdir)\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'feature_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_folder(training_params):\n",
    "    n_block = training_params['n_block']\n",
    "    inputs_combined = '+'.join([ i_name.split('_')[0] for i_name in training_params['input_names']])\n",
    "    auxillary_weight = training_params['loss_weights']['auxillary_task']\n",
    "    adversarial_weight = training_params['adversarial_weight']\n",
    "    channel_n = training_params['channel_n']\n",
    "    \n",
    "    list_act = '+'.join( [str(int) for int in training_params['activity_names']] )\n",
    "    \n",
    "    sweep_folder = '{}blocks-{}-weight{}+{}-{}ch-act{}-{}'.format(n_block, inputs_combined, auxillary_weight, adversarial_weight, channel_n, list_act, training_params['regressor_type'])\n",
    "    \n",
    "    return sweep_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputdirs(training_params):\n",
    "\n",
    "    outputdir = training_params['outputdir']\n",
    "    sweep_folder = get_sweep_folder(training_params)\n",
    "    outputdir_sweep = outputdir+'{}/'.format(sweep_folder)\n",
    "\n",
    "    outputdir_numeric = outputdir_sweep + 'numeric_results/'\n",
    "    if outputdir_numeric is not None:\n",
    "        if not os.path.exists(outputdir_numeric):\n",
    "            os.makedirs(outputdir_numeric)\n",
    "\n",
    "    outputdir_modelout = outputdir_sweep + 'model_output/'\n",
    "    if outputdir_modelout is not None:\n",
    "        if not os.path.exists(outputdir_modelout):\n",
    "            os.makedirs(outputdir_modelout)\n",
    "\n",
    "    outputdir_activation = outputdir_sweep + 'activation_layers/'\n",
    "    if outputdir_activation is not None:\n",
    "        if not os.path.exists(outputdir_activation):\n",
    "            os.makedirs(outputdir_activation)\n",
    "\n",
    "    outputdir_feature = outputdir_sweep + 'feature_visualization/'\n",
    "    if outputdir_feature is not None:\n",
    "        if not os.path.exists(outputdir_feature):\n",
    "            os.makedirs(outputdir_feature)\n",
    "\n",
    "    training_params['outputdir_sweep'] = outputdir_sweep\n",
    "    training_params['outputdir_numeric'] = outputdir_numeric\n",
    "    training_params['outputdir_modelout'] = outputdir_modelout\n",
    "    training_params['outputdir_activation'] = outputdir_activation\n",
    "    training_params['outputdir_feature'] = outputdir_feature\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_auxillary = False\n",
    "\n",
    "def train_master(training_params):\n",
    "    \n",
    "    # TODO: change all to training_params['xxx'] = get_xxx(training_params)\n",
    "    training_params = get_outputdirs(training_params) # could be tricky since it changes several keys\n",
    "    training_params = get_regressor_names(training_params) # may not need this in this task\n",
    "    training_params['model_out_names'] = get_model_out_names(training_params)\n",
    "    training_params['modality_dict'] = get_modality_dict(training_params)\n",
    "    training_params = update_freq_meta(training_params) # could be tricky since it changes several keys\n",
    "\n",
    "    \n",
    "#     pprint.pprint(training_params)\n",
    "\n",
    "    if len(training_params['input_names'])==1:\n",
    "#         if  training_params['loss_weights']['auxillary_task']!=0:\n",
    "        if training_params['adversarial_weight']!=0 or training_params['loss_weights']['auxillary_task']!=0:\n",
    "            print('ony one signal, no need to try all DA weights')\n",
    "            return\n",
    "        \n",
    "\n",
    "    \n",
    "    df_performance_train = {}\n",
    "    df_performance_val = {}\n",
    "\n",
    "    df_outputlabel_train = {}\n",
    "    df_outputlabel_val = {}\n",
    "\n",
    "#     for task in training_params['tasks']:\n",
    "#     for task in training_params['regressor_names']:\n",
    "    for task in training_params['model_out_names']:\n",
    "\n",
    "        df_performance_train[task] = pd.DataFrame()\n",
    "        df_performance_val[task] = pd.DataFrame()\n",
    "\n",
    "        df_outputlabel_train[task] = pd.DataFrame()\n",
    "        df_outputlabel_val[task] = pd.DataFrame()\n",
    "\n",
    "#     ordered_subject_ids = np.asarray([115, 107, 113, 110, 101, 104, 106, 121, 212, 102, 103, 111, 114, 116, 118, 119, 120])\n",
    "#     ordered_subject_ids = np.asarray([107, 113, 110, 101, 104, 115, 106, 121, 212, 102, 103, 111, 114, 116, 118, 119, 120])\n",
    "#         ordered_subject_ids = np.asarray([101, 110, 113, 119, 115, 107, 104, 106, 121, 212, 102, 103, 111, 114, 116, 118, 120])\n",
    "#     ordered_subject_ids = np.asarray([101, 110, 113, 119, 115])\n",
    "\n",
    "    ordered_subject_ids = training_params['ordered_subject_ids']\n",
    "    main_task = training_params['output_names'][0].split('-')[0]\n",
    "    i_activity = training_params['list_meta'].index('task') #  i_activity th column has the activity info\n",
    "\n",
    "    # look at the interesting subjects first so I can quickly debug\n",
    "    for i_CV, subject_id in enumerate(ordered_subject_ids):\n",
    "        \n",
    "#         if subject_id !=110:\n",
    "#             continue\n",
    "        \n",
    "        if 'CV_max' in training_params:\n",
    "            if i_CV >= training_params['CV_max']:\n",
    "                continue\n",
    "\n",
    "        training_params['CV_config']['subject_id'] = subject_id\n",
    "\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "        print('using device', device)\n",
    "\n",
    "        \n",
    "        # need to load the data for each LOSO CV\n",
    "        dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "        print('data dimensions are:', dataloaders['train'].dataset.data.shape)\n",
    "        print('dataset_sizes: ', dataset_sizes)\n",
    "\n",
    "        # update the dimension so the model is created correctly\n",
    "        data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "        training_params['data_dimensions'] = list(data_dimensions)\n",
    "\n",
    "        print('using model ', training_params['model_name'])\n",
    "\n",
    "#         training_params = get_regressor_names(training_params)\n",
    "#         model = resp_multiverse(training_params=training_params)\n",
    "        model = resp_DANN(training_params=training_params)\n",
    "\n",
    "#         print(model)\n",
    "        \n",
    "        model = model.to(device).float()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'], weight_decay=0.01)\n",
    "\n",
    "#         criterion = MultiTaskLoss(training_params)\n",
    "        criterion = AdversarialLoss(training_params)\n",
    "\n",
    "        \n",
    "        training_params['criterion'] = criterion\n",
    "        training_params['optimizer'] = optimizer\n",
    "        training_params['inputdir'] = inputdir\n",
    "        \n",
    "        \n",
    "#         print( training_params['regressor_names'])\n",
    "        CV_dict = train_model(model, training_params, dataloaders, trainer, evaler, preder)\n",
    "\n",
    "#         print(CV_dict['performance_dict_val']['out_dict'])\n",
    "#         sys.exit()\n",
    "\n",
    "        plot_losses(CV_dict, outputdir=training_params['outputdir_sweep'], show_plot=False)\n",
    "        \n",
    "\n",
    "#         TODO: fix this code\n",
    "#         print(training_params['regressor_names'])\n",
    "        \n",
    "#         for task in CV_dict['performance_dict_train']['out_dict'].keys():\n",
    "#         for task in training_params['regressor_names']:\n",
    "        for task in training_params['model_out_names']:\n",
    "            if 'domain' in task:\n",
    "                continue\n",
    "        \n",
    "            label_est_val = CV_dict['performance_dict_val']['out_dict'][task]\n",
    "            label_val = CV_dict['performance_dict_val']['label_dict'][task]\n",
    "            activity_val = CV_dict['performance_dict_val']['meta_arr'][:,i_activity]\n",
    "            \n",
    "\n",
    "            label_est_train = CV_dict['performance_dict_train']['out_dict'][task]\n",
    "            label_train = CV_dict['performance_dict_train']['label_dict'][task]\n",
    "            activity_train = CV_dict['performance_dict_train']['meta_arr'][:,i_activity]\n",
    "\n",
    "#             if 'domain' in task:\n",
    "#                 np.argmax(a, axis=1)\n",
    "            \n",
    "            \n",
    "            # rescale the label after making estimations\n",
    "            if 'perc' in training_params['output_names'][0]:\n",
    "                i_meta = training_params['meta_names'].index('EEavg_est')\n",
    "#                 print(CV_dict['performance_dict_train']['meta_arr'], CV_dict['performance_dict_train']['meta_arr'].shape)\n",
    "                meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "                meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "                label_train = label_train*meta_train\n",
    "                label_val = label_val*meta_val\n",
    "                label_est_train = label_est_train*meta_train\n",
    "                label_est_val = label_est_val*meta_val\n",
    "            elif 'weighted' in training_params['output_names'][0]:\n",
    "                i_meta = training_params['meta_names'].index('weight')\n",
    "                meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "                meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "                label_train = label_train*meta_train\n",
    "                label_val = label_val*meta_val\n",
    "                label_est_train = label_est_train*meta_train\n",
    "                label_est_val = label_est_val*meta_val\n",
    "\n",
    "            \n",
    "            # get performance df for training and testing dataset\n",
    "            df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, subject_id, task), ignore_index=True )\n",
    "\n",
    "            df_performance_train[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            \n",
    "#             print(task , label_train, label_train.shape )\n",
    "            \n",
    "            \n",
    "            df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "                pd.DataFrame( {\n",
    "                'label_est': label_est_train,\n",
    "                'label': label_train,\n",
    "                'CV': [subject_id]*label_train.shape[0],\n",
    "                'task': [task]*label_train.shape[0],\n",
    "                'activity': np.vectorize(tasks_dict_reversed.get)(activity_train)\n",
    "                }), ignore_index=True )\n",
    "\n",
    "            df_outputlabel_train[task].to_csv(training_params['outputdir_numeric'] + 'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, subject_id, task), ignore_index=True )\n",
    "            df_performance_val[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "                pd.DataFrame( {\n",
    "                'label_est': label_est_val,\n",
    "                'label': label_val,\n",
    "                'CV': [subject_id]*label_val.shape[0],\n",
    "                'task': [task]*label_val.shape[0],\n",
    "                'activity': np.vectorize(tasks_dict_reversed.get)(activity_val)\n",
    "                }), ignore_index=True )\n",
    "\n",
    "            df_outputlabel_val[task].to_csv(training_params['outputdir_numeric'] + 'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            # plot performance training and testing dataset\n",
    "            if (main_task not in task) and (debug_auxillary==False):\n",
    "                continue\n",
    "            \n",
    "#             plot_regression(df_outputlabel_train[task], df_performance_train[task], task, fig_name='regression_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "#             plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "            plot_regression(df_outputlabel_val[task], df_performance_val[task], task, training_params, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "#             plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "#             plot_output(df_outputlabel_train[task], task, fig_name = 'outputINtime_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout)\n",
    "        \n",
    "        if training_params['regressor_type']=='DominantFreqRegression':\n",
    "            check_featuremap(model, training_params, mode='worst', fig_name = 'DL_activation_{}_'.format(subject_id), outputdir=training_params['outputdir_activation']+'worst/{}/'.format(subject_id), show_plot=False)\n",
    "            check_featuremap(model, training_params, mode='best', fig_name = 'DL_activation_{}_'.format(subject_id), outputdir=training_params['outputdir_activation']+'best/{}/'.format(subject_id), show_plot=False)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for task in training_params['model_out_names']:\n",
    "        if main_task not in task:\n",
    "            continue\n",
    "#         if task!=main_task:\n",
    "#             continue\n",
    "#         plot_regression_all_agg(df_outputlabel_train[task], df_performance_train[task], training_params, fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "        plot_regression(df_outputlabel_train[task], df_performance_train[task], task, training_params, fig_name='regression_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "        plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "\n",
    "#         plot_regression_all_agg(df_outputlabel_val[task], df_performance_val[task], training_params, fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "        plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "\n",
    "        plot_output(df_outputlabel_val[task], task, fig_name = 'outputINtime_val_{}'.format(task),  show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "\n",
    "#     plot_BA(df_outputlabel_val[main_task], main_task, fig_name='BA_val_{}'.format(main_task), show_plot=False, outputdir=outputdir+'model_output/', log_wandb=training_params['wandb'])\n",
    "#     plot_regression_all_agg(df_outputlabel_val[main_task], df_performance_val[main_task], outputdir=outputdir+'model_output/', show_plot=False, log_wandb=training_params['wandb'])\n",
    "\n",
    "    # log metrices on wnadb\n",
    "    if training_params['wandb']==True:\n",
    "        main_task = training_params['model_out_names'][0]\n",
    "\n",
    "        for task in training_params['model_out_names']:\n",
    "            if 'domain' in task:\n",
    "                continue\n",
    "            \n",
    "            label = df_outputlabel_val[task]['label'].values\n",
    "            label_est = df_outputlabel_val[task]['label_est'].values\n",
    "\n",
    "            PCC = get_PCC(label, label_est)\n",
    "            Rsquared = get_CoeffDeterm(label, label_est)\n",
    "            MAE, _ = get_MAE(label, label_est)\n",
    "            RMSE = get_RMSE(label, label_est)\n",
    "            MAPE, _ = get_MAPE(label, label_est)\n",
    "            wandb.log(\n",
    "                {\n",
    "                    '{}_MAE'.format(task): MAE,\n",
    "#                     '{}_RMSE'.format(task): RMSE,\n",
    "#                     '{}_MAPE'.format(task): MAPE,\n",
    "                    '{}_PCC'.format(task): PCC,\n",
    "                    '{}_Rsquared'.format(task): Rsquared,\n",
    "                })\n",
    "            \n",
    "            if task == main_task:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        'val_MAE'.format(task): MAE,\n",
    "    #                     '{}_RMSE'.format(task): RMSE,\n",
    "    #                     '{}_MAPE'.format(task): MAPE,\n",
    "                        'val_PCC'.format(task): PCC,\n",
    "                        'val_Rsquared'.format(task): Rsquared,\n",
    "                    })\n",
    "\n",
    "\n",
    "        \n",
    "    data_saver(training_params, 'training_params', training_params['outputdir_sweep'])\n",
    "    # training_params = data_loader('training_params', outputdir).item()\n",
    "        \n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['model_out_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['model_out_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# x = torch.randn( 3, 4)\n",
    "# y = torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(config=None):\n",
    "\n",
    "#     with wandb.init(config=config, entity='inanlab', project=\"[TL] stage2_cnn\", reinit=True, dir=outputdir):\n",
    "    with wandb.init(config=config, reinit=True, dir=outputdir):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "#         print(config)\n",
    "        pprint.pprint(config)\n",
    "\n",
    "        # init the model\n",
    "        \n",
    "        # things that need to be change when using a new config\n",
    "        for key in config.keys():\n",
    "            if key=='loss_weights':\n",
    "#                 training_params[key]['RR_cosmed'] = config[key]\n",
    "                training_params[key]['auxillary_task'] = config[key]\n",
    "            else:\n",
    "                training_params[key] = config[key]\n",
    "\n",
    "        train_master(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: use training_params['regressor_names'] for looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    print('sweeping for:', sweep_name)\n",
    "    sweep_config = training_params['sweep_config']    \n",
    "#     with wandb.init(config=config, entity='inanlab', project=\"[TL] stage2_cnn\", reinit=True, dir=outputdir):\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='inanlab', project='[FL] stage4_'+training_params['sweep_name'])\n",
    "\n",
    "#     sweep_id = wandb.sweep(sweep_config, project=sweep_name)\n",
    "    wandb.agent(sweep_id, train_sweep)\n",
    "    \n",
    "else:\n",
    "    train_master(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'label_range' in training_params:\n",
    "#     if training_params['label_range'] == 'label+estimated':\n",
    "#         print('hji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: think where to place this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_end = datetime.now(tz_NY)\n",
    "print(\"end time:\", datetime_end.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n",
    "duration = datetime_end-datetime_start\n",
    "duration_in_s = duration.total_seconds()\n",
    "days    = divmod(duration_in_s, 86400)        # Get days (without [0]!)\n",
    "hours   = divmod(days[1], 3600)               # Use remainder of days to calc hours\n",
    "minutes = divmod(hours[1], 60)                # Use remainder of hours to calc minutes\n",
    "seconds = divmod(minutes[1], 1)               # Use remainder of minutes to calc seconds\n",
    "print(\"Time between dates: %d days, %d hours, %d minutes and %d seconds\" % (days[0], hours[0], minutes[0], seconds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mapped(training_params):\n",
    "    \n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    label = dataloaders['val'].dataset.label\n",
    "    # this function is written to map one vector to another (find the closest value in a dictionary vector)\n",
    "    xf = np.linspace(0.0, 1.0/2.0*training_params['FS_Extracted'] , 375//2)*60\n",
    "    mask = (xf>=label_range_dict['HR_DL'][0]) & (xf<=label_range_dict['HR_DL'][1])\n",
    "\n",
    "    diff_matrix = np.abs(xf[None,:] - label.squeeze()[:,None]) # label_dim (subject data) x reference_dim (ordered)\n",
    "    indices = np.argmin(diff_matrix, axis=1) # label_dim \n",
    "    print('indices', indices.shape)\n",
    "    xf_repeated = np.tile(xf, (indices.shape[0], 1)).T\n",
    "    print('xf_repeated', xf_repeated.shape)\n",
    "\n",
    "    label_mapped = xf_repeated[ indices, range(xf_repeated.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label_mapped(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x size torch.Size([2, 89])\n",
    "# index_dominant torch.Size([2])\n",
    "# self.xf_masked torch.Size([89])\n",
    "# xf_repeated torch.Size([89, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def get_activation(name):\n",
    "# #     activation = {}\n",
    "# #     def hook(model, input, output):\n",
    "# #         activation[name] = output.detach()\n",
    "# #     return hook\n",
    "    \n",
    "# def model_hooking(model, training_params, get_activation):\n",
    "\n",
    "#     model_name = training_params['model_name']\n",
    "    \n",
    "#     if model_name=='FeatureExtractor_CNN2':\n",
    "#         key = list(model.feature_extractors.keys())[0]\n",
    "#         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "#         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "#         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "#         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "#         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "#         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n",
    "        \n",
    "# #         layer_names = ['layer1', ]\n",
    "        \n",
    "#     elif model_name=='FeatureExtractor_CNN':\n",
    "#         key = list(model.feature_extractors.keys())[0]\n",
    "#         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "#         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "#         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "#         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "#         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "#         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n",
    "#     if model_name=='ResNet1D':\n",
    "#         pass\n",
    "# #         key = list(model.feature_extractors.keys())[0]\n",
    "# #         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "# #         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "# #         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "# #         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "# #         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "# #         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: improve this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dict_train = preder(model, dataloaders['train'], training_params)\n",
    "performance_dict_val = preder(model, dataloaders['val'], training_params)\n",
    "\n",
    "unit = unit_dict[task.split('_')[0]]\n",
    "\n",
    "for task in training_params['tasks']:\n",
    "    \n",
    "    \n",
    "    print('evaluating task:', task)\n",
    "    MAE, std_AE = get_MAE(performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task])\n",
    "    print('\\ttrainin: {:.2f}±{:.2f} {}'.format(MAE, std_AE, unit))\n",
    "\n",
    "    MAE, std_AE = get_MAE(performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task])\n",
    "    print('\\tval: {:.2f}±{:.2f} {}'.format(MAE, std_AE, unit))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce output figures\n",
    "# TODO: implement the plotting functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO!!!!\n",
    "\n",
    "# for task in tasks\n",
    "#     plot_loss vs epoch (train and val)\n",
    "#     plot_MAE, RMSE, vs epoch (train and val)\n",
    "#     plot scatter plots (show PCC, BD, std, ect.) (just val)\n",
    "#     plot output vs. label (just val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = training_params['output_names'][0].split('_')[0]\n",
    "unit_dict[output_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kcalpmin2watt = 69.7333333\n",
    "\n",
    "# sub_weight = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_dict_train['label_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in training_params['tasks']:\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5), dpi=100)\n",
    "    fontsize = 15\n",
    "    data_min = np.min(np.r_[performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task]])\n",
    "    data_max = np.max(np.r_[performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task]])\n",
    "    ax1.scatter(performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task], alpha=0.3)\n",
    "    ax1.set_xlim(data_min, data_max)\n",
    "    ax1.set_ylim(data_min, data_max)\n",
    "    ax1.plot( [data_min, data_max],[data_min, data_max], '--', color='gray', alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('estimated {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    ax1.set_ylabel('true {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "#     ax.set_xlabel('estimated {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "#     ax.set_ylabel('true {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "\n",
    "    ax1.set_title('training', fontsize=fontsize)\n",
    "\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(5,5))\n",
    "#     fontsize = 15\n",
    "    data_min = np.min(np.r_[performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task]])\n",
    "    data_max = np.max(np.r_[performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task]])\n",
    "    ax2.scatter(performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task], alpha=0.3)\n",
    "    ax2.set_xlim(data_min, data_max)\n",
    "    ax2.set_ylim(data_min, data_max)\n",
    "    ax2.plot( [data_min, data_max],[data_min, data_max], '--', color='gray', alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('estimated {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    ax2.set_ylabel('true {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    \n",
    "    ax2.set_title('testing', fontsize=fontsize)\n",
    "\n",
    "#     ax.set_xlabel('estimated {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "#     ax.set_ylabel('true {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "mienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
