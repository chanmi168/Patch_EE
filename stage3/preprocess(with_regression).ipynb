{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add features\n",
    "# window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from filters import *\n",
    "from setting import *\n",
    "from segmentation import *\n",
    "# from stage1_patch import *\n",
    "# from TimeStampReader import *\n",
    "# from Subject import *\n",
    "# from PatchParser import *\n",
    "from preprocessing import *\n",
    "from ECG_module import *\n",
    "# from PPG_module import *\n",
    "from evaluate import *\n",
    "from stage1_cosmed import *\n",
    "from stage3_preprocess import *\n",
    "from dataIO import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../data/stage2/', output_folder='../../data/stage3/', training_params_file='training_params.json')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='SpO2_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "# parser.add_argument('--subject_id', metavar='subject_id', help='subject_id',\n",
    "#                     default='101')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "args = parser.parse_args(['--input_folder', '../../data/stage2/', \n",
    "                          '--output_folder', '../../data/stage3/',\n",
    "#                           '--subject_id', 'sub118',\n",
    "                          '--training_params_file', 'training_params.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fs = FS_RESAMPLE\n",
    "Fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "# subject_id = args.subject_id\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "    \n",
    "# outputdir_sub = outputdir+subject_id+'/'\n",
    "# if not os.path.exists(outputdir_sub):\n",
    "#     os.makedirs(outputdir_sub)\n",
    "    \n",
    "training_params_file = args.training_params_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "    \n",
    "training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'list_signal': ['ECG', 'accelZ', 'ppg_r_1'],\n",
       " 'list_feature': ['VE_cosmed', 'HR_patch'],\n",
       " 'list_output': ['RR_cosmed',\n",
       "  'VT_cosmed',\n",
       "  'EE_cosmed',\n",
       "  'SPO2_cosmed',\n",
       "  'HR_cosmed',\n",
       "  'VO2_cosmed',\n",
       "  'resp_cosmed'],\n",
       " 'list_meta': ['subject_id', 'task'],\n",
       " 'FS_RESAMPLE_DL': 100,\n",
       " 'window_size': 60,\n",
       " 'overlap': 0.9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_signal = training_params['list_signal']\n",
    "list_feature = training_params['list_feature']\n",
    "list_output = training_params['list_output']\n",
    "list_meta = training_params['list_meta']\n",
    "FS_RESAMPLE_DL = training_params['FS_RESAMPLE_DL']\n",
    "window_size = training_params['window_size'] # s\n",
    "overlap = training_params['overlap'] # out of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cosmed_unfiltered:\n",
    "#     obj_cosmed = load_sub('../../data/stage1/'+subject_id+'/cosmed_unfiltered/data')\n",
    "# else:\n",
    "\n",
    "# obj_cosmed = load_sub('../../data/stage1/'+subject_id+'/cosmed/data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Baseline 0',\n",
       " 'Recovery 0',\n",
       " 'Recovery 1',\n",
       " 'Recovery 2',\n",
       " 'Recovery 3',\n",
       " 'Recovery 4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationary_names = ['Baseline 0']\n",
    "for j in range(5):\n",
    "    stationary_names.append('Recovery ' + str(j))\n",
    "# stationary_names.append( '6MWT 0' )\n",
    "# stationary_names.append( '6MWT-R 0' )\n",
    "# stationary_names.append( 'Stair 0' )\n",
    "# stationary_names.append( 'Walk 0' )\n",
    "# stationary_names.append( 'Run 0' )\n",
    "\n",
    "stationary_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get sync data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change activity name\n",
    "## maybe remove scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    print(i+101)\n",
    "#     subject_id = 'sub'+str(i+101)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "105 ECG\n",
    "108 ECG\n",
    "109 ECG\n",
    "112 cosmed\n",
    "115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_id is  sub101\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub102\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub103\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub104\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub105\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run' 'Recovery 5']\n",
      "subject_id is  sub106\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run' 'Recovery 5']\n",
      "subject_id is  sub107\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub108\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub109\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub110\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub111\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub113\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub114\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub115\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub116\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub117\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub118\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub119\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub120\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub121\n",
      "['Baseline' 'Recovery 6MWT-R' 'Recovery 6MWT' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n",
      "subject_id is  sub212\n",
      "['Baseline' 'Recovery 6MWT' 'Recovery 6MWT-R' 'Recovery Stair'\n",
      " 'Recovery Walk' 'Recovery Run']\n"
     ]
    }
   ],
   "source": [
    "no_scaling = False\n",
    "\n",
    "df_sync = []\n",
    "# for i in range(21):\n",
    "\n",
    "for i in range(300):\n",
    "    subject_id = 'sub'+str(i)\n",
    "\n",
    "#     if subject_id =='sub105' or subject_id =='sub108' or subject_id =='sub109' or subject_id =='sub112' or subject_id =='sub117':\n",
    "    if subject_id =='sub112':\n",
    "        continue\n",
    "        \n",
    "\n",
    "#     if subject_id !='sub106':\n",
    "#         print(subject_id)\n",
    "#         continue\n",
    "    \n",
    "    inputdir_sub = inputdir+subject_id+'/'\n",
    "    if subject_id not in os.listdir(inputdir):\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    print('subject_id is ', subject_id)\n",
    "\n",
    "    for syn_file in os.listdir(inputdir_sub):\n",
    "        if '.feather'  in syn_file:\n",
    "            df_sub = pd.read_feather(inputdir_sub+syn_file)\n",
    "#             df_sub = filter_DFcolumns(df_sub.copy(), Fs)\n",
    "\n",
    "#     sys.exit()\n",
    "    # get raw task names except for Transition\n",
    "    task_unique = df_sub['task'].unique()\n",
    "    task_unique = task_unique[task_unique!='Transition']\n",
    "\n",
    "#     df_sub = filter_DFcolumns(df_sub, Fs)\n",
    "        \n",
    "    for stationary_name in stationary_names:\n",
    "        \n",
    "        if 'Baseline' not in stationary_name and 'Recovery' not in stationary_name:\n",
    "            stationary_name_new = stationary_name.split(' ')[0]\n",
    "        else:\n",
    "\n",
    "            # get recovery index\n",
    "            i_recovery = np.where(task_unique==stationary_name)[0]\n",
    "\n",
    "            # the index of the task associated with this recovery is i_recovery-1. Get its name\n",
    "            task_name = task_unique[i_recovery-1][0]\n",
    "\n",
    "            # spell out new recovery name\n",
    "    #         recovery_name_new = ' '.join([recovery_name.split(' ')[0], task_name.split(' ')[0] ,recovery_name.split(' ')[1]])\n",
    "\n",
    "            if 'Baseline' in stationary_name:\n",
    "                stationary_name_new = stationary_name.split(' ')[0]\n",
    "            else:\n",
    "                stationary_name_new = ' '.join([stationary_name.split(' ')[0], task_name.split(' ')[0]])\n",
    "        \n",
    "\n",
    "        df_sub.loc[df_sub['task']==stationary_name, 'task'] = stationary_name_new\n",
    "        \n",
    "        if no_scaling:\n",
    "            obj_cosmed = load_sub('../../data/stage1/'+subject_id+'/cosmed/data')\n",
    "            print(obj_cosmed.Weight)\n",
    "            df_sub['VT_cosmed'] = df_sub['VT_cosmed'] * obj_cosmed.Weight\n",
    "            df_sub['VE_cosmed'] = df_sub['VE_cosmed'] * obj_cosmed.Weight\n",
    "            df_sub['VO2_cosmed'] = df_sub['VO2_cosmed'] * obj_cosmed.Weight\n",
    "            df_sub['VCO2_cosmed'] = df_sub['VCO2_cosmed'] * obj_cosmed.Weight\n",
    "            df_sub['EE_cosmed'] = df_sub['EE_cosmed'] * obj_cosmed.Weight\n",
    "\n",
    "    df_sub = df_sub.loc[(df_sub['task'].str.contains(\"Baseline\", case=False)) | (df_sub['task'].str.contains(\"Recovery\", case=False)) | \n",
    "                        (df_sub['task']==\"6MWT\") | (df_sub['task']==\"6MWT-R\") | (df_sub['task']==\"Run\") | (df_sub['task']==\"Walk\") | (df_sub['task']==\"Stair\")\n",
    "                       ].reset_index(drop=True)\n",
    "#     df_sub = df_sub.loc[(df_sub['task'].str.contains(\"Baseline\", case=False))].reset_index(drop=True)\n",
    "    df_sub['subject_id'] = subject_id[3:]\n",
    "    \n",
    "    print(df_sub['task'].unique())\n",
    "\n",
    "    df_sync.append(df_sub.copy())\n",
    "    #     print((df_sync['task']==recovery_name).sum())\n",
    "    \n",
    "df_sync = pd.concat(df_sync).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add HR_patch and resp_cosmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub: 101, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 101, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 101, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 101, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 101, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 101, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 102, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 103, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 104, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 105, task: Baseline\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 105, task: Recovery 6MWT\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 105, task: Recovery 6MWT-R\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 105, task: Recovery Stair\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 105, task: Recovery Walk\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 105, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 105, task: Recovery 5\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 106, task: Recovery 5\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 107, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 108, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 109, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 110, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 111, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 113, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 114, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 115, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 116, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 117, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 118, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 119, task: Baseline\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 119, task: Recovery 6MWT-R\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 119, task: Recovery 6MWT\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 119, task: Recovery Stair\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 119, task: Recovery Walk\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 119, task: Recovery Run\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 120, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 120, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 120, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 120, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 120, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 120, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 121, task: Baseline\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 121, task: Recovery 6MWT-R\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 121, task: Recovery 6MWT\n",
      "\t use R peaks\n",
      "\t use R peaks\n",
      "sub: 121, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 121, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 121, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Baseline\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Recovery 6MWT\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Recovery 6MWT-R\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Recovery Stair\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Recovery Walk\n",
      "\t use S peaks\n",
      "\t use S peaks\n",
      "sub: 212, task: Recovery Run\n",
      "\t use S peaks\n",
      "\t use S peaks\n"
     ]
    }
   ],
   "source": [
    "df_sync['HR_patch'] = 0\n",
    "df_sync['resp_cosmed'] = 0\n",
    "\n",
    "include_spectral = False\n",
    "\n",
    "for subject_id in df_sync['subject_id'].unique():\n",
    "\n",
    "\n",
    "    df_sub = df_sync[df_sync['subject_id']==subject_id]\n",
    "    outputdir_sub = outputdir+subject_id+'/'\n",
    "    \n",
    "    for task_name in df_sub['task'].unique():\n",
    "        \n",
    "#         if subject_id!='111':\n",
    "#             continue\n",
    "#         if task_name!='Recovery Run':\n",
    "#             continue\n",
    "        print('sub: {}, task: {}'.format(subject_id, task_name))\n",
    "        df_task = df_sub[df_sub['task']==task_name]\n",
    "        ECG_raw_patch = df_task['ECG'].values\n",
    "        \n",
    "        fig_name = 'ECG_diagnostics_'+task_name\n",
    "        QRS_detector_dict_patch = task_HR_detector(ECG_raw_patch, Fs, fig_name=fig_name, outputdir=outputdir_sub, show_plot=False)\n",
    "        \n",
    "#         sys.exit()\n",
    "        fig_name = 'physio_diagnostics_'+task_name\n",
    "        plot_inputsigs(df_task, Fs, fig_name=fig_name, outputdir=outputdir_sub, show_plot=False)\n",
    "#         if subject_id=='102':\n",
    "#         sys.exit()\n",
    "\n",
    "        def get_RRVT2resp(df_task, Fs):\n",
    "            f_arr_raw = df_task['RR_cosmed'].values/60\n",
    "            t_arr = np.arange(df_task.shape[0])/Fs\n",
    "            # smooth it since it's usally digital and disgusting\n",
    "            f_arr = get_smooth(f_arr_raw, N=Fs*5)\n",
    "            # v_arr is the simulated sinusoidal breath signal, sampled eqaully as t_arr\n",
    "            v_arr, f_sim_interp = get_sim_breath(t_arr, f_arr, downsample_factor=Fs//5) # Fs=500Hz\n",
    "            resp_cosmed = v_arr*df_task['VT_cosmed']/2\n",
    "            \n",
    "            return resp_cosmed\n",
    "\n",
    "        resp_cosmed = get_RRVT2resp(df_task, Fs)\n",
    "        \n",
    "        t_ecg = np.arange(ECG_raw_patch.shape[0])/Fs\n",
    "        hr_interp = np.interp(t_ecg, QRS_detector_dict_patch['ts_hr'], QRS_detector_dict_patch['hr'])\n",
    "        O2pulse_cosmedpatch = df_task['VO2_cosmed']/hr_interp\n",
    "\n",
    "        \n",
    "#         sys.exit()\n",
    "        df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name), 'HR_patch'] = hr_interp\n",
    "        df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name), 'resp_cosmed'] = resp_cosmed\n",
    "        df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name), 'O2pulse_cosmedpatch'] = O2pulse_cosmedpatch\n",
    "\n",
    "        \n",
    "        if include_spectral:\n",
    "            i_peaks = QRS_detector_dict_patch['i_beat_peaks']\n",
    "            features_spectral = get_spectral_features(df_task, i_peaks, Fs)\n",
    "\n",
    "            for key, value in features_spectral.items():\n",
    "                df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name), key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sync[df_sync['subject_id']=='101'].copy()\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(5,5), dpi=100)\n",
    "ts= np.arange(df_sub.shape[0])/Fs\n",
    "ax1.plot(ts, df_sub['O2pulse_cosmedpatch'])\n",
    "ax1.set_ylabel('O2pulse_cosmedpatch\\n[ml/beat]')\n",
    "ax2.plot(ts, df_sub['task'])\n",
    "ax2.set_xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = df_sync[df_sync['subject_id']==101]\n",
    "\n",
    "# for task_name in df_sub['task'].unique():\n",
    "\n",
    "#     df_task = df_sub[df_sub['task']==task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = filter_DFcolumns(df_task.copy(), Fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa['ppg_g_1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filterd = []\n",
    "\n",
    "for subject_id in df_sync['subject_id'].unique():\n",
    "\n",
    "\n",
    "    df_sub = df_sync[df_sync['subject_id']==subject_id]\n",
    "    \n",
    "    for task_name in df_sub['task'].unique():\n",
    "        \n",
    "        df_task = df_sub[df_sub['task']==task_name]\n",
    "        df_task = filter_DFcolumns(df_task.copy(), Fs)\n",
    "        \n",
    "        df_filterd.append(df_task)\n",
    "        \n",
    "df_filterd = pd.concat(df_filterd).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = filter_DFcolumns(df_task.copy(), Fs)\n",
    "\n",
    "# plt.plot(aaa['ppg_r_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correlation = False\n",
    "\n",
    "if show_correlation:\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    for subject_id in df_sync['subject_id'].unique():\n",
    "        fig, ax = plt.subplots(1,1, figsize=(8,8), dpi=80)\n",
    "\n",
    "        df_sub = df_sync[df_sync['subject_id']==subject_id]\n",
    "        df_sub = df_sub[df_sub['Sampled_cosmed']==1]\n",
    "\n",
    "        sns.scatterplot(data=df_sub, x=\"VE_cosmed\", y=\"EE_cosmed\", hue=\"task\", ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check N_samples, focus on important signals, features, output, and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = df_filterd.shape[0]\n",
    "print('there are {} sampels ({:.2f} min) in all subjects'.format(N_samples, N_samples/Fs/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('list_signal', list_signal)\n",
    "print('list_feature', list_feature)\n",
    "print('list_output', list_output)\n",
    "print('list_meta', list_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_sync[list_signal+list_feature+list_output+list_meta]\n",
    "df = df_filterd[list_signal+list_feature+list_output+list_meta]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# windowing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sync['HR_patch'] = 0\n",
    "\n",
    "# for subject_id in df_sync['subject_id'].unique():\n",
    "\n",
    "\n",
    "#     df_sub = df_sync[df_sync['subject_id']==subject_id]\n",
    "#     outputdir_sub = outputdir+subject_id+'/'\n",
    "    \n",
    "#     for task_name in df_sub['task'].unique():\n",
    "        \n",
    "# #         if subject_id!='111':\n",
    "# #             continue\n",
    "# #         if task_name!='Recovery Run':\n",
    "# #             continue\n",
    "#         print('sub: {}, task: {}'.format(subject_id, task_name))\n",
    "#         df_task = df_sub[df_sub['task']==task_name]\n",
    "#         ECG_raw_patch = df_task['ECG'].values\n",
    "        \n",
    "#         fig_name = 'ECG_diagnostics_'+task_name\n",
    "#         QRS_detector_dict_patch = task_HR_detector(ECG_raw_patch, Fs, fig_name=fig_name, outputdir=outputdir_sub, show_plot=False)\n",
    "        \n",
    "#         fig_name = 'physio_diagnostics_'+task_name\n",
    "#         plot_inputsigs(df_task, fig_name=fig_name, outputdir=outputdir_sub, show_plot=False)\n",
    "# #         if subject_id=='102':\n",
    "# #         sys.exit()\n",
    "        \n",
    "#         t_ecg = np.arange(ECG_raw_patch.shape[0])/Fs\n",
    "#         hr_interp = np.interp(t_ecg, QRS_detector_dict_patch['ts_hr'], QRS_detector_dict_patch['hr'])\n",
    "        \n",
    "#         df_sync.loc[(df_sync['subject_id']==subject_id) & (df_sync['task']==task_name), 'HR_patch'] = hr_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick only the stationary segment\n",
    "df_Baseline = df.loc[df['task'].str.contains(\"Baseline\", case=False)]\n",
    "df_Recovery = df.loc[df['task'].str.contains(\"Recovery\", case=False)]\n",
    "\n",
    "df_Movement = df[df['task']=='6MWT']\n",
    "df_Stationary = pd.concat([df_Baseline,df_Recovery, df_Movement])\n",
    "\n",
    "N_samples = df_Stationary.shape[0]\n",
    "print('there are {} sampels ({:.2f} min)'.format(N_samples, N_samples/Fs/60))\n",
    "\n",
    "data_all = []\n",
    "feature_all = []\n",
    "label_all = []\n",
    "meta_all = []\n",
    "\n",
    "\n",
    "\n",
    "for subject_id in df_Stationary['subject_id'].unique():\n",
    "    \n",
    "    print('working on', subject_id)\n",
    "    \n",
    "    obj_cosmed = load_sub('../../data/stage1/sub'+str(subject_id)+'/cosmed/data')\n",
    "    weight = obj_cosmed.Weight\n",
    "\n",
    "    df_sub = df_Stationary[df_Stationary['subject_id']==subject_id].copy()\n",
    "\n",
    "    \n",
    "    task_name = 'Baseline'\n",
    "    df_task = df_sub[df_sub['task']==task_name].reset_index(drop=True).copy()\n",
    "    \n",
    "    # normalize HR based on Altini's method\n",
    "    \n",
    "    HR_baseline = df_task['HR_patch'].mean()\n",
    "    HR_range = df_sub['HR_patch'].max() - df_sub['HR_patch'].min()\n",
    "    \n",
    "    df_sub['HR_patch'] = (df_sub['HR_patch'] - HR_baseline ) / HR_range\n",
    "    \n",
    "    \n",
    "    \n",
    "    for task_name in df_sub['task'].unique():\n",
    "        \n",
    "        \n",
    "        if (subject_id=='110') & (task_name=='Recovery 6MWT-R'):\n",
    "            continue\n",
    "        \n",
    "    #     print(task_name)\n",
    "        if task_name == 'Recovery 5':\n",
    "            continue\n",
    "        df_task = df_sub[df_sub['task']==task_name].reset_index(drop=True).copy()\n",
    "    #     df_task\n",
    "        N = df_task.shape[0]\n",
    "\n",
    "        window_length = int(window_size*Fs)  # windown length in number of samples\n",
    "    #     window_length, overlap\n",
    "        N_overlap = int(window_length*overlap)\n",
    "        # int(60*0.8*Fs) = 4.8 sec * Fs\n",
    "\n",
    "        N_windows = (N-N_overlap) // (window_length-N_overlap)\n",
    "        i_starts = np.arange(0,N_windows)*(window_length-N_overlap)\n",
    "        i_ends = i_starts + window_length\n",
    "        print('\\t{} has {} windows'.format(task_name, N_windows) )\n",
    "\n",
    "#         print(df_task['subject_id'].unique())\n",
    "    #     if 'Run' in task_name:\n",
    "    #         sys.exit()\n",
    "#         sys.exit()\n",
    "    #     print()\n",
    "        for (i_start, i_end) in zip(i_starts, i_ends):\n",
    "            df_window = df_task[i_start:i_end].copy()\n",
    "\n",
    "            data = get_data_condensed(df_window, list_signal, Fs, FS_RESAMPLE_DL)\n",
    "    #         data = df_window[list_input].values\n",
    "            feature = get_feature_condensed(df_window, list_feature)\n",
    "            feature = np.r_[feature, weight]\n",
    "    #         data = get_data_condensed(data)\n",
    "\n",
    "            label = get_label_condensed(df_window, list_output)\n",
    "            meta = get_meta_condensed(df_window, list_meta)\n",
    "\n",
    "    #         sys.exit()\n",
    "\n",
    "    #         label = df_window[list_output].values\n",
    "\n",
    "    #         print(data.shape)\n",
    "            data_all.append(data)\n",
    "            feature_all.append(feature)\n",
    "            label_all.append(label)\n",
    "            meta_all.append(meta)\n",
    "\n",
    "\n",
    "feature_all = np.stack(feature_all)\n",
    "label_all = np.stack(label_all)\n",
    "\n",
    "data_all = np.stack(data_all)\n",
    "data_all = data_all.transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "meta_all = np.stack(meta_all)\n",
    "\n",
    "tasks_dict = {\n",
    "    'Baseline': 0, \n",
    "    'Recovery 6MWT': 1, \n",
    "    'Recovery 6MWT-R': 2, \n",
    "    'Recovery Stair': 3, \n",
    "    'Recovery Walk': 4,\n",
    "    'Recovery Run': 5,\n",
    "    '6MWT': 6,\n",
    "}\n",
    "\n",
    "np.unique(meta_all[:,1])\n",
    "task_temp = np.zeros(meta_all.shape[0])\n",
    "for j, meta_task in enumerate(meta_all[:,1]):\n",
    "    task_temp[j] = tasks_dict[meta_task]\n",
    "\n",
    "meta_all = np.c_[meta_all[:,0].astype(int), task_temp].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_feature.append('weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_all[:,1,:].mean(axis=1, keepdims=True).shape\n",
    "# data_all[:,[1],:].shape\n",
    "\n",
    "# data_all[:,[1],:].mean(axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all[:,i_data,:].mean(axis=-1)[mask_train].shape\n",
    "\n",
    "feature_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "\n",
    "est_concat = []\n",
    "label_concat = []\n",
    "\n",
    "# i_label = list_output.index('VO2_cosmed')\n",
    "# i_data = list_input.index('HR_patch')\n",
    "\n",
    "label_name = ['EE_cosmed']\n",
    "# data_name = ['HR_patch', 'VE_cosmed']\n",
    "# feature_name = ['HR_patch', 'VE_cosmed']\n",
    "feature_name = ['HR_patch', 'VE_cosmed', 'weight']\n",
    "# feature_name = ['HR_patch', 'weight']\n",
    "# feature_name = ['weight']\n",
    "\n",
    "\n",
    "# i_label = list_output.index(label_name)\n",
    "# i_data = list_input.index(data_name)\n",
    "i_label = [i for i,l in enumerate(list_output) if l in label_name]\n",
    "i_feature = [i for i,l in enumerate(list_feature) if l in feature_name]\n",
    "\n",
    "\n",
    "df_performance = []\n",
    "\n",
    "for subject_id in np.unique( meta_all[:,0]):\n",
    "    print('working on sub', subject_id)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "\n",
    "    mask_train = meta_all[:,0]!=subject_id\n",
    "    mask_val = meta_all[:,0]==subject_id\n",
    "    \n",
    "#     mask_train = (meta_all[:,0]==subject_id) & (meta_all[:,1]<3)\n",
    "#     mask_val = (meta_all[:,0]==subject_id) & (meta_all[:,1]>=3) & (meta_all[:,1]!=5)\n",
    "\n",
    "#     mask_train = (meta_all[:,0]!=subject_id) & (meta_all[:,1]!=5)\n",
    "#     mask_val = (meta_all[:,0]==subject_id) & (meta_all[:,1]!=5)\n",
    "\n",
    "\n",
    "\n",
    "#     data_train = data_all[:,i_data,:].mean(axis=1)[mask_train][:,None]\n",
    "    feature_train = feature_all[:,i_feature][mask_train]\n",
    "    label_train = label_all[mask_train, i_label]\n",
    "    meta_train = meta_all[mask_train, :]\n",
    "\n",
    "#     data_val = data_all[:,i_data,:].mean(axis=1)[mask_val][:,None]\n",
    "    feature_val = feature_all[:,i_feature][mask_val]\n",
    "    label_val = label_all[mask_val, i_label]\n",
    "    meta_val = meta_all[mask_val, :]\n",
    "\n",
    "\n",
    "    model = model.fit(feature_train, label_train)\n",
    "    label_est_train = model.predict(feature_train)\n",
    "    label_est_val = model.predict(feature_val)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(label_train, label_est_train))\n",
    "    rmse_val = np.sqrt(mean_squared_error(label_val, label_est_val))\n",
    "#     print('\\t', rmse_train, rmse_val)\n",
    "    \n",
    "    \n",
    "    mae_train, _ = get_MAE(label_train, label_est_train)\n",
    "    mape_train, _ = get_MAPE(label_train, label_est_train)\n",
    "\n",
    "    mae_val, _ = get_MAE(label_val, label_est_val)\n",
    "    mape_val, _ = get_MAPE(label_val, label_est_val)\n",
    "    # rmse_val = np.sqrt(mean_squared_error(SpO2_val, SpO2_est_val))\n",
    "    if subject_id==112:\n",
    "        sys.exit()\n",
    "\n",
    "    Rsquared_train = get_CoeffDeterm(label=label_train, predictions=label_est_train)\n",
    "    Rsquared_val = get_CoeffDeterm(label=label_val, predictions=label_est_val)\n",
    "#     print('\\t', Rsquared_train, Rsquared_val)\n",
    "    \n",
    "\n",
    "    df_performance.append(pd.DataFrame({\n",
    "       'label_est_val': label_est_val, \n",
    "       'label_val': label_val, \n",
    "       'subject_id': meta_val[:,0], \n",
    "       'task_id': meta_val[:,1], \n",
    "        'Rsquared_train': Rsquared_train,\n",
    "        'Rsquared_val': Rsquared_val,\n",
    "        'rmse_train': rmse_train,\n",
    "        'rmse_val': rmse_val,\n",
    "        'mae_train': mae_train,\n",
    "        'mae_val': mae_val,\n",
    "        'mape_train': mape_train,\n",
    "        'mape_val': mape_val,\n",
    "    }))\n",
    "#     est_concat.append(label_est_val)\n",
    "#     label_concat.append(label_val)\n",
    "\n",
    "    # PCC_train, _ = pearsonr(SpO2_train, SpO2_est_train)\n",
    "    # PCC_val, _ = pearsonr(SpO2_val, SpO2_est_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance = pd.concat(df_performance)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,2), dpi=80)\n",
    "\n",
    "ax.plot(df_performance['label_est_val'].values, color='r', alpha=0.5)\n",
    "ax.plot(df_performance['label_val'].values, color='b', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared_val_mean = df_performance[['Rsquared_val', 'subject_id']].drop_duplicates()['Rsquared_val'].mean()\n",
    "Rsquared_val_std = df_performance[['Rsquared_val', 'subject_id']].drop_duplicates()['Rsquared_val'].std()\n",
    "\n",
    "rmse_val_mean = df_performance[['rmse_val', 'subject_id']].drop_duplicates()['rmse_val'].mean()\n",
    "rmse_val_std = df_performance[['rmse_val', 'subject_id']].drop_duplicates()['rmse_val'].std()\n",
    "\n",
    "mae_val_mean = df_performance[['mae_val', 'subject_id']].drop_duplicates()['mae_val'].mean()\n",
    "mae_val_std = df_performance[['mae_val', 'subject_id']].drop_duplicates()['mae_val'].std()\n",
    "\n",
    "mape_val_mean = df_performance[['mape_val', 'subject_id']].drop_duplicates()['mape_val'].mean()\n",
    "mape_val_std = df_performance[['mape_val', 'subject_id']].drop_duplicates()['mape_val'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sub = len(df_performance['subject_id'].unique())\n",
    "\n",
    "N_window = df_performance.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sub, N_window, df_performance['subject_id'].unique(), df_performance['task_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance[['rmse_val', 'subject_id']].drop_duplicates()[['subject_id', 'rmse_val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance[['mape_val', 'subject_id']].drop_duplicates()[['subject_id', 'mape_val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance['task_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = unit_dict[label_name[0].split('_')[0]]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8,), dpi=80)\n",
    "sns.scatterplot(data=df_performance, x='label_val', y='label_est_val', hue='subject_id', ax=ax, palette=subject_palette)\n",
    "# sns.scatterplot(data=df_performance, x='label_val', y='label_est_val', hue='task_id', ax=ax, palette=task_palette)\n",
    "\n",
    "# ax.scatter(df_performance['label_val'].values, df_performance['label_est_val'].values, alpha=0.5)\n",
    "textstr = 'RMSE={:.3f}±{:.3f} {}\\nMAE={:.3f}±{:.3f} {}\\nMAPE={:.3f}±{:.3f} %\\nCoeffDetermination={:.2f}±{:.2f}\\nN_sub = {}\\nN_window = {}' \\\n",
    ".format(rmse_val_mean, rmse_val_std, unit, mae_val_mean, mae_val_std, unit, mape_val_mean*100, mape_val_std*100, Rsquared_val_mean, Rsquared_val_std, N_sub, N_window )\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "fontsize = 20\n",
    "# place a text box in upper left in axes coords\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=fontsize*0.7,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax_min = min(df_performance['label_val'].values.min(), df_performance['label_est_val'].values.min())\n",
    "ax_max = min(df_performance['label_val'].values.max(), df_performance['label_est_val'].values.max())\n",
    "ax.plot([ax_min, ax_max], [ax_min, ax_max])\n",
    "ax.set_xlim([ax_min, ax_max])\n",
    "ax.set_ylim([ax_min, ax_max])\n",
    "ax.legend(bbox_to_anchor=(1.15, 1.05), frameon=True, fontsize=fontsize)\n",
    "ax.set_xlabel('{} Actual\\n{}'.format(label_name[0], unit), fontsize=fontsize)\n",
    "ax.set_ylabel('{} Estiamted\\n{}'.format(label_name[0], unit), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert meta to int array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_all[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_all[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_all[100,2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.shape, meta_all.shape, label_all.shape, feature_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_dict = {\n",
    "    'list_feature': list_feature,\n",
    "    'list_output': list_output,\n",
    "    'list_meta': list_meta,\n",
    "    'list_signal': list_signal,\n",
    "    'FS_RESAMPLE_DL': FS_RESAMPLE_DL,\n",
    "    'subject_ids': np.unique(meta_all[:,0]),\n",
    "    'task_ids': np.unique(meta_all[:,1]),\n",
    "    'window_size': window_size,\n",
    "    'overlap': overlap,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver(stage3_dict, 'stage3_dict', outputdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver(data_all, 'data', outputdir)\n",
    "data_saver(feature_all, 'feature', outputdir)\n",
    "data_saver(label_all, 'label', outputdir)\n",
    "data_saver(meta_all, 'meta', outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 5\n",
    "\n",
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(data_all[j,0,:])\n",
    "plt.show()\n",
    "# plt.plot(ecg_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.where(labels==1)[0]\n",
    "# np.random.shuffle(indices)\n",
    "# indices = indices[0:examples_N]\n",
    "\n",
    "# # get their FT\n",
    "# data_FT = []\n",
    "# for i_plot in range(examples_N):\n",
    "#     v_acc = data[indices[i_plot],:,:]\n",
    "#     yf = np.zeros((v_acc.shape[0],v_acc.shape[1]-1))\n",
    "#     for i_axis in range(v_acc.shape[1]-1): # don't do it for HR\n",
    "#         yf[:,i_axis] = scipy.fftpack.fft(data[indices[i_plot],:,i_axis])\n",
    "#     T = 1/sampling_freq\n",
    "#     N = v_acc.shape[0]\n",
    "#     xf = np.linspace(0.0, 1.0/(2.0*T), N/2)\n",
    "#     data_FT.append({'freq':xf, 'magnitude': yf})\n",
    "    \n",
    "# # plot\n",
    "# plot_indices(data, labels, i_seizures, indices, visual_resultsdir, normalize_window_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all[j, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "v_arr = data_all[j,1,:]\n",
    "# yf = np.zeros(v_arr.shape[0])\n",
    "# for i_axis in range(v_arr.shape[1]-1): # don't do it for HR\n",
    "T = 1/FS_RESAMPLE_DL\n",
    "N = v_arr.shape[0]\n",
    "yf = scipy.fftpack.fft(v_arr)\n",
    "yf = 2.0/N * np.abs(yf[:N//2])\n",
    "\n",
    "xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "# data_FT.append({'freq':xf, 'magnitude': yf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xf, yf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(data_all[j,1,:])\n",
    "plt.show()\n",
    "# plt.plot(ecg_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(100,10))\n",
    "# plt.plot(t_arr, ecg_filt)\n",
    "# plt.plot(time_interp, ecg_filt_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sub.shape, label_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subject specific model, leave-one-task-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.shape, meta_all.shape, label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_4sub(data_sub, meta_sub, label_sub):\n",
    "\n",
    "    for task_id in np.unique(meta_sub[:,1]):\n",
    "\n",
    "        mask_train = meta_all[:,1]!=task_id\n",
    "        mask_val = meta_all[:,1]==task_id\n",
    "\n",
    "\n",
    "        data_train = data_all[mask_train,:,:]\n",
    "        data_val = data_all[mask_val,:,:]\n",
    "\n",
    "        label_train = label_all[mask_train,:]\n",
    "        label_val = label_all[mask_val,:]\n",
    "\n",
    "        meta_train = meta_all[mask_train,:]\n",
    "        meta_val = meta_all[mask_val,:]\n",
    "\n",
    "        outputdir_CV = outputdir_sub + 'CV{}/'.format(task_id)\n",
    "\n",
    "        outputdir_train = outputdir_CV+'train/'\n",
    "        if not os.path.exists(outputdir_train):\n",
    "            os.makedirs(outputdir_train)\n",
    "        data_saver(data_train, 'data', outputdir_train)\n",
    "        data_saver(label_train, 'label', outputdir_train)\n",
    "        data_saver(meta_train, 'meta', outputdir_train)\n",
    "\n",
    "        outputdir_val = outputdir_CV+'val/'\n",
    "        if not os.path.exists(outputdir_val):\n",
    "            os.makedirs(outputdir_val)\n",
    "\n",
    "        data_saver(data_val, 'data', outputdir_val)\n",
    "        data_saver(label_val, 'label', outputdir_val)\n",
    "        data_saver(meta_val, 'meta', outputdir_val)\n",
    "\n",
    "    print('finish saving!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in np.unique(meta_all[:,0]):\n",
    "    outputdir_sub = outputdir+str(subject_id)+'/'\n",
    "    \n",
    "    mask_all = meta_all[:,0]==subject_id\n",
    "    \n",
    "    data_sub = data_all[mask_all, :, :]\n",
    "    meta_sub = meta_all[mask_all, :]\n",
    "    label_sub = label_all[mask_all, :]\n",
    "    split_train_val_4sub(data_sub, meta_sub, label_sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(meta_all[:,0]))\n",
    "print(np.unique(meta_all[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LO_name = 'Recovery 6MWT'\n",
    "# mask_train = meta_sub[:,1]!=LO_name\n",
    "# mask_val = meta_sub[:,1]==LO_name\n",
    "\n",
    "\n",
    "LO_name = 110\n",
    "mask_train = meta_sub[:,0]!=LO_name\n",
    "mask_val = meta_sub[:,0]==LO_name\n",
    "\n",
    "\n",
    "data_train = data_sub[mask_train,:,:]\n",
    "data_val = data_sub[mask_val,:,:]\n",
    "\n",
    "label_train = label_sub[mask_train,:]\n",
    "label_val = label_sub[mask_val,:]\n",
    "\n",
    "meta_train = meta_sub[mask_train,:]\n",
    "meta_val = meta_sub[mask_val,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask_train.sum(), mask_val.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir_train = outputdir+'train/'\n",
    "if not os.path.exists(outputdir_train):\n",
    "    os.makedirs(outputdir_train)\n",
    "\n",
    "data_saver(data_train, 'data', outputdir_train)\n",
    "data_saver(label_train, 'label', outputdir_train)\n",
    "data_saver(meta_train, 'meta', outputdir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir_val = outputdir+'val/'\n",
    "if not os.path.exists(outputdir_val):\n",
    "    os.makedirs(outputdir_val)\n",
    "\n",
    "data_saver(data_val, 'data', outputdir_val)\n",
    "data_saver(label_val, 'label', outputdir_val)\n",
    "data_saver(meta_val, 'meta', outputdir_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "mienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
