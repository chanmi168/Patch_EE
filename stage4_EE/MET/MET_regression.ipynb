{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this notebook parse patch data\n",
    "# TODO: meet with Venu to confirm the changes I made\n",
    "# TODO: figure out the units in COSMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "from math import sin\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from plotting_tools import *\n",
    "from setting import *\n",
    "# from models import *\n",
    "# from models_CNN import *\n",
    "from evaluate import *\n",
    "\n",
    "from stage3_preprocess import *\n",
    "from stage4_regression import *\n",
    "# from training_util import *\n",
    "# from dataset_util import *\n",
    "# from FL_extension.training_util import *\n",
    "from FL_extension.dataset_util import *\n",
    "from FL_extension.evaluation_util import *\n",
    "# from FL_extension.models import *\n",
    "# from FL_extension.models_CNNlight import *\n",
    "from dataIO import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../data/stage3/', output_folder='../../data/stage4/MET_regression/', training_params_file='training_params_baseline.json')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='SpO2_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3/', \n",
    "                          '--output_folder', '../../data/stage4/MET_regression/',\n",
    "                          '--training_params_file', 'training_params_baseline.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training params and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_params_baseline.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26572/3311802745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtraining_params_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtraining_params\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_params_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# include device in training_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_params_baseline.json'"
     ]
    }
   ],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in training_params_list:\n",
    "    # include device in training_params\n",
    "#     device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "#     training_params['device'] = device\n",
    "\n",
    "\n",
    "    training_params['CV_config'] = {\n",
    "        'subject_id': 113,\n",
    "        'task_id': 1,\n",
    "    }\n",
    "    stage3_dict = data_loader('stage3_dict', inputdir).item()\n",
    "    training_params['list_signal'] = stage3_dict['list_signal']\n",
    "    training_params['list_feature'] = stage3_dict['list_feature']\n",
    "    training_params['list_output'] = stage3_dict['list_output']\n",
    "    training_params['list_meta'] = stage3_dict['list_meta']\n",
    "    training_params['FS_RESAMPLE_DL'] = stage3_dict['FS_RESAMPLE_DL']\n",
    "    training_params['subject_ids'] = stage3_dict['subject_ids']\n",
    "    training_params['task_ids'] = stage3_dict['task_ids']\n",
    "    \n",
    "#     input_CV = '../../data/stage3/113/CV2/'\n",
    "#     dataloaders, dataset_sizes = get_loaders(input_CV, training_params)\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['train'].dataset.data.shape)\n",
    "\n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "    \n",
    "    sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "\n",
    "training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_params['CV_config'] = {\n",
    "    'subject_id': 113,\n",
    "    'task_id': 1,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26572/723342181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_params' is not defined"
     ]
    }
   ],
   "source": [
    "training_params['output_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute MET using our own data\n",
    "\n",
    "## sedentary MET=1.5 kcal/kg/h = 1.5/60 kcal/kg/min = 0.025 kcal/kg/min\n",
    "### ref: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4448542/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataloaders['train'].dataset.data\n",
    "\n",
    "# i_feature = training_params['feature_names'].index('weight')\n",
    "# i_label = training_params['output_names'].index('EE_cosmed')\n",
    "\n",
    "\n",
    "# MET_train = dataloaders['train'].dataset.label[:,i_label] / dataloaders['train'].dataset.feature[:,i_feature]\n",
    "# MET_train = MET_train.mean()\n",
    "# print('MET_train is {:.3f} kcal/kg/min'.format(MET_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_df_performance(df_performance, label, label_est, task, subject_id):\n",
    "#     rmse = np.sqrt(mean_squared_error(label, label_est))\n",
    "\n",
    "#     mae, _ = get_MAE(label, label_est)\n",
    "#     mape, _ = get_MAPE(label, label_est)\n",
    "\n",
    "#     Rsquared = get_CoeffDeterm(label=label, predictions=label_est)\n",
    "\n",
    "#     df_performance.append(pd.DataFrame({\n",
    "#         'CV': [subject_id],\n",
    "#         'task': [task],\n",
    "#         'Rsquared': [Rsquared],\n",
    "#         'rmse': [rmse],\n",
    "#         'mae': [mae],\n",
    "#         'mape': [mape],\n",
    "#     }))\n",
    "    \n",
    "#     return df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = training_params['output_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_df_performance(df_performance, label, label_est, task, subject_id):\n",
    "#     rmse = np.sqrt(mean_squared_error(label, label_est))\n",
    "\n",
    "#     mae, _ = get_MAE(label, label_est)\n",
    "#     mape, _ = get_MAPE(label, label_est)\n",
    "\n",
    "#     Rsquared = get_CoeffDeterm(label=label, predictions=label_est)\n",
    "\n",
    "#     df_performance.append(pd.DataFrame({\n",
    "#         'CV': [subject_id],\n",
    "#         'task': [task],\n",
    "#         'Rsquared': [Rsquared],\n",
    "#         'rmse': [rmse],\n",
    "#         'mae': [mae],\n",
    "#         'mape': [mape],\n",
    "#     }))\n",
    "    \n",
    "#     return df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     ordered_subject_ids = np.asarray([115, 107, 113, 110, 101, 104, 106, 121, 212, 102, 103, 111, 114, 116, 118, 119, 120])\n",
    "\n",
    "\n",
    "\n",
    "#     # for subject_id in training_params['subject_ids']:\n",
    "#     for subject_id in ordered_subject_ids:\n",
    "\n",
    "#         training_params['CV_config']['subject_id'] = subject_id\n",
    "\n",
    "#         device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "#         print('using device', device)\n",
    "\n",
    "\n",
    "#         print('using model ', training_params['model_name'])\n",
    "\n",
    "#         model = resp_multiverse(training_params=training_params)\n",
    "# #         print(model)\n",
    "        \n",
    "#         model = model.to(device).float()\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'], weight_decay=0.01)\n",
    "\n",
    "#         training_params['criterion'] = criterion\n",
    "#         training_params['optimizer'] = optimizer\n",
    "#         training_params['inputdir'] = inputdir\n",
    "\n",
    "#         CV_dict = train_model(model, training_params, trainer, evaler, preder)\n",
    "\n",
    "# #             sys.exit()\n",
    "\n",
    "#         plot_losses(CV_dict, outputdir=outputdir, show_plot=False)\n",
    "\n",
    "#         for task in training_params['tasks']:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             label_est_val = CV_dict['performance_dict_val']['out_dict'][task]\n",
    "#             label_val = CV_dict['performance_dict_val']['label_dict'][task]\n",
    "\n",
    "#             label_est_train = CV_dict['performance_dict_train']['out_dict'][task]\n",
    "#             label_train = CV_dict['performance_dict_train']['label_dict'][task]\n",
    "\n",
    "#             # get performance df for training and testing dataset\n",
    "#             df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, subject_id, task), ignore_index=True )\n",
    "#             df_performance_train[task].to_csv(outputdir+'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_train,\n",
    "#                 'label': label_train,\n",
    "#                 'CV': [subject_id]*label_train.shape[0],\n",
    "#                 'task': [task]*label_train.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_train[task].to_csv(outputdir+'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, subject_id, task), ignore_index=True )\n",
    "#             df_performance_val[task].to_csv(outputdir+'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_val,\n",
    "#                 'label': label_val,\n",
    "#                 'CV': [subject_id]*label_val.shape[0],\n",
    "#                 'task': [task]*label_val.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_val[task].to_csv(outputdir+'df_outputlabel_val_{}.csv'.format(task), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_train = {}\n",
    "df_performance_val = {}\n",
    "\n",
    "df_outputlabel_train = {}\n",
    "df_outputlabel_val = {}\n",
    "\n",
    "# for task in training_params['output_names']:\n",
    "task = 'EE_cosmed'\n",
    "#     if 'EE' not in task:\n",
    "#         continue\n",
    "\n",
    "df_performance_train[task] = pd.DataFrame()\n",
    "df_performance_val[task] = pd.DataFrame()\n",
    "\n",
    "df_outputlabel_train[task] = pd.DataFrame()\n",
    "df_outputlabel_val[task] = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(dataloaders['train'].dataset.label[:,i_label])\n",
    "# plt.plot( dataloaders['train'].dataset.label[:,i_label] / dataloaders['train'].dataset.feature[:,i_feature])\n",
    "# # MET_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_performance_train = []\n",
    "# df_performance_val = []\n",
    "\n",
    "# df_outputlabel_train = []\n",
    "# df_outputlabel_val = []\n",
    "\n",
    "\n",
    "training_params['feature_names'] = ['weight']\n",
    "training_params['output_names'] = ['EE_cosmed']\n",
    "\n",
    "for subject_id in training_params['subject_ids']:\n",
    "\n",
    "    training_params['CV_config']['subject_id'] = subject_id\n",
    "\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    \n",
    "    # convert feature back to raw values\n",
    "    \n",
    "    feature_mean = dataloaders['feature_mean']\n",
    "    feature_std = dataloaders['feature_std']\n",
    "\n",
    "    features_train = dataloaders['train'].dataset.feature\n",
    "    features_train = features_train * feature_std + feature_mean\n",
    "\n",
    "    features_val = dataloaders['val'].dataset.feature\n",
    "    features_val = features_val * feature_std + feature_mean\n",
    "    \n",
    "    \n",
    "    i_feature = training_params['feature_names'].index('weight')\n",
    "    i_label = training_params['output_names'].index('EE_cosmed')\n",
    "\n",
    "    MET_train = dataloaders['train'].dataset.label[:,i_label] / features_train[:,i_feature]\n",
    "    MET_train = MET_train.mean()\n",
    "\n",
    "    label_est_train = MET_train*features_train[:,i_feature]\n",
    "    label_train = dataloaders['train'].dataset.label[:,i_label]\n",
    "    \n",
    "    label_est_val = MET_train*features_val[:,i_feature]\n",
    "    label_val = dataloaders['val'].dataset.label[:,i_label]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # get performance df for training and testing dataset\n",
    "    df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, subject_id, task), ignore_index=True )\n",
    "    df_performance_train[task].to_csv(outputdir+'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "    df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "        pd.DataFrame( {\n",
    "        'label_est': label_est_train,\n",
    "        'label': label_train,\n",
    "        'CV': [subject_id]*label_train.shape[0],\n",
    "        'task': [task]*label_train.shape[0]\n",
    "        }), ignore_index=True )\n",
    "\n",
    "    df_outputlabel_train[task].to_csv(outputdir+'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "    df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, subject_id, task), ignore_index=True )\n",
    "    df_performance_val[task].to_csv(outputdir+'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "    df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "        pd.DataFrame( {\n",
    "        'label_est': label_est_val,\n",
    "        'label': label_val,\n",
    "        'CV': [subject_id]*label_val.shape[0],\n",
    "        'task': [task]*label_val.shape[0]\n",
    "        }), ignore_index=True )\n",
    "\n",
    "    df_outputlabel_val[task].to_csv(outputdir+'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            \n",
    "            \n",
    "#     df_performance_train = get_df_performance(label_train, label_est_train, subject_id, task)\n",
    "#     df_performance_val = get_df_performance(label_val, label_est_val, subject_id, task )\n",
    "\n",
    "        \n",
    "#     df_outputlabel_val.append(\n",
    "#         pd.DataFrame( {\n",
    "#         'label_est': label_est_val,\n",
    "#         'label': label_val,\n",
    "#         'CV': subject_id,\n",
    "#         'MET_train': MET_train\n",
    "# #             'task': [task]*label_val.shape[0]\n",
    "#         })\n",
    "#     )\n",
    "#     # df_outputlabel\n",
    "#     df_outputlabel_train.append(\n",
    "#         pd.DataFrame( {\n",
    "#         'label_est': label_est_train,\n",
    "#         'label': label_train,\n",
    "#         'CV': subject_id,\n",
    "#         'MET_train': MET_train\n",
    "\n",
    "# #             'task': [task]*label_train.shape[0]\n",
    "#         })\n",
    "#     )\n",
    "\n",
    "# df_performance_train = pd.concat(df_performance_train)\n",
    "# df_performance_val = pd.concat(df_performance_val)\n",
    "\n",
    "# df_outputlabel_train = pd.concat(df_outputlabel_train)\n",
    "# df_outputlabel_val = pd.concat(df_outputlabel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputlabel = df_outputlabel_val[task]\n",
    "df_performance = df_performance_val[task]\n",
    "\n",
    "N_sub = len(df_outputlabel['CV'].unique())\n",
    "N_samples = df_outputlabel.shape[0]\n",
    "t_dur = N_samples*3/60\n",
    "PCC = get_PCC(df_outputlabel['label'], df_outputlabel['label_est'])\n",
    "MAE = get_MAE(df_outputlabel['label'], df_outputlabel['label_est'])[0]\n",
    "RMSE = get_RMSE(df_outputlabel['label'], df_outputlabel['label_est'])\n",
    "MAPE = get_MAPE(df_outputlabel['label'], df_outputlabel['label_est'])[0]\n",
    "\n",
    "Rsquared = get_CoeffDeterm(df_outputlabel['label'], df_outputlabel['label_est'])\n",
    "\n",
    "label_range = [df_outputlabel['label'].min(), df_outputlabel['label'].max()]\n",
    "task_name = task.split('_')[0]\n",
    "\n",
    "title_str = '{} range: {:.1f}-{:.1f} {}'.format(task.split('_')[0], label_range[0], label_range[1], unit_dict[task_name])\n",
    "textstr = 'RMSE={:.2f} {}\\nMAE={:.2f} {}\\nMAPE={:.2f} {}\\nPCC={:.2f}\\nR2={:.2f}\\nN_sub={}\\nN_samples={}\\nduration={:.2f} min'.format(\n",
    "    RMSE, unit_dict[task_name], MAE, unit_dict[task_name], MAPE*100, '%', PCC, Rsquared,\n",
    "    N_sub, N_samples, t_dur)\n",
    "\n",
    "print(textstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_performance_val['rmse'].mean(), df_performance_val['mae'].mean(), df_performance_val['mape'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_performance_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# def plot_regression_partial(ax, subjects_performance_dict, subject_id_plt, training_params,outputdir=None, show_plot=False, log_wandb=False):\n",
    "\n",
    "#     agg_performance_dict = aggregate_performance_dict(subjects_performance_dict)\n",
    "\n",
    "#     props = dict(boxstyle='round,pad=0.7', facecolor='white', edgecolor='black', alpha=0.7)\n",
    "#     fontsize = 16\n",
    "\n",
    "#     subject_ids = list(subjects_performance_dict.keys())\n",
    "#     N_beats_val = 0\n",
    "\n",
    "\n",
    "#     for subject_id in subject_ids:\n",
    "#         if int(subject_id)//100 == 0:\n",
    "#             marker = marker_dict['x']\n",
    "#         elif int(subject_id)//100 == 1:\n",
    "#             marker = marker_dict['circle']\n",
    "\n",
    "#         performance_dict = subjects_performance_dict[subject_id]\n",
    "\n",
    "#         SpO2_val = performance_dict['SpO2_val']\n",
    "#         SpO2_est_val = performance_dict['SpO2_est_val']\n",
    "\n",
    "\n",
    "#         color = color_dict[color_names[int(subject_id)%100]]\n",
    "\n",
    "#         if subject_id == subject_id_plt:\n",
    "#             alpha=0.6\n",
    "#             ax.set_title('{}'.format(subject_id), fontsize=fontsize+5)\n",
    "#             rmse_val = performance_dict['rmse_val']\n",
    "#         else:\n",
    "#             alpha=0.03\n",
    "\n",
    "#         ax.scatter(SpO2_val, SpO2_est_val, alpha=alpha, color=color, marker=marker)\n",
    "\n",
    "#         N_beats_val += SpO2_val.shape[0]\n",
    "        \n",
    "    \n",
    "#     performance_dict = subjects_performance_dict[subject_id_plt]\n",
    "# #     print(training_params['use_calibration'], subjects_performance_dict.keys())\n",
    "#     if training_params['use_calibration']:\n",
    "#         if 'SpO2_cal_range' in performance_dict:\n",
    "#             SpO2_cal_range = performance_dict['SpO2_cal_range']\n",
    "#             subject_id_plt = str(subject_id_plt) + '\\n[' + r'$SpO_{2}$' + ' range: {:.1f}-{:.1f}%]'.format(SpO2_cal_range[0], SpO2_cal_range[1])\n",
    "#     ax_conditioning2(ax, subject_id_plt, fontsize, props, 'RMSE={:.2f}%'.format(rmse_val))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# def ax_conditioning(ax, title_str, fontsize, props, textstr):\n",
    "#     ax.set_title(title_str, fontsize=fontsize+5)\n",
    "\n",
    "#     ax.set_ylabel(r'$SpO_{2}$'+' estimated (%)', fontsize=fontsize)\n",
    "#     ax.set_xlabel(r'$SpO_{2}$'+' label (%)', fontsize=fontsize)\n",
    "\n",
    "#     major_ticks = np.arange(label_range_dict['SpO2'][0],label_range_dict['SpO2'][1]+1,5)\n",
    "#     minor_ticks = np.arange(label_range_dict['SpO2'][0],label_range_dict['SpO2'][1]+1)\n",
    "\n",
    "#     ax.set_xticks(major_ticks)\n",
    "#     ax.set_xticks(minor_ticks, minor=True)\n",
    "#     ax.set_yticks(major_ticks)\n",
    "#     ax.set_yticks(minor_ticks, minor=True)\n",
    "#     # Or if you want different settings for the grids:\n",
    "#     ax.grid(which='minor', alpha=0.3)\n",
    "#     ax.grid(which='major', alpha=0.8)\n",
    "\n",
    "\n",
    "#     ax.plot( label_range_dict['SpO2'],label_range_dict['SpO2'] , color='gray', alpha=0.5)\n",
    "# #     ax.legend(loc='upper left', frameon=True, fontsize=fontsize-7)\n",
    "\n",
    "\n",
    "#     # place a text box in bottom right in axes coords\n",
    "# #     ax.text(0.5, 0.08, textstr, transform=ax.transAxes, fontsize=fontsize-5,\n",
    "# #     verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
    "#     ax.set_ylim(label_range_dict['SpO2'])\n",
    "#     ax.set_xlim(label_range_dict['SpO2'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(data=aaa, x='label', y='label_est', hue='CV', palette=subject_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ax_no_top_right(ax):\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "mienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
