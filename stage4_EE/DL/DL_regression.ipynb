{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "from math import sin\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from plotting_tools import *\n",
    "from setting import *\n",
    "\n",
    "# from EE_extension.dataset_util import *\n",
    "\n",
    "from EE_extension.models import *\n",
    "# from models_CNN import *\n",
    "# from models_CNN2 import *\n",
    "from EE_extension.models_resnet import *\n",
    "from EE_extension.dataset_util import *\n",
    "from EE_extension.training_util import *\n",
    "from EE_extension.evaluation_util import *\n",
    "\n",
    "from evaluate import *\n",
    "\n",
    "from stage3_preprocess import *\n",
    "from stage4_regression import *\n",
    "from dataIO import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_regressor_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24940/3122217902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_regressor_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_regressor_names' is not defined"
     ]
    }
   ],
   "source": [
    "# get_regressor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../../data/stage3/win60_overlap90/', output_folder='../../../data/stage4_TEST/', subject_id='101', training_params_file='training_params_baseline.json')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='SpO2_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--subject_id', metavar='subject_id', help='subject_id',\n",
    "                    default='101')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "args = parser.parse_args(['--input_folder', '../../../data/stage3/win60_overlap90/', \n",
    "                          '--output_folder', '../../../data/stage4/DL/TEST/',\n",
    "                          '--training_params_file', 'training_params_baseline.json',\n",
    "                          # '--training_params_file', 'training_params_dummy.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2023-Feb-13 23:38:59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_start = datetime.now(tz_NY)\n",
    "print(\"start time:\", datetime_start.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training params and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions are: (2399, 3, 6000)\n"
     ]
    }
   ],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in [training_params_list[0]]:\n",
    "    # include device in training_params\n",
    "    device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "    training_params['device'] = device\n",
    "    \n",
    "    if 'training_mode' in training_params:\n",
    "        training_mode = training_params['training_mode']\n",
    "    else:\n",
    "        training_params = 'subject_ind'\n",
    "\n",
    "    training_params['CV_config'] = {\n",
    "        'subject_id': 113,\n",
    "        'task_id': 5,\n",
    "    }\n",
    "    stage3_dict = data_loader('stage3_dict', inputdir).item()\n",
    "    training_params['list_signal'] = stage3_dict['list_signal']\n",
    "    training_params['list_feature'] = stage3_dict['list_feature']\n",
    "    training_params['list_output'] = stage3_dict['list_output']\n",
    "    training_params['list_meta'] = stage3_dict['list_meta']\n",
    "    training_params['FS_RESAMPLE_DL'] = stage3_dict['FS_RESAMPLE_DL']\n",
    "    training_params['subject_ids'] = stage3_dict['subject_ids']\n",
    "    training_params['task_ids'] = stage3_dict['task_ids']\n",
    "    training_params['sequence'] = stage3_dict['sequence']\n",
    "    \n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['train'].dataset.data.shape)\n",
    "\n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "    \n",
    "    sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "    if training_params['model_name'] == 'FeatureExtractor_CNN':\n",
    "        training_params['featrue_extractor'] = FeatureExtractor_CNN\n",
    "    elif training_params['model_name'] == 'ResNet1D':\n",
    "        training_params['featrue_extractor'] = ResNet1D\n",
    "    elif training_params['model_name'] == 'FeatureExtractor_CNN2':\n",
    "        training_params['featrue_extractor'] = FeatureExtractor_CNN2\n",
    "\n",
    "    training_params['ordered_subject_ids'] = np.asarray(training_params['ordered_subject_ids'])\n",
    "\n",
    "    \n",
    "    training_params['inputdir'] = inputdir\n",
    "    training_params['outputdir'] = outputdir\n",
    "    \n",
    "    training_params = get_regressor_names(training_params)\n",
    "\n",
    "    if 'LSTM' in training_params['model_name']:\n",
    "        training_params = change_output_dim(training_params)\n",
    "\n",
    "    \n",
    "    del dataloaders\n",
    "# training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train, feature_train, label_train, meta_train = get_samples(inputdir, 'train/', training_params)\n",
    "# data_train.shape, feature_train.shape, label_train.shape, meta_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = outputdir+training_params['model_name']+'/'\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define trainer, evaler, preder functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_resnet\n",
    "evaler = eval_resnet\n",
    "preder = pred_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some random param\n",
    "## kernel_size = 5 ~ 16*100/300 (16 for 300 Hz sampling rate, 5 for 100 sampling rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['downsample_gap']\n",
    "\n",
    "# # n_block = training_params['n_block_macro'][0] * training_params['downsample_gap'][0]\n",
    "# # downsample_gap = training_params['downsample_gap'][0]\n",
    "# # base_filters = training_params['base_filters'][0]\n",
    "# # use_sc =  training_params['use_sc']\n",
    "# # increasefilter_gap = downsample_gap * 2\n",
    "# # kernel_size = training_params['kernel_size']\n",
    "# # groups = training_params['base_filters'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['model_name'] == 'ResNet1D':\n",
    "# reference: https://github.com/hsd1503/resnet1d/blob/master/test_physionet.py\n",
    "    training_params['base_filters'] = training_params['channel_n'] # [64] \n",
    "    training_params['in_channels'] = training_params['data_dimensions']\n",
    "#     training_params['increasefilter_gap'] = [training_params['downsample_gap'][0] * 2]\n",
    "    training_params['n_block'] = training_params['n_block_macro'] * training_params['downsample_gap']\n",
    "    training_params['increasefilter_gap'] = training_params['downsample_gap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kernel_size = 10 # ~ 16*100/300 (16 for 300 Hz sampling rate, 5 for 100 sampling rate)\n",
    "\n",
    "# training_params['stride'] = 2\n",
    "# # stride = 2\n",
    "# # increasefilter_gap = 12\n",
    "# # base_filters = 32\n",
    "# training_params['n_classes'] = 1\n",
    "# # n_classes = 1 # regression\n",
    "# training_params['groups'] = 32\n",
    "# # groups = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6000]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params['data_dimensions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: need to fix summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resp_multiverse(training_params=training_params)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# # print([n for n, _ in model.named_children()])\n",
    "# try:\n",
    "# #     model.regressors['EErq_cosmed'].ch_pooling.register_forward_hook(get_activation('aaa'+'_layer_pooling'))\n",
    "#     print(model.feature_extractors['ppg_ir_1_cardiac'].final_ch_pooling)\n",
    "# #     model.feature_extractors['ppg_ir_1_cardiac'].final_ch_pooling.register_forward_hook(get_activation('j'+'_layer_pool'))\n",
    "\n",
    "# except:\n",
    "#     print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_test(training_params):\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    # prepare model\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    model = model.to(device).float()\n",
    "\n",
    "    # prepare data\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    data = torch.from_numpy(data)\n",
    "\n",
    "    feature = dataloaders['val'].dataset.feature[:5,:]\n",
    "    feature = torch.from_numpy(feature)\n",
    "\n",
    "    label = dataloaders['val'].dataset.label[:5,:]\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    data = data.to(device=device, dtype=torch.float)\n",
    "    feature = feature.to(device=device, dtype=torch.float)\n",
    "    label = label.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model inference\n",
    "    out = model(data, feature)\n",
    "\n",
    "    # compute loss\n",
    "    criterion = MultiTaskLoss(training_params)\n",
    "    losses = criterion(out, label)\n",
    "\n",
    "    # check losses\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model  ResNet1D\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 4, 6000]              44\n",
      "   MyConv1dPadSame-2              [-1, 4, 6000]               0\n",
      "       BatchNorm1d-3              [-1, 4, 6000]               8\n",
      "              ReLU-4              [-1, 4, 6000]               0\n",
      "            Conv1d-5              [-1, 4, 6000]             164\n",
      "   MyConv1dPadSame-6              [-1, 4, 6000]               0\n",
      "       BatchNorm1d-7              [-1, 4, 6000]               8\n",
      "              ReLU-8              [-1, 4, 6000]               0\n",
      "           Dropout-9              [-1, 4, 6000]               0\n",
      "           Conv1d-10              [-1, 4, 6000]             164\n",
      "  MyConv1dPadSame-11              [-1, 4, 6000]               0\n",
      "       BasicBlock-12              [-1, 4, 6000]               0\n",
      "      BatchNorm1d-13              [-1, 4, 6000]               8\n",
      "             ReLU-14              [-1, 4, 6000]               0\n",
      "          Dropout-15              [-1, 4, 6000]               0\n",
      "           Conv1d-16              [-1, 4, 6000]             164\n",
      "  MyConv1dPadSame-17              [-1, 4, 6000]               0\n",
      "      BatchNorm1d-18              [-1, 4, 6000]               8\n",
      "             ReLU-19              [-1, 4, 6000]               0\n",
      "          Dropout-20              [-1, 4, 6000]               0\n",
      "           Conv1d-21              [-1, 4, 6000]             164\n",
      "  MyConv1dPadSame-22              [-1, 4, 6000]               0\n",
      "       BasicBlock-23              [-1, 4, 6000]               0\n",
      "      BatchNorm1d-24              [-1, 4, 6000]               8\n",
      "             ReLU-25              [-1, 4, 6000]               0\n",
      "          Dropout-26              [-1, 4, 6000]               0\n",
      "           Conv1d-27              [-1, 8, 3000]             328\n",
      "  MyConv1dPadSame-28              [-1, 8, 3000]               0\n",
      "      BatchNorm1d-29              [-1, 8, 3000]              16\n",
      "             ReLU-30              [-1, 8, 3000]               0\n",
      "          Dropout-31              [-1, 8, 3000]               0\n",
      "           Conv1d-32              [-1, 8, 3000]             648\n",
      "  MyConv1dPadSame-33              [-1, 8, 3000]               0\n",
      "        AvgPool1d-34              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-35              [-1, 4, 3000]               0\n",
      "       BasicBlock-36              [-1, 8, 3000]               0\n",
      "      BatchNorm1d-37              [-1, 8, 3000]              16\n",
      "             ReLU-38              [-1, 8, 3000]               0\n",
      "          Dropout-39              [-1, 8, 3000]               0\n",
      "           Conv1d-40              [-1, 8, 3000]             648\n",
      "  MyConv1dPadSame-41              [-1, 8, 3000]               0\n",
      "      BatchNorm1d-42              [-1, 8, 3000]              16\n",
      "             ReLU-43              [-1, 8, 3000]               0\n",
      "          Dropout-44              [-1, 8, 3000]               0\n",
      "           Conv1d-45              [-1, 8, 3000]             648\n",
      "  MyConv1dPadSame-46              [-1, 8, 3000]               0\n",
      "       BasicBlock-47              [-1, 8, 3000]               0\n",
      "      BatchNorm1d-48              [-1, 8, 3000]              16\n",
      "             ReLU-49              [-1, 8, 3000]               0\n",
      "          Dropout-50              [-1, 8, 3000]               0\n",
      "           Conv1d-51             [-1, 16, 1500]           1,296\n",
      "  MyConv1dPadSame-52             [-1, 16, 1500]               0\n",
      "      BatchNorm1d-53             [-1, 16, 1500]              32\n",
      "             ReLU-54             [-1, 16, 1500]               0\n",
      "          Dropout-55             [-1, 16, 1500]               0\n",
      "           Conv1d-56             [-1, 16, 1500]           2,576\n",
      "  MyConv1dPadSame-57             [-1, 16, 1500]               0\n",
      "        AvgPool1d-58              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-59              [-1, 8, 1500]               0\n",
      "       BasicBlock-60             [-1, 16, 1500]               0\n",
      "      BatchNorm1d-61             [-1, 16, 1500]              32\n",
      "             ReLU-62             [-1, 16, 1500]               0\n",
      "          Dropout-63             [-1, 16, 1500]               0\n",
      "           Conv1d-64             [-1, 16, 1500]           2,576\n",
      "  MyConv1dPadSame-65             [-1, 16, 1500]               0\n",
      "      BatchNorm1d-66             [-1, 16, 1500]              32\n",
      "             ReLU-67             [-1, 16, 1500]               0\n",
      "          Dropout-68             [-1, 16, 1500]               0\n",
      "           Conv1d-69             [-1, 16, 1500]           2,576\n",
      "  MyConv1dPadSame-70             [-1, 16, 1500]               0\n",
      "       BasicBlock-71             [-1, 16, 1500]               0\n",
      "      BatchNorm1d-72             [-1, 16, 1500]              32\n",
      "             ReLU-73             [-1, 16, 1500]               0\n",
      "          Dropout-74             [-1, 16, 1500]               0\n",
      "           Conv1d-75              [-1, 32, 750]           5,152\n",
      "  MyConv1dPadSame-76              [-1, 32, 750]               0\n",
      "      BatchNorm1d-77              [-1, 32, 750]              64\n",
      "             ReLU-78              [-1, 32, 750]               0\n",
      "          Dropout-79              [-1, 32, 750]               0\n",
      "           Conv1d-80              [-1, 32, 750]          10,272\n",
      "  MyConv1dPadSame-81              [-1, 32, 750]               0\n",
      "        AvgPool1d-82              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-83              [-1, 16, 750]               0\n",
      "       BasicBlock-84              [-1, 32, 750]               0\n",
      "      BatchNorm1d-85              [-1, 32, 750]              64\n",
      "             ReLU-86              [-1, 32, 750]               0\n",
      "          Dropout-87              [-1, 32, 750]               0\n",
      "           Conv1d-88              [-1, 32, 750]          10,272\n",
      "  MyConv1dPadSame-89              [-1, 32, 750]               0\n",
      "      BatchNorm1d-90              [-1, 32, 750]              64\n",
      "             ReLU-91              [-1, 32, 750]               0\n",
      "          Dropout-92              [-1, 32, 750]               0\n",
      "           Conv1d-93              [-1, 32, 750]          10,272\n",
      "  MyConv1dPadSame-94              [-1, 32, 750]               0\n",
      "       BasicBlock-95              [-1, 32, 750]               0\n",
      "      BatchNorm1d-96              [-1, 32, 750]              64\n",
      "             ReLU-97              [-1, 32, 750]               0\n",
      "          Dropout-98              [-1, 32, 750]               0\n",
      "           Conv1d-99              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-100              [-1, 64, 375]               0\n",
      "     BatchNorm1d-101              [-1, 64, 375]             128\n",
      "            ReLU-102              [-1, 64, 375]               0\n",
      "         Dropout-103              [-1, 64, 375]               0\n",
      "          Conv1d-104              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-105              [-1, 64, 375]               0\n",
      "       AvgPool1d-106              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-107              [-1, 32, 375]               0\n",
      "      BasicBlock-108              [-1, 64, 375]               0\n",
      "     BatchNorm1d-109              [-1, 64, 375]             128\n",
      "            ReLU-110              [-1, 64, 375]               0\n",
      "         Dropout-111              [-1, 64, 375]               0\n",
      "          Conv1d-112              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-113              [-1, 64, 375]               0\n",
      "     BatchNorm1d-114              [-1, 64, 375]             128\n",
      "            ReLU-115              [-1, 64, 375]               0\n",
      "         Dropout-116              [-1, 64, 375]               0\n",
      "          Conv1d-117              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-118              [-1, 64, 375]               0\n",
      "      BasicBlock-119              [-1, 64, 375]               0\n",
      "     BatchNorm1d-120              [-1, 64, 375]             128\n",
      "            ReLU-121              [-1, 64, 375]               0\n",
      "         Dropout-122              [-1, 64, 375]               0\n",
      "          Conv1d-123               [-1, 1, 375]              65\n",
      "        ResNet1D-124                  [-1, 375]               0\n",
      "          Conv1d-125              [-1, 4, 6000]              44\n",
      " MyConv1dPadSame-126              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-127              [-1, 4, 6000]               8\n",
      "            ReLU-128              [-1, 4, 6000]               0\n",
      "          Conv1d-129              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-130              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-131              [-1, 4, 6000]               8\n",
      "            ReLU-132              [-1, 4, 6000]               0\n",
      "         Dropout-133              [-1, 4, 6000]               0\n",
      "          Conv1d-134              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-135              [-1, 4, 6000]               0\n",
      "      BasicBlock-136              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-137              [-1, 4, 6000]               8\n",
      "            ReLU-138              [-1, 4, 6000]               0\n",
      "         Dropout-139              [-1, 4, 6000]               0\n",
      "          Conv1d-140              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-141              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-142              [-1, 4, 6000]               8\n",
      "            ReLU-143              [-1, 4, 6000]               0\n",
      "         Dropout-144              [-1, 4, 6000]               0\n",
      "          Conv1d-145              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-146              [-1, 4, 6000]               0\n",
      "      BasicBlock-147              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-148              [-1, 4, 6000]               8\n",
      "            ReLU-149              [-1, 4, 6000]               0\n",
      "         Dropout-150              [-1, 4, 6000]               0\n",
      "          Conv1d-151              [-1, 8, 3000]             328\n",
      " MyConv1dPadSame-152              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-153              [-1, 8, 3000]              16\n",
      "            ReLU-154              [-1, 8, 3000]               0\n",
      "         Dropout-155              [-1, 8, 3000]               0\n",
      "          Conv1d-156              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-157              [-1, 8, 3000]               0\n",
      "       AvgPool1d-158              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-159              [-1, 4, 3000]               0\n",
      "      BasicBlock-160              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-161              [-1, 8, 3000]              16\n",
      "            ReLU-162              [-1, 8, 3000]               0\n",
      "         Dropout-163              [-1, 8, 3000]               0\n",
      "          Conv1d-164              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-165              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-166              [-1, 8, 3000]              16\n",
      "            ReLU-167              [-1, 8, 3000]               0\n",
      "         Dropout-168              [-1, 8, 3000]               0\n",
      "          Conv1d-169              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-170              [-1, 8, 3000]               0\n",
      "      BasicBlock-171              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-172              [-1, 8, 3000]              16\n",
      "            ReLU-173              [-1, 8, 3000]               0\n",
      "         Dropout-174              [-1, 8, 3000]               0\n",
      "          Conv1d-175             [-1, 16, 1500]           1,296\n",
      " MyConv1dPadSame-176             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-177             [-1, 16, 1500]              32\n",
      "            ReLU-178             [-1, 16, 1500]               0\n",
      "         Dropout-179             [-1, 16, 1500]               0\n",
      "          Conv1d-180             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-181             [-1, 16, 1500]               0\n",
      "       AvgPool1d-182              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-183              [-1, 8, 1500]               0\n",
      "      BasicBlock-184             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-185             [-1, 16, 1500]              32\n",
      "            ReLU-186             [-1, 16, 1500]               0\n",
      "         Dropout-187             [-1, 16, 1500]               0\n",
      "          Conv1d-188             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-189             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-190             [-1, 16, 1500]              32\n",
      "            ReLU-191             [-1, 16, 1500]               0\n",
      "         Dropout-192             [-1, 16, 1500]               0\n",
      "          Conv1d-193             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-194             [-1, 16, 1500]               0\n",
      "      BasicBlock-195             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-196             [-1, 16, 1500]              32\n",
      "            ReLU-197             [-1, 16, 1500]               0\n",
      "         Dropout-198             [-1, 16, 1500]               0\n",
      "          Conv1d-199              [-1, 32, 750]           5,152\n",
      " MyConv1dPadSame-200              [-1, 32, 750]               0\n",
      "     BatchNorm1d-201              [-1, 32, 750]              64\n",
      "            ReLU-202              [-1, 32, 750]               0\n",
      "         Dropout-203              [-1, 32, 750]               0\n",
      "          Conv1d-204              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-205              [-1, 32, 750]               0\n",
      "       AvgPool1d-206              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-207              [-1, 16, 750]               0\n",
      "      BasicBlock-208              [-1, 32, 750]               0\n",
      "     BatchNorm1d-209              [-1, 32, 750]              64\n",
      "            ReLU-210              [-1, 32, 750]               0\n",
      "         Dropout-211              [-1, 32, 750]               0\n",
      "          Conv1d-212              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-213              [-1, 32, 750]               0\n",
      "     BatchNorm1d-214              [-1, 32, 750]              64\n",
      "            ReLU-215              [-1, 32, 750]               0\n",
      "         Dropout-216              [-1, 32, 750]               0\n",
      "          Conv1d-217              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-218              [-1, 32, 750]               0\n",
      "      BasicBlock-219              [-1, 32, 750]               0\n",
      "     BatchNorm1d-220              [-1, 32, 750]              64\n",
      "            ReLU-221              [-1, 32, 750]               0\n",
      "         Dropout-222              [-1, 32, 750]               0\n",
      "          Conv1d-223              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-224              [-1, 64, 375]               0\n",
      "     BatchNorm1d-225              [-1, 64, 375]             128\n",
      "            ReLU-226              [-1, 64, 375]               0\n",
      "         Dropout-227              [-1, 64, 375]               0\n",
      "          Conv1d-228              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-229              [-1, 64, 375]               0\n",
      "       AvgPool1d-230              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-231              [-1, 32, 375]               0\n",
      "      BasicBlock-232              [-1, 64, 375]               0\n",
      "     BatchNorm1d-233              [-1, 64, 375]             128\n",
      "            ReLU-234              [-1, 64, 375]               0\n",
      "         Dropout-235              [-1, 64, 375]               0\n",
      "          Conv1d-236              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-237              [-1, 64, 375]               0\n",
      "     BatchNorm1d-238              [-1, 64, 375]             128\n",
      "            ReLU-239              [-1, 64, 375]               0\n",
      "         Dropout-240              [-1, 64, 375]               0\n",
      "          Conv1d-241              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-242              [-1, 64, 375]               0\n",
      "      BasicBlock-243              [-1, 64, 375]               0\n",
      "     BatchNorm1d-244              [-1, 64, 375]             128\n",
      "            ReLU-245              [-1, 64, 375]               0\n",
      "         Dropout-246              [-1, 64, 375]               0\n",
      "          Conv1d-247               [-1, 1, 375]              65\n",
      "        ResNet1D-248                  [-1, 375]               0\n",
      "          Conv1d-249              [-1, 4, 6000]              44\n",
      " MyConv1dPadSame-250              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-251              [-1, 4, 6000]               8\n",
      "            ReLU-252              [-1, 4, 6000]               0\n",
      "          Conv1d-253              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-254              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-255              [-1, 4, 6000]               8\n",
      "            ReLU-256              [-1, 4, 6000]               0\n",
      "         Dropout-257              [-1, 4, 6000]               0\n",
      "          Conv1d-258              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-259              [-1, 4, 6000]               0\n",
      "      BasicBlock-260              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-261              [-1, 4, 6000]               8\n",
      "            ReLU-262              [-1, 4, 6000]               0\n",
      "         Dropout-263              [-1, 4, 6000]               0\n",
      "          Conv1d-264              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-265              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-266              [-1, 4, 6000]               8\n",
      "            ReLU-267              [-1, 4, 6000]               0\n",
      "         Dropout-268              [-1, 4, 6000]               0\n",
      "          Conv1d-269              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-270              [-1, 4, 6000]               0\n",
      "      BasicBlock-271              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-272              [-1, 4, 6000]               8\n",
      "            ReLU-273              [-1, 4, 6000]               0\n",
      "         Dropout-274              [-1, 4, 6000]               0\n",
      "          Conv1d-275              [-1, 8, 3000]             328\n",
      " MyConv1dPadSame-276              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-277              [-1, 8, 3000]              16\n",
      "            ReLU-278              [-1, 8, 3000]               0\n",
      "         Dropout-279              [-1, 8, 3000]               0\n",
      "          Conv1d-280              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-281              [-1, 8, 3000]               0\n",
      "       AvgPool1d-282              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-283              [-1, 4, 3000]               0\n",
      "      BasicBlock-284              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-285              [-1, 8, 3000]              16\n",
      "            ReLU-286              [-1, 8, 3000]               0\n",
      "         Dropout-287              [-1, 8, 3000]               0\n",
      "          Conv1d-288              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-289              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-290              [-1, 8, 3000]              16\n",
      "            ReLU-291              [-1, 8, 3000]               0\n",
      "         Dropout-292              [-1, 8, 3000]               0\n",
      "          Conv1d-293              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-294              [-1, 8, 3000]               0\n",
      "      BasicBlock-295              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-296              [-1, 8, 3000]              16\n",
      "            ReLU-297              [-1, 8, 3000]               0\n",
      "         Dropout-298              [-1, 8, 3000]               0\n",
      "          Conv1d-299             [-1, 16, 1500]           1,296\n",
      " MyConv1dPadSame-300             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-301             [-1, 16, 1500]              32\n",
      "            ReLU-302             [-1, 16, 1500]               0\n",
      "         Dropout-303             [-1, 16, 1500]               0\n",
      "          Conv1d-304             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-305             [-1, 16, 1500]               0\n",
      "       AvgPool1d-306              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-307              [-1, 8, 1500]               0\n",
      "      BasicBlock-308             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-309             [-1, 16, 1500]              32\n",
      "            ReLU-310             [-1, 16, 1500]               0\n",
      "         Dropout-311             [-1, 16, 1500]               0\n",
      "          Conv1d-312             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-313             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-314             [-1, 16, 1500]              32\n",
      "            ReLU-315             [-1, 16, 1500]               0\n",
      "         Dropout-316             [-1, 16, 1500]               0\n",
      "          Conv1d-317             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-318             [-1, 16, 1500]               0\n",
      "      BasicBlock-319             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-320             [-1, 16, 1500]              32\n",
      "            ReLU-321             [-1, 16, 1500]               0\n",
      "         Dropout-322             [-1, 16, 1500]               0\n",
      "          Conv1d-323              [-1, 32, 750]           5,152\n",
      " MyConv1dPadSame-324              [-1, 32, 750]               0\n",
      "     BatchNorm1d-325              [-1, 32, 750]              64\n",
      "            ReLU-326              [-1, 32, 750]               0\n",
      "         Dropout-327              [-1, 32, 750]               0\n",
      "          Conv1d-328              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-329              [-1, 32, 750]               0\n",
      "       AvgPool1d-330              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-331              [-1, 16, 750]               0\n",
      "      BasicBlock-332              [-1, 32, 750]               0\n",
      "     BatchNorm1d-333              [-1, 32, 750]              64\n",
      "            ReLU-334              [-1, 32, 750]               0\n",
      "         Dropout-335              [-1, 32, 750]               0\n",
      "          Conv1d-336              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-337              [-1, 32, 750]               0\n",
      "     BatchNorm1d-338              [-1, 32, 750]              64\n",
      "            ReLU-339              [-1, 32, 750]               0\n",
      "         Dropout-340              [-1, 32, 750]               0\n",
      "          Conv1d-341              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-342              [-1, 32, 750]               0\n",
      "      BasicBlock-343              [-1, 32, 750]               0\n",
      "     BatchNorm1d-344              [-1, 32, 750]              64\n",
      "            ReLU-345              [-1, 32, 750]               0\n",
      "         Dropout-346              [-1, 32, 750]               0\n",
      "          Conv1d-347              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-348              [-1, 64, 375]               0\n",
      "     BatchNorm1d-349              [-1, 64, 375]             128\n",
      "            ReLU-350              [-1, 64, 375]               0\n",
      "         Dropout-351              [-1, 64, 375]               0\n",
      "          Conv1d-352              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-353              [-1, 64, 375]               0\n",
      "       AvgPool1d-354              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-355              [-1, 32, 375]               0\n",
      "      BasicBlock-356              [-1, 64, 375]               0\n",
      "     BatchNorm1d-357              [-1, 64, 375]             128\n",
      "            ReLU-358              [-1, 64, 375]               0\n",
      "         Dropout-359              [-1, 64, 375]               0\n",
      "          Conv1d-360              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-361              [-1, 64, 375]               0\n",
      "     BatchNorm1d-362              [-1, 64, 375]             128\n",
      "            ReLU-363              [-1, 64, 375]               0\n",
      "         Dropout-364              [-1, 64, 375]               0\n",
      "          Conv1d-365              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-366              [-1, 64, 375]               0\n",
      "      BasicBlock-367              [-1, 64, 375]               0\n",
      "     BatchNorm1d-368              [-1, 64, 375]             128\n",
      "            ReLU-369              [-1, 64, 375]               0\n",
      "         Dropout-370              [-1, 64, 375]               0\n",
      "          Conv1d-371               [-1, 1, 375]              65\n",
      "        ResNet1D-372                  [-1, 375]               0\n",
      "          Conv1d-373              [-1, 4, 6000]              44\n",
      " MyConv1dPadSame-374              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-375              [-1, 4, 6000]               8\n",
      "            ReLU-376              [-1, 4, 6000]               0\n",
      "          Conv1d-377              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-378              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-379              [-1, 4, 6000]               8\n",
      "            ReLU-380              [-1, 4, 6000]               0\n",
      "         Dropout-381              [-1, 4, 6000]               0\n",
      "          Conv1d-382              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-383              [-1, 4, 6000]               0\n",
      "      BasicBlock-384              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-385              [-1, 4, 6000]               8\n",
      "            ReLU-386              [-1, 4, 6000]               0\n",
      "         Dropout-387              [-1, 4, 6000]               0\n",
      "          Conv1d-388              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-389              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-390              [-1, 4, 6000]               8\n",
      "            ReLU-391              [-1, 4, 6000]               0\n",
      "         Dropout-392              [-1, 4, 6000]               0\n",
      "          Conv1d-393              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-394              [-1, 4, 6000]               0\n",
      "      BasicBlock-395              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-396              [-1, 4, 6000]               8\n",
      "            ReLU-397              [-1, 4, 6000]               0\n",
      "         Dropout-398              [-1, 4, 6000]               0\n",
      "          Conv1d-399              [-1, 8, 3000]             328\n",
      " MyConv1dPadSame-400              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-401              [-1, 8, 3000]              16\n",
      "            ReLU-402              [-1, 8, 3000]               0\n",
      "         Dropout-403              [-1, 8, 3000]               0\n",
      "          Conv1d-404              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-405              [-1, 8, 3000]               0\n",
      "       AvgPool1d-406              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-407              [-1, 4, 3000]               0\n",
      "      BasicBlock-408              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-409              [-1, 8, 3000]              16\n",
      "            ReLU-410              [-1, 8, 3000]               0\n",
      "         Dropout-411              [-1, 8, 3000]               0\n",
      "          Conv1d-412              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-413              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-414              [-1, 8, 3000]              16\n",
      "            ReLU-415              [-1, 8, 3000]               0\n",
      "         Dropout-416              [-1, 8, 3000]               0\n",
      "          Conv1d-417              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-418              [-1, 8, 3000]               0\n",
      "      BasicBlock-419              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-420              [-1, 8, 3000]              16\n",
      "            ReLU-421              [-1, 8, 3000]               0\n",
      "         Dropout-422              [-1, 8, 3000]               0\n",
      "          Conv1d-423             [-1, 16, 1500]           1,296\n",
      " MyConv1dPadSame-424             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-425             [-1, 16, 1500]              32\n",
      "            ReLU-426             [-1, 16, 1500]               0\n",
      "         Dropout-427             [-1, 16, 1500]               0\n",
      "          Conv1d-428             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-429             [-1, 16, 1500]               0\n",
      "       AvgPool1d-430              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-431              [-1, 8, 1500]               0\n",
      "      BasicBlock-432             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-433             [-1, 16, 1500]              32\n",
      "            ReLU-434             [-1, 16, 1500]               0\n",
      "         Dropout-435             [-1, 16, 1500]               0\n",
      "          Conv1d-436             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-437             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-438             [-1, 16, 1500]              32\n",
      "            ReLU-439             [-1, 16, 1500]               0\n",
      "         Dropout-440             [-1, 16, 1500]               0\n",
      "          Conv1d-441             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-442             [-1, 16, 1500]               0\n",
      "      BasicBlock-443             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-444             [-1, 16, 1500]              32\n",
      "            ReLU-445             [-1, 16, 1500]               0\n",
      "         Dropout-446             [-1, 16, 1500]               0\n",
      "          Conv1d-447              [-1, 32, 750]           5,152\n",
      " MyConv1dPadSame-448              [-1, 32, 750]               0\n",
      "     BatchNorm1d-449              [-1, 32, 750]              64\n",
      "            ReLU-450              [-1, 32, 750]               0\n",
      "         Dropout-451              [-1, 32, 750]               0\n",
      "          Conv1d-452              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-453              [-1, 32, 750]               0\n",
      "       AvgPool1d-454              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-455              [-1, 16, 750]               0\n",
      "      BasicBlock-456              [-1, 32, 750]               0\n",
      "     BatchNorm1d-457              [-1, 32, 750]              64\n",
      "            ReLU-458              [-1, 32, 750]               0\n",
      "         Dropout-459              [-1, 32, 750]               0\n",
      "          Conv1d-460              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-461              [-1, 32, 750]               0\n",
      "     BatchNorm1d-462              [-1, 32, 750]              64\n",
      "            ReLU-463              [-1, 32, 750]               0\n",
      "         Dropout-464              [-1, 32, 750]               0\n",
      "          Conv1d-465              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-466              [-1, 32, 750]               0\n",
      "      BasicBlock-467              [-1, 32, 750]               0\n",
      "     BatchNorm1d-468              [-1, 32, 750]              64\n",
      "            ReLU-469              [-1, 32, 750]               0\n",
      "         Dropout-470              [-1, 32, 750]               0\n",
      "          Conv1d-471              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-472              [-1, 64, 375]               0\n",
      "     BatchNorm1d-473              [-1, 64, 375]             128\n",
      "            ReLU-474              [-1, 64, 375]               0\n",
      "         Dropout-475              [-1, 64, 375]               0\n",
      "          Conv1d-476              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-477              [-1, 64, 375]               0\n",
      "       AvgPool1d-478              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-479              [-1, 32, 375]               0\n",
      "      BasicBlock-480              [-1, 64, 375]               0\n",
      "     BatchNorm1d-481              [-1, 64, 375]             128\n",
      "            ReLU-482              [-1, 64, 375]               0\n",
      "         Dropout-483              [-1, 64, 375]               0\n",
      "          Conv1d-484              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-485              [-1, 64, 375]               0\n",
      "     BatchNorm1d-486              [-1, 64, 375]             128\n",
      "            ReLU-487              [-1, 64, 375]               0\n",
      "         Dropout-488              [-1, 64, 375]               0\n",
      "          Conv1d-489              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-490              [-1, 64, 375]               0\n",
      "      BasicBlock-491              [-1, 64, 375]               0\n",
      "     BatchNorm1d-492              [-1, 64, 375]             128\n",
      "            ReLU-493              [-1, 64, 375]               0\n",
      "         Dropout-494              [-1, 64, 375]               0\n",
      "          Conv1d-495               [-1, 1, 375]              65\n",
      "        ResNet1D-496                  [-1, 375]               0\n",
      "          Conv1d-497              [-1, 4, 6000]              44\n",
      " MyConv1dPadSame-498              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-499              [-1, 4, 6000]               8\n",
      "            ReLU-500              [-1, 4, 6000]               0\n",
      "          Conv1d-501              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-502              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-503              [-1, 4, 6000]               8\n",
      "            ReLU-504              [-1, 4, 6000]               0\n",
      "         Dropout-505              [-1, 4, 6000]               0\n",
      "          Conv1d-506              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-507              [-1, 4, 6000]               0\n",
      "      BasicBlock-508              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-509              [-1, 4, 6000]               8\n",
      "            ReLU-510              [-1, 4, 6000]               0\n",
      "         Dropout-511              [-1, 4, 6000]               0\n",
      "          Conv1d-512              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-513              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-514              [-1, 4, 6000]               8\n",
      "            ReLU-515              [-1, 4, 6000]               0\n",
      "         Dropout-516              [-1, 4, 6000]               0\n",
      "          Conv1d-517              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-518              [-1, 4, 6000]               0\n",
      "      BasicBlock-519              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-520              [-1, 4, 6000]               8\n",
      "            ReLU-521              [-1, 4, 6000]               0\n",
      "         Dropout-522              [-1, 4, 6000]               0\n",
      "          Conv1d-523              [-1, 8, 3000]             328\n",
      " MyConv1dPadSame-524              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-525              [-1, 8, 3000]              16\n",
      "            ReLU-526              [-1, 8, 3000]               0\n",
      "         Dropout-527              [-1, 8, 3000]               0\n",
      "          Conv1d-528              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-529              [-1, 8, 3000]               0\n",
      "       AvgPool1d-530              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-531              [-1, 4, 3000]               0\n",
      "      BasicBlock-532              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-533              [-1, 8, 3000]              16\n",
      "            ReLU-534              [-1, 8, 3000]               0\n",
      "         Dropout-535              [-1, 8, 3000]               0\n",
      "          Conv1d-536              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-537              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-538              [-1, 8, 3000]              16\n",
      "            ReLU-539              [-1, 8, 3000]               0\n",
      "         Dropout-540              [-1, 8, 3000]               0\n",
      "          Conv1d-541              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-542              [-1, 8, 3000]               0\n",
      "      BasicBlock-543              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-544              [-1, 8, 3000]              16\n",
      "            ReLU-545              [-1, 8, 3000]               0\n",
      "         Dropout-546              [-1, 8, 3000]               0\n",
      "          Conv1d-547             [-1, 16, 1500]           1,296\n",
      " MyConv1dPadSame-548             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-549             [-1, 16, 1500]              32\n",
      "            ReLU-550             [-1, 16, 1500]               0\n",
      "         Dropout-551             [-1, 16, 1500]               0\n",
      "          Conv1d-552             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-553             [-1, 16, 1500]               0\n",
      "       AvgPool1d-554              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-555              [-1, 8, 1500]               0\n",
      "      BasicBlock-556             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-557             [-1, 16, 1500]              32\n",
      "            ReLU-558             [-1, 16, 1500]               0\n",
      "         Dropout-559             [-1, 16, 1500]               0\n",
      "          Conv1d-560             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-561             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-562             [-1, 16, 1500]              32\n",
      "            ReLU-563             [-1, 16, 1500]               0\n",
      "         Dropout-564             [-1, 16, 1500]               0\n",
      "          Conv1d-565             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-566             [-1, 16, 1500]               0\n",
      "      BasicBlock-567             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-568             [-1, 16, 1500]              32\n",
      "            ReLU-569             [-1, 16, 1500]               0\n",
      "         Dropout-570             [-1, 16, 1500]               0\n",
      "          Conv1d-571              [-1, 32, 750]           5,152\n",
      " MyConv1dPadSame-572              [-1, 32, 750]               0\n",
      "     BatchNorm1d-573              [-1, 32, 750]              64\n",
      "            ReLU-574              [-1, 32, 750]               0\n",
      "         Dropout-575              [-1, 32, 750]               0\n",
      "          Conv1d-576              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-577              [-1, 32, 750]               0\n",
      "       AvgPool1d-578              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-579              [-1, 16, 750]               0\n",
      "      BasicBlock-580              [-1, 32, 750]               0\n",
      "     BatchNorm1d-581              [-1, 32, 750]              64\n",
      "            ReLU-582              [-1, 32, 750]               0\n",
      "         Dropout-583              [-1, 32, 750]               0\n",
      "          Conv1d-584              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-585              [-1, 32, 750]               0\n",
      "     BatchNorm1d-586              [-1, 32, 750]              64\n",
      "            ReLU-587              [-1, 32, 750]               0\n",
      "         Dropout-588              [-1, 32, 750]               0\n",
      "          Conv1d-589              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-590              [-1, 32, 750]               0\n",
      "      BasicBlock-591              [-1, 32, 750]               0\n",
      "     BatchNorm1d-592              [-1, 32, 750]              64\n",
      "            ReLU-593              [-1, 32, 750]               0\n",
      "         Dropout-594              [-1, 32, 750]               0\n",
      "          Conv1d-595              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-596              [-1, 64, 375]               0\n",
      "     BatchNorm1d-597              [-1, 64, 375]             128\n",
      "            ReLU-598              [-1, 64, 375]               0\n",
      "         Dropout-599              [-1, 64, 375]               0\n",
      "          Conv1d-600              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-601              [-1, 64, 375]               0\n",
      "       AvgPool1d-602              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-603              [-1, 32, 375]               0\n",
      "      BasicBlock-604              [-1, 64, 375]               0\n",
      "     BatchNorm1d-605              [-1, 64, 375]             128\n",
      "            ReLU-606              [-1, 64, 375]               0\n",
      "         Dropout-607              [-1, 64, 375]               0\n",
      "          Conv1d-608              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-609              [-1, 64, 375]               0\n",
      "     BatchNorm1d-610              [-1, 64, 375]             128\n",
      "            ReLU-611              [-1, 64, 375]               0\n",
      "         Dropout-612              [-1, 64, 375]               0\n",
      "          Conv1d-613              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-614              [-1, 64, 375]               0\n",
      "      BasicBlock-615              [-1, 64, 375]               0\n",
      "     BatchNorm1d-616              [-1, 64, 375]             128\n",
      "            ReLU-617              [-1, 64, 375]               0\n",
      "         Dropout-618              [-1, 64, 375]               0\n",
      "          Conv1d-619               [-1, 1, 375]              65\n",
      "        ResNet1D-620                  [-1, 375]               0\n",
      "          Conv1d-621              [-1, 4, 6000]              44\n",
      " MyConv1dPadSame-622              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-623              [-1, 4, 6000]               8\n",
      "            ReLU-624              [-1, 4, 6000]               0\n",
      "          Conv1d-625              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-626              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-627              [-1, 4, 6000]               8\n",
      "            ReLU-628              [-1, 4, 6000]               0\n",
      "         Dropout-629              [-1, 4, 6000]               0\n",
      "          Conv1d-630              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-631              [-1, 4, 6000]               0\n",
      "      BasicBlock-632              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-633              [-1, 4, 6000]               8\n",
      "            ReLU-634              [-1, 4, 6000]               0\n",
      "         Dropout-635              [-1, 4, 6000]               0\n",
      "          Conv1d-636              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-637              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-638              [-1, 4, 6000]               8\n",
      "            ReLU-639              [-1, 4, 6000]               0\n",
      "         Dropout-640              [-1, 4, 6000]               0\n",
      "          Conv1d-641              [-1, 4, 6000]             164\n",
      " MyConv1dPadSame-642              [-1, 4, 6000]               0\n",
      "      BasicBlock-643              [-1, 4, 6000]               0\n",
      "     BatchNorm1d-644              [-1, 4, 6000]               8\n",
      "            ReLU-645              [-1, 4, 6000]               0\n",
      "         Dropout-646              [-1, 4, 6000]               0\n",
      "          Conv1d-647              [-1, 8, 3000]             328\n",
      " MyConv1dPadSame-648              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-649              [-1, 8, 3000]              16\n",
      "            ReLU-650              [-1, 8, 3000]               0\n",
      "         Dropout-651              [-1, 8, 3000]               0\n",
      "          Conv1d-652              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-653              [-1, 8, 3000]               0\n",
      "       AvgPool1d-654              [-1, 4, 3000]               0\n",
      "MyAvgPool1dPadSame-655              [-1, 4, 3000]               0\n",
      "      BasicBlock-656              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-657              [-1, 8, 3000]              16\n",
      "            ReLU-658              [-1, 8, 3000]               0\n",
      "         Dropout-659              [-1, 8, 3000]               0\n",
      "          Conv1d-660              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-661              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-662              [-1, 8, 3000]              16\n",
      "            ReLU-663              [-1, 8, 3000]               0\n",
      "         Dropout-664              [-1, 8, 3000]               0\n",
      "          Conv1d-665              [-1, 8, 3000]             648\n",
      " MyConv1dPadSame-666              [-1, 8, 3000]               0\n",
      "      BasicBlock-667              [-1, 8, 3000]               0\n",
      "     BatchNorm1d-668              [-1, 8, 3000]              16\n",
      "            ReLU-669              [-1, 8, 3000]               0\n",
      "         Dropout-670              [-1, 8, 3000]               0\n",
      "          Conv1d-671             [-1, 16, 1500]           1,296\n",
      " MyConv1dPadSame-672             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-673             [-1, 16, 1500]              32\n",
      "            ReLU-674             [-1, 16, 1500]               0\n",
      "         Dropout-675             [-1, 16, 1500]               0\n",
      "          Conv1d-676             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-677             [-1, 16, 1500]               0\n",
      "       AvgPool1d-678              [-1, 8, 1500]               0\n",
      "MyAvgPool1dPadSame-679              [-1, 8, 1500]               0\n",
      "      BasicBlock-680             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-681             [-1, 16, 1500]              32\n",
      "            ReLU-682             [-1, 16, 1500]               0\n",
      "         Dropout-683             [-1, 16, 1500]               0\n",
      "          Conv1d-684             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-685             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-686             [-1, 16, 1500]              32\n",
      "            ReLU-687             [-1, 16, 1500]               0\n",
      "         Dropout-688             [-1, 16, 1500]               0\n",
      "          Conv1d-689             [-1, 16, 1500]           2,576\n",
      " MyConv1dPadSame-690             [-1, 16, 1500]               0\n",
      "      BasicBlock-691             [-1, 16, 1500]               0\n",
      "     BatchNorm1d-692             [-1, 16, 1500]              32\n",
      "            ReLU-693             [-1, 16, 1500]               0\n",
      "         Dropout-694             [-1, 16, 1500]               0\n",
      "          Conv1d-695              [-1, 32, 750]           5,152\n",
      " MyConv1dPadSame-696              [-1, 32, 750]               0\n",
      "     BatchNorm1d-697              [-1, 32, 750]              64\n",
      "            ReLU-698              [-1, 32, 750]               0\n",
      "         Dropout-699              [-1, 32, 750]               0\n",
      "          Conv1d-700              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-701              [-1, 32, 750]               0\n",
      "       AvgPool1d-702              [-1, 16, 750]               0\n",
      "MyAvgPool1dPadSame-703              [-1, 16, 750]               0\n",
      "      BasicBlock-704              [-1, 32, 750]               0\n",
      "     BatchNorm1d-705              [-1, 32, 750]              64\n",
      "            ReLU-706              [-1, 32, 750]               0\n",
      "         Dropout-707              [-1, 32, 750]               0\n",
      "          Conv1d-708              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-709              [-1, 32, 750]               0\n",
      "     BatchNorm1d-710              [-1, 32, 750]              64\n",
      "            ReLU-711              [-1, 32, 750]               0\n",
      "         Dropout-712              [-1, 32, 750]               0\n",
      "          Conv1d-713              [-1, 32, 750]          10,272\n",
      " MyConv1dPadSame-714              [-1, 32, 750]               0\n",
      "      BasicBlock-715              [-1, 32, 750]               0\n",
      "     BatchNorm1d-716              [-1, 32, 750]              64\n",
      "            ReLU-717              [-1, 32, 750]               0\n",
      "         Dropout-718              [-1, 32, 750]               0\n",
      "          Conv1d-719              [-1, 64, 375]          20,544\n",
      " MyConv1dPadSame-720              [-1, 64, 375]               0\n",
      "     BatchNorm1d-721              [-1, 64, 375]             128\n",
      "            ReLU-722              [-1, 64, 375]               0\n",
      "         Dropout-723              [-1, 64, 375]               0\n",
      "          Conv1d-724              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-725              [-1, 64, 375]               0\n",
      "       AvgPool1d-726              [-1, 32, 375]               0\n",
      "MyAvgPool1dPadSame-727              [-1, 32, 375]               0\n",
      "      BasicBlock-728              [-1, 64, 375]               0\n",
      "     BatchNorm1d-729              [-1, 64, 375]             128\n",
      "            ReLU-730              [-1, 64, 375]               0\n",
      "         Dropout-731              [-1, 64, 375]               0\n",
      "          Conv1d-732              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-733              [-1, 64, 375]               0\n",
      "     BatchNorm1d-734              [-1, 64, 375]             128\n",
      "            ReLU-735              [-1, 64, 375]               0\n",
      "         Dropout-736              [-1, 64, 375]               0\n",
      "          Conv1d-737              [-1, 64, 375]          41,024\n",
      " MyConv1dPadSame-738              [-1, 64, 375]               0\n",
      "      BasicBlock-739              [-1, 64, 375]               0\n",
      "     BatchNorm1d-740              [-1, 64, 375]             128\n",
      "            ReLU-741              [-1, 64, 375]               0\n",
      "         Dropout-742              [-1, 64, 375]               0\n",
      "          Conv1d-743               [-1, 1, 375]              65\n",
      "        ResNet1D-744                  [-1, 375]               0\n",
      "     BatchNorm1d-745                 [-1, 1131]           2,262\n",
      "            ReLU-746                 [-1, 1131]               0\n",
      "          Linear-747                   [-1, 50]          56,600\n",
      "     BatchNorm1d-748                   [-1, 50]             100\n",
      "            ReLU-749                   [-1, 50]               0\n",
      "          Linear-750                    [-1, 1]              51\n",
      "            ReLU-751                    [-1, 1]               0\n",
      "RespiratoryRegression-752                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 1,214,883\n",
      "Trainable params: 1,214,883\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.41\n",
      "Forward/backward pass size (MB): 129.69\n",
      "Params size (MB): 4.63\n",
      "Estimated Total Size (MB): 134.74\n",
      "----------------------------------------------------------------\n",
      "resp_multiverse(\n",
      "  (feature_extractors): ModuleDict(\n",
      "    (ECG_filt): ResNet1D(\n",
      "      (first_block_conv): MyConv1dPadSame(\n",
      "        (conv): Conv1d(1, 4, kernel_size=(10,), stride=(1,))\n",
      "      )\n",
      "      (first_block_bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (first_block_relu): ReLU()\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 8, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 16, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (6): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 32, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (7): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (8): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 64, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (9): BasicBlock(\n",
      "          (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (final_relu): ReLU(inplace=True)\n",
      "      (do): Dropout(p=0.5, inplace=False)\n",
      "      (final_ch_pooling): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (scgZ): ResNet1D(\n",
      "      (first_block_conv): MyConv1dPadSame(\n",
      "        (conv): Conv1d(1, 4, kernel_size=(10,), stride=(1,))\n",
      "      )\n",
      "      (first_block_bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (first_block_relu): ReLU()\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 8, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 16, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (6): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 32, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (7): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (8): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 64, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (9): BasicBlock(\n",
      "          (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (final_relu): ReLU(inplace=True)\n",
      "      (do): Dropout(p=0.5, inplace=False)\n",
      "      (final_ch_pooling): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (ppg_ir_1_cardiac): ResNet1D(\n",
      "      (first_block_conv): MyConv1dPadSame(\n",
      "        (conv): Conv1d(1, 4, kernel_size=(10,), stride=(1,))\n",
      "      )\n",
      "      (first_block_bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (first_block_relu): ReLU()\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 4, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(4, 8, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 8, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(8, 16, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 16, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (6): BasicBlock(\n",
      "          (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(16, 32, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (7): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 32, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (8): BasicBlock(\n",
      "          (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(32, 64, kernel_size=(10,), stride=(2,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(2,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "        (9): BasicBlock(\n",
      "          (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (do1): Dropout(p=0.5, inplace=False)\n",
      "          (conv1): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "          (do2): Dropout(p=0.5, inplace=False)\n",
      "          (conv2): MyConv1dPadSame(\n",
      "            (conv): Conv1d(64, 64, kernel_size=(10,), stride=(1,))\n",
      "          )\n",
      "          (pooling): MyAvgPool1dPadSame(\n",
      "            (avg_pool): AvgPool1d(kernel_size=(10,), stride=(1,), padding=(0,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (final_relu): ReLU(inplace=True)\n",
      "      (do): Dropout(p=0.5, inplace=False)\n",
      "      (final_ch_pooling): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (regressors): ModuleDict(\n",
      "    (EErq_cosmed): RespiratoryRegression(\n",
      "      (bn1): BatchNorm1d(1131, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "      (fc1): Linear(in_features=1131, out_features=50, bias=True)\n",
      "      (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def test_model(training_params):\n",
    "\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    print(model)\n",
    "    del model\n",
    "\n",
    "\n",
    "debug_model = True\n",
    "if debug_model==True:\n",
    "    if 'LSTM' not in training_params['model_name']:\n",
    "        test_model(training_params)\n",
    "    else:\n",
    "        ma_test(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "# # data = dataloaders['val'].dataset.data[:5,:,:]\n",
    "# # data = torch.from_numpy(data)\n",
    "\n",
    "# # feature = dataloaders['val'].dataset.feature[:5,:]\n",
    "# # feature = torch.from_numpy(feature)\n",
    "\n",
    "# label = dataloaders['val'].dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label[:,0,:].mean(axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /6000//4//4//4//4//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out['EErq_cosmedperc'].size(), label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand(10,1)\n",
    "# aaa = []\n",
    "# aaa.append(a)\n",
    "# aaa\n",
    "# # print(a.transpose(0,3).size())\n",
    "# # print(a.permute(3,2,1,0).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.concat(aaa, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_total_params/(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training, validating, and evaluating funcitons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, val, eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmchan81\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "if training_params['wandb']:\n",
    "    wandb.login()\n",
    "    os.environ[\"WANDB_DIR\"] = os.path.abspath(outputdir)\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'DL_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir_numeric = outputdir + 'numeric_results/'\n",
    "if outputdir_numeric is not None:\n",
    "    if not os.path.exists(outputdir_numeric):\n",
    "        os.makedirs(outputdir_numeric)\n",
    "        \n",
    "    \n",
    "outputdir_modelout = outputdir + 'model_output/'\n",
    "if outputdir_modelout is not None:\n",
    "    if not os.path.exists(outputdir_modelout):\n",
    "        os.makedirs(outputdir_modelout)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_losses(CV_dict, fig_name=None, show_plot=False, outputdir=None, log_wandb=False):\n",
    "#     fig, ax = plt.subplots(1,1,figsize=(20,3), dpi=80)\n",
    "#     ax.plot(CV_dict['total_loss_train'], color='steelblue')\n",
    "#     ax.plot(CV_dict['total_loss_val'], color='firebrick')\n",
    "    \n",
    "#     ax_no_top_right(ax)\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "#     if fig_name is None:\n",
    "#         fig_name = 'loss'\n",
    "\n",
    "#     if log_wandb:\n",
    "#         wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "#     if outputdir is not None:\n",
    "#         if not os.path.exists(outputdir):\n",
    "#             os.makedirs(outputdir)\n",
    "#         fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "#     if show_plot == False:\n",
    "#         plt.close(fig)\n",
    "#         pyplot.close(fig)\n",
    "#         plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_auxillary = False\n",
    "\n",
    "def train_master(training_params):\n",
    "    \n",
    "    training_params = get_regressor_names(training_params)\n",
    "\n",
    "    pprint.pprint(training_params)\n",
    "    \n",
    "    df_performance_train = {}\n",
    "    df_performance_val = {}\n",
    "\n",
    "    df_outputlabel_train = {}\n",
    "    df_outputlabel_val = {}\n",
    "\n",
    "#     for task in training_params['tasks']:\n",
    "    for task in training_params['regressor_names']:\n",
    "\n",
    "        df_performance_train[task] = pd.DataFrame()\n",
    "        df_performance_val[task] = pd.DataFrame()\n",
    "\n",
    "        df_outputlabel_train[task] = pd.DataFrame()\n",
    "        df_outputlabel_val[task] = pd.DataFrame()\n",
    "\n",
    "#     ordered_subject_ids = np.asarray([115, 107, 113, 110, 101, 104, 106, 121, 212, 102, 103, 111, 114, 116, 118, 119, 120])\n",
    "#     ordered_subject_ids = np.asarray([107, 113, 110, 101, 104, 115, 106, 121, 212, 102, 103, 111, 114, 116, 118, 119, 120])\n",
    "#         ordered_subject_ids = np.asarray([101, 110, 113, 119, 115, 107, 104, 106, 121, 212, 102, 103, 111, 114, 116, 118, 120])\n",
    "#     ordered_subject_ids = np.asarray([101, 110, 113, 119, 115])\n",
    "\n",
    "    ordered_subject_ids = training_params['ordered_subject_ids']\n",
    "    main_task = training_params['output_names'][0].split('-')[0]\n",
    "\n",
    "\n",
    "    # for subject_id in training_params['subject_ids']:\n",
    "    for i_CV, subject_id in enumerate(ordered_subject_ids):\n",
    "        \n",
    "        if 'CV_max' in training_params:\n",
    "            if i_CV >= training_params['CV_max']:\n",
    "                continue\n",
    "\n",
    "        training_params['CV_config']['subject_id'] = subject_id\n",
    "\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "        print('using device', device)\n",
    "\n",
    "\n",
    "        print('using model ', training_params['model_name'])\n",
    "\n",
    "#         training_params = get_regressor_names(training_params)\n",
    "        model = resp_multiverse(training_params=training_params)\n",
    "#         print(model)\n",
    "        \n",
    "        model = model.to(device).float()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'], weight_decay=0.01)\n",
    "\n",
    "        criterion = MultiTaskLoss(training_params)\n",
    "        \n",
    "        training_params['criterion'] = criterion\n",
    "        training_params['optimizer'] = optimizer\n",
    "        training_params['inputdir'] = inputdir\n",
    "        \n",
    "\n",
    "        \n",
    "#         print( training_params['regressor_names'])\n",
    "        CV_dict = train_model(model, training_params, trainer, evaler, preder)\n",
    "\n",
    "        print(CV_dict['performance_dict_val'].keys())\n",
    "#         sys.exit()\n",
    "\n",
    "        plot_losses(CV_dict, outputdir=outputdir, show_plot=False, fig_name='loss_{}'.format(subject_id))\n",
    "\n",
    "        \n",
    "        # print(CV_dict.keys())\n",
    "        # sys.exit()\n",
    "\n",
    "#         TODO: fix this code\n",
    "#         print(training_params['regressor_names'])\n",
    "        \n",
    "#         for task in CV_dict['performance_dict_train']['out_dict'].keys():\n",
    "        for task in training_params['regressor_names']:\n",
    "        \n",
    "#             print(task, df_performance_train)\n",
    "\n",
    "#         for task in training_params['tasks']:\n",
    "\n",
    "#             print( CV_dict['performance_dict_val']['out_dict'].keys(), CV_dict['performance_dict_val']['label_dict'].keys(), task)\n",
    "            label_est_val = CV_dict['performance_dict_val']['out_dict'][task]\n",
    "            label_val = CV_dict['performance_dict_val']['label_dict'][task]\n",
    "\n",
    "            label_est_train = CV_dict['performance_dict_train']['out_dict'][task]\n",
    "            label_train = CV_dict['performance_dict_train']['label_dict'][task]\n",
    "            \n",
    "            \n",
    "            # rescale the label after making estimations\n",
    "            if 'perc' in training_params['output_names'][0]:\n",
    "                i_meta = training_params['meta_names'].index('EEavg_est')\n",
    "#                 print(CV_dict['performance_dict_train']['meta_arr'], CV_dict['performance_dict_train']['meta_arr'].shape)\n",
    "                meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "                meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "                label_train = label_train*meta_train\n",
    "                label_val = label_val*meta_val\n",
    "                label_est_train = label_est_train*meta_train\n",
    "                label_est_val = label_est_val*meta_val\n",
    "            elif 'weighted' in training_params['output_names'][0]:\n",
    "                i_meta = training_params['meta_names'].index('weight')\n",
    "                meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "                meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "                label_train = label_train*meta_train\n",
    "                label_val = label_val*meta_val\n",
    "                label_est_train = label_est_train*meta_train\n",
    "                label_est_val = label_est_val*meta_val\n",
    "\n",
    "            \n",
    "            # get performance df for training and testing dataset\n",
    "            df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, subject_id, task), ignore_index=True )\n",
    "\n",
    "            df_performance_train[task].to_csv(outputdir_numeric + 'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "                pd.DataFrame( {\n",
    "                'label_est': label_est_train,\n",
    "                'label': label_train,\n",
    "                'CV': [subject_id]*label_train.shape[0],\n",
    "                'task': [task]*label_train.shape[0]\n",
    "                }), ignore_index=True )\n",
    "\n",
    "            df_outputlabel_train[task].to_csv(outputdir_numeric + 'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, subject_id, task), ignore_index=True )\n",
    "            df_performance_val[task].to_csv(outputdir_numeric + 'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "                pd.DataFrame( {\n",
    "                'label_est': label_est_val,\n",
    "                'label': label_val,\n",
    "                'CV': [subject_id]*label_val.shape[0],\n",
    "                'task': [task]*label_val.shape[0]\n",
    "                }), ignore_index=True )\n",
    "\n",
    "            df_outputlabel_val[task].to_csv(outputdir_numeric + 'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            # plot performance training and testing dataset\n",
    "            if (main_task not in task) and (debug_auxillary==False):\n",
    "                continue\n",
    "            \n",
    "#             plot_regression(df_outputlabel_train[task], df_performance_train[task], task, fig_name='regression_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "#             plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "            # plot_regression(df_outputlabel_val[task], task, training_params, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "            plot_regression(df_outputlabel_val[task], task, training_params, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#             plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "#             plot_output(df_outputlabel_train[task], task, fig_name = 'outputINtime_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout)\n",
    "        \n",
    "        # check_featuremap(model, training_params, mode='worst', fig_name = 'DL_activation_{}_'.format(subject_id), outputdir=outputdir+'activation_layers_worst/{}/'.format(subject_id), show_plot=False)\n",
    "        # check_featuremap(model, training_params, mode='best', fig_name = 'DL_activation_{}_'.format(subject_id), outputdir=outputdir+'activation_layers_best/{}/'.format(subject_id), show_plot=False)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for task in training_params['regressor_names']:\n",
    "        if task!=main_task:\n",
    "            continue\n",
    "        plot_regression_all_agg(df_outputlabel_train[task], training_params, fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "        plot_BA(df_outputlabel_train[task], task, training_params, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "        plot_regression_all_agg(df_outputlabel_val[task], training_params, fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "        plot_BA(df_outputlabel_val[task], task, training_params, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "        plot_output(df_outputlabel_val[task], task, fig_name = 'outputINtime_val_{}'.format(task),  show_plot=False, outputdir=outputdir_modelout)\n",
    "\n",
    "#     plot_BA(df_outputlabel_val[main_task], main_task, fig_name='BA_val_{}'.format(main_task), show_plot=False, outputdir=outputdir+'model_output/', log_wandb=training_params['wandb'])\n",
    "#     plot_regression_all_agg(df_outputlabel_val[main_task], df_performance_val[main_task], outputdir=outputdir+'model_output/', show_plot=False, log_wandb=training_params['wandb'])\n",
    "\n",
    "    # log metrices on wnadb\n",
    "    if training_params['wandb']==True:\n",
    "        # W&B\n",
    "        label = df_outputlabel_val[main_task]['label'].values\n",
    "        label_est = df_outputlabel_val[main_task]['label_est'].values\n",
    "#         print(label.shape, label)\n",
    "#         print(label_est.shape, label_est)\n",
    "    \n",
    "        PCC = get_PCC(label, label_est)\n",
    "        Rsquared = get_CoeffDeterm(label, label_est)\n",
    "        MAE, _ = get_MAE(label, label_est)\n",
    "        RMSE = get_RMSE(label, label_est)\n",
    "        MAPE, _ = get_MAPE(label, label_est)\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'val_MAE': MAE,\n",
    "                'val_RMSE': RMSE,\n",
    "                'val_MAPE': MAPE,\n",
    "                'val_PCC': PCC,\n",
    "                'val_Rsquared': Rsquared,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_featuremap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_featuremap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(config=None):\n",
    "\n",
    "#     with wandb.init(config=config, entity='inanlab', project=\"[TL] stage2_cnn\", reinit=True, dir=outputdir):\n",
    "    with wandb.init(config=config, reinit=True, dir=outputdir):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        print(config)\n",
    "        \n",
    "        # init the model\n",
    "        \n",
    "        for key in config.keys():\n",
    "            if key=='loss_weights':\n",
    "#                 training_params[key]['RR_cosmed'] = config[key]\n",
    "                training_params[key]['auxillary_task'] = config[key]\n",
    "            else:\n",
    "                training_params[key] = config[key]\n",
    "\n",
    "\n",
    "        train_master(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: use training_params['regressor_names'] for looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweeping for: baseline\n",
      "Create sweep with ID: pfjk9h3g\n",
      "Sweep URL: https://wandb.ai/inanlab/%5BEE%5D%20stage4_baseline/sweeps/pfjk9h3g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fxvp5ix5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_names: ['ECG_filt', 'scgZ', 'ppg_ir_1_resp']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_weights: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_names: ['EErq_cosmedperc', 'HR_patch']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find DL_regression.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmchan81\u001b[0m (\u001b[33minanlab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../../../data/stage4_TEST/ResNet1D/wandb/run-20230213_233905-fxvp5ix5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/inanlab/%5BEE%5D%20stage4_baseline/runs/fxvp5ix5\" target=\"_blank\">logical-sweep-1</a></strong> to <a href=\"https://wandb.ai/inanlab/%5BEE%5D%20stage4_baseline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/inanlab/%5BEE%5D%20stage4_baseline/sweeps/pfjk9h3g\" target=\"_blank\">https://wandb.ai/inanlab/%5BEE%5D%20stage4_baseline/sweeps/pfjk9h3g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_names': ['ECG_filt', 'scgZ', 'ppg_ir_1_resp'], 'loss_weights': 0, 'output_names': ['EErq_cosmedperc', 'HR_patch']}\n",
      "{'CV_config': {'subject_id': 113, 'task_id': 5},\n",
      " 'FS_RESAMPLE_DL': 100,\n",
      " 'base_filters': 4,\n",
      " 'batch_size': 64,\n",
      " 'channel_n': 4,\n",
      " 'cuda_i': 1,\n",
      " 'data_dimensions': [3, 6000],\n",
      " 'device': device(type='cuda', index=1),\n",
      " 'downsample_gap': 2,\n",
      " 'featrue_extractor': <class 'EE_extension.models_resnet.ResNet1D'>,\n",
      " 'feature_names': ['weight',\n",
      "                   'height',\n",
      "                   'EEavg_est',\n",
      "                   'scg_std',\n",
      "                   'HR_patch',\n",
      "                   'CO_patchsur'],\n",
      " 'fusion_type': 'late',\n",
      " 'groups': 1,\n",
      " 'in_channels': [3, 6000],\n",
      " 'increasefilter_gap': 2,\n",
      " 'input_names': ['ECG_filt', 'scgZ', 'ppg_ir_1_resp'],\n",
      " 'inputdir': '../../../data/stage3/win60_overlap90/',\n",
      " 'kernel_size': 10,\n",
      " 'learning_rate': 0.001,\n",
      " 'list_feature': ['HR_patch',\n",
      "                  'HR_patchnormed',\n",
      "                  'scg_std',\n",
      "                  'scg_std_perc',\n",
      "                  'VE_cosmed',\n",
      "                  'CO_patchsur',\n",
      "                  '0.00Hz',\n",
      "                  '3.91Hz',\n",
      "                  '7.81Hz',\n",
      "                  '11.72Hz',\n",
      "                  '15.62Hz',\n",
      "                  '19.53Hz',\n",
      "                  '23.44Hz',\n",
      "                  'weight',\n",
      "                  'height',\n",
      "                  'gender',\n",
      "                  'age',\n",
      "                  'BMR',\n",
      "                  'BMI',\n",
      "                  'EEavg_est',\n",
      "                  'VTavg_est'],\n",
      " 'list_meta': ['subject_id', 'task', 'EEavg_est', 'VTavg_est', 'weight'],\n",
      " 'list_output': ['RR_cosmed',\n",
      "                 'VT_cosmed',\n",
      "                 'EE_cosmed',\n",
      "                 'HR_cosmed',\n",
      "                 'EErq_cosmed',\n",
      "                 'HR_patch',\n",
      "                 'HR_patchnormed',\n",
      "                 'VO2_cosmed',\n",
      "                 'resp_cosmed',\n",
      "                 'O2pulse_cosmedpatch',\n",
      "                 'VT_cosmedperc',\n",
      "                 'EErq_cosmedperc',\n",
      "                 'EErq_cosmedweighted',\n",
      "                 'EErq_cosmeddelta'],\n",
      " 'list_signal': ['ECG_filt',\n",
      "                 'scgZ',\n",
      "                 'ppg_g_1_resp',\n",
      "                 'ppg_ir_1_resp',\n",
      "                 'ppg_r_1_resp',\n",
      "                 'ppg_g_1_cardiac',\n",
      "                 'ppg_ir_1_cardiac',\n",
      "                 'ppg_r_1_cardiac',\n",
      "                 'ppg_g_2_resp',\n",
      "                 'ppg_ir_2_resp',\n",
      "                 'ppg_r_2_resp',\n",
      "                 'ppg_g_2_cardiac',\n",
      "                 'ppg_ir_2_cardiac',\n",
      "                 'ppg_r_2_cardiac'],\n",
      " 'loss_weights': {'auxillary_task': 0, 'main_task': 1},\n",
      " 'meta_names': ['subject_id', 'task', 'EEavg_est', 'VTavg_est', 'weight'],\n",
      " 'model_name': 'ResNet1D',\n",
      " 'n_block': 10,\n",
      " 'n_block_macro': 5,\n",
      " 'num_epochs': 50,\n",
      " 'ordered_subject_ids': array([101, 103, 104, 106, 107, 110, 111, 115, 116, 117, 118, 119, 120,\n",
      "       121, 113]),\n",
      " 'output_dim': 1,\n",
      " 'output_names': ['EErq_cosmedperc', 'HR_patch'],\n",
      " 'outputdir': '../../../data/stage4_TEST/',\n",
      " 'pad_type': 'reflect',\n",
      " 'pooling_type': 'avg_pooling',\n",
      " 'regressor_names': ['EErq_cosmedperc',\n",
      "                     'HR_patch-ECG_filt',\n",
      "                     'HR_patch-scgZ',\n",
      "                     'HR_patch-ppg_ir_1_resp'],\n",
      " 'sequence': False,\n",
      " 'stride': 2,\n",
      " 'subject_ids': array([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114,\n",
      "       115, 116, 117, 118, 119, 120, 121, 212], dtype=object),\n",
      " 'sweep_config': {'method': 'grid',\n",
      "                  'metric': {'goal': 'minimize', 'name': 'val_MAE'},\n",
      "                  'parameters': {'input_names': {'values': [['ECG_filt',\n",
      "                                                             'scgZ',\n",
      "                                                             'ppg_ir_1_resp']]},\n",
      "                                 'loss_weights': {'values': [0, 0.5, 1, 3]},\n",
      "                                 'output_names': {'values': [['EErq_cosmedperc',\n",
      "                                                              'HR_patch'],\n",
      "                                                             ['EErq_cosmed',\n",
      "                                                              'HR_patch']]}},\n",
      "                  'program': 'train_master.py'},\n",
      " 'sweep_name': 'baseline',\n",
      " 'task_ids': array([0, 1, 2, 3, 4, 5], dtype=object),\n",
      " 'training_mode': 'subject_ind',\n",
      " 'use_sc': True,\n",
      " 'wandb': True}\n",
      "using device cuda:1\n",
      "using model  ResNet1D\n",
      "\t start training.....\n",
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "\t done with training.....\n",
      "dict_keys(['total_loss', 'out_dict', 'label_dict', 'meta_arr', 'feature_arr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:1\n",
      "using model  ResNet1D\n",
      "\t start training.....\n",
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "\t done with training.....\n",
      "dict_keys(['total_loss', 'out_dict', 'label_dict', 'meta_arr', 'feature_arr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:1\n",
      "using model  ResNet1D\n",
      "\t start training.....\n",
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "if training_params['wandb']:\n",
    "    print('sweeping for:', sweep_name)\n",
    "    sweep_config = training_params['sweep_config']    \n",
    "#     with wandb.init(config=config, entity='inanlab', project=\"[TL] stage2_cnn\", reinit=True, dir=outputdir):\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='inanlab', project='[EE] stage4_'+training_params['sweep_name'])\n",
    "\n",
    "#     sweep_id = wandb.sweep(sweep_config, project=sweep_name)\n",
    "    wandb.agent(sweep_id, train_sweep)\n",
    "    \n",
    "else:\n",
    "    train_master(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCC = get_PCC(np.ones(5), np.ones(5))\n",
    "\n",
    "# print('{:.2f}'.format(PCC))\n",
    "# # print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datetime_end = datetime.now(tz_NY)\n",
    "print(\"end time:\", datetime_end.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n",
    "duration = datetime_end-datetime_start\n",
    "duration_in_s = duration.total_seconds()\n",
    "days    = divmod(duration_in_s, 86400)        # Get days (without [0]!)\n",
    "hours   = divmod(days[1], 3600)               # Use remainder of days to calc hours\n",
    "minutes = divmod(hours[1], 60)                # Use remainder of hours to calc minutes\n",
    "seconds = divmod(minutes[1], 1)               # Use remainder of minutes to calc seconds\n",
    "print(\"Time between dates: %d days, %d hours, %d minutes and %d seconds\" % (days[0], hours[0], minutes[0], seconds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def get_activation(name):\n",
    "# #     activation = {}\n",
    "# #     def hook(model, input, output):\n",
    "# #         activation[name] = output.detach()\n",
    "# #     return hook\n",
    "    \n",
    "# def model_hooking(model, training_params, get_activation):\n",
    "\n",
    "#     model_name = training_params['model_name']\n",
    "    \n",
    "#     if model_name=='FeatureExtractor_CNN2':\n",
    "#         key = list(model.feature_extractors.keys())[0]\n",
    "#         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "#         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "#         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "#         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "#         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "#         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n",
    "        \n",
    "# #         layer_names = ['layer1', ]\n",
    "        \n",
    "#     elif model_name=='FeatureExtractor_CNN':\n",
    "#         key = list(model.feature_extractors.keys())[0]\n",
    "#         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "#         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "#         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "#         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "#         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "#         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n",
    "#     if model_name=='ResNet1D':\n",
    "#         pass\n",
    "# #         key = list(model.feature_extractors.keys())[0]\n",
    "# #         model.feature_extractors[key].layer1.register_forward_hook(get_activation('layer1'))\n",
    "# #         model.feature_extractors[key].layer2.register_forward_hook(get_activation('layer2'))\n",
    "# #         model.feature_extractors[key].layer3.register_forward_hook(get_activation('layer3'))\n",
    "# #         model.feature_extractors[key].layer4.register_forward_hook(get_activation('layer4'))\n",
    "# #         model.regressors.EE_cosmed.fc1.register_forward_hook(get_activation('fc1'))\n",
    "# #         model.regressors.EE_cosmed.fc2.register_forward_hook(get_activation('fc2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: improve this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dict_train = preder(model, dataloaders['train'], training_params)\n",
    "performance_dict_val = preder(model, dataloaders['val'], training_params)\n",
    "\n",
    "unit = unit_dict[task.split('_')[0]]\n",
    "\n",
    "for task in training_params['tasks']:\n",
    "    \n",
    "    \n",
    "    print('evaluating task:', task)\n",
    "    MAE, std_AE = get_MAE(performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task])\n",
    "    print('\\ttrainin: {:.2f}{:.2f} {}'.format(MAE, std_AE, unit))\n",
    "\n",
    "    MAE, std_AE = get_MAE(performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task])\n",
    "    print('\\tval: {:.2f}{:.2f} {}'.format(MAE, std_AE, unit))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce output figures\n",
    "# TODO: implement the plotting functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO!!!!\n",
    "\n",
    "# for task in tasks\n",
    "#     plot_loss vs epoch (train and val)\n",
    "#     plot_MAE, RMSE, vs epoch (train and val)\n",
    "#     plot scatter plots (show PCC, BD, std, ect.) (just val)\n",
    "#     plot output vs. label (just val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = training_params['output_names'][0].split('_')[0]\n",
    "unit_dict[output_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kcalpmin2watt = 69.7333333\n",
    "\n",
    "# sub_weight = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_dict_train['label_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in training_params['tasks']:\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5), dpi=100)\n",
    "    fontsize = 15\n",
    "    data_min = np.min(np.r_[performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task]])\n",
    "    data_max = np.max(np.r_[performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task]])\n",
    "    ax1.scatter(performance_dict_train['out_dict'][task], performance_dict_train['label_dict'][task], alpha=0.3)\n",
    "    ax1.set_xlim(data_min, data_max)\n",
    "    ax1.set_ylim(data_min, data_max)\n",
    "    ax1.plot( [data_min, data_max],[data_min, data_max], '--', color='gray', alpha=0.8)\n",
    "\n",
    "    ax1.set_xlabel('estimated {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    ax1.set_ylabel('true {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "#     ax.set_xlabel('estimated {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "#     ax.set_ylabel('true {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "\n",
    "    ax1.set_title('training', fontsize=fontsize)\n",
    "\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(5,5))\n",
    "#     fontsize = 15\n",
    "    data_min = np.min(np.r_[performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task]])\n",
    "    data_max = np.max(np.r_[performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task]])\n",
    "    ax2.scatter(performance_dict_val['out_dict'][task], performance_dict_val['label_dict'][task], alpha=0.3)\n",
    "    ax2.set_xlim(data_min, data_max)\n",
    "    ax2.set_ylim(data_min, data_max)\n",
    "    ax2.plot( [data_min, data_max],[data_min, data_max], '--', color='gray', alpha=0.8)\n",
    "\n",
    "    ax2.set_xlabel('estimated {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    ax2.set_ylabel('true {} ({})'.format(task.split('_')[0], unit_dict[task.split('_')[0]]), fontsize=fontsize)\n",
    "    \n",
    "    ax2.set_title('testing', fontsize=fontsize)\n",
    "\n",
    "#     ax.set_xlabel('estimated {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "#     ax.set_ylabel('true {} ({})'.format(output_name, 'W'), fontsize=fontsize)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matlab",
   "language": "python",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
