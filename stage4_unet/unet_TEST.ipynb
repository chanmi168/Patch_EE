{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchan/miniconda3/envs/mienv/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.11) or chardet (2.1.1) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from math import sin\n",
    "from icecream import ic\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# checklist 1: uncomment matplotlib.use('Agg')\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.signal import hilbert, chirp\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "i_seed = 0\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchsummary import summary\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../../') # add this line so Data and data are visible in this file\n",
    "sys.path.append('../PatchWand/') # add this line so Data and data are visible in this file\n",
    "\n",
    "from plotting_tools import *\n",
    "from preprocessing import *\n",
    "from setting import *\n",
    "from surrogate_extraction import *\n",
    "from dataIO import *\n",
    "from filters import *\n",
    "from spectral_module import *\n",
    "from resp_module import *\n",
    "from stage4_regression import *\n",
    "from unet_extension.dataset_util import *\n",
    "from unet_extension.evaluation_util import *\n",
    "from unet_extension.models import *\n",
    "from unet_extension.training_util import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# checklist 2: comment out all magic command\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(TF_type='prepare', grad_clip='true', input_folder='../../data/stage3-1_windowing/CDC_dataset/win60_overlap95_seq20_norm/', input_names='ECG_SR+ECG_SR', output_folder='../../data/stage4_UNet_test/', training_params_file='training_params_baseline_test.json', variant='AT_block')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='RR_estimate')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "# parser.add_argument('--task_ids_train', metavar='task_ids_train', help='task_ids_train',\n",
    "#                     default='')\n",
    "# parser.add_argument('--task_ids_val', metavar='task_ids_val', help='task_ids_val',\n",
    "#                     default='')\n",
    "parser.add_argument('--TF_type', metavar='TF_type', help='TF_type',\n",
    "                    default='target')\n",
    "parser.add_argument('--variant', metavar='variant', help='variant',\n",
    "                    default='baseline')\n",
    "parser.add_argument('--grad_clip', metavar='grad_clip', help='grad_clip',\n",
    "                    default=False)\n",
    "parser.add_argument('--input_names', metavar='input_names', help='input_names',\n",
    "                    default='ECG_SR')\n",
    "# parser.add_argument('--model_name', metavar='model_name', help='model_name',\n",
    "#                     default='UNet')\n",
    "# parser.add_argument('--training_scheme', metavar='training_scheme', help='training_scheme',\n",
    "#                     default='LOSO')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "\n",
    "# args = parser.parse_args(['--input_folder', '../../../covid/results/stage3/win60_overlap95_seq20_ECG_AMpt+ECG_AMr+ECG_AMbi+ECG_SR_norm_script16/', \n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3-1_windowing/CDC_dataset/win60_overlap95_seq20_norm/', \n",
    "# args = parser.parse_args(['--input_folder', '../../data/stage3-1_windowing/GT_dataset/win60_overlap95_seq20_norm/', \n",
    "#                           '--output_folder', '../../../covid/results/stage4/unet_test/win60_overlap95_seq20_ECGAMnorm+SCGBWnorm/',\n",
    "                          '--output_folder', '../../data/stage4_UNet_test/',\n",
    "#                           '--task_ids_train', '0,1,2,3,4,5',\n",
    "#                           '--task_ids_val', '0,1,2,3,4,5,6,7,9,10',\n",
    "                          '--variant', 'AT_block',\n",
    "#                           '--variant', 'baseline',\n",
    "#                           '--variant', 'Late_UNet',\n",
    "#                           '--variant', 'Attention_UNet',\n",
    "                          '--TF_type', 'prepare',\n",
    "                          '--grad_clip', 'true',\n",
    "                          '--input_names', 'ECG_SR+ECG_SR',\n",
    "#                           '--input_names', 'PPG',\n",
    "                          '--training_params_file', 'training_params_baseline_test.json',])\n",
    "#                           '--model_name', 'UNet'])\n",
    "                          \n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "## add late fusion\n",
    "## SE_block fusion\n",
    "## \n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_type\n",
    "# source data only\n",
    "# 1. source\n",
    "# include target data\n",
    "# 2. FT_top # Fine-tune top layer\n",
    "# 3. FT_top2 # Fine-tune top 2 layers\n",
    "# 4. FT_all # Fine-tune all layers\n",
    "# 5. target # Train from scratch\n",
    "# 6. prepare # train a model use all data in `domain` and `input_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will export data to ../../data/stage4_UNet_test/CDC_dataset/ECG-SR+ECG-SR/AT_block/prepare/\n"
     ]
    }
   ],
   "source": [
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "\n",
    "# if len(args.task_ids_train)==0:\n",
    "#     task_ids_train = None\n",
    "# else:\n",
    "#     task_ids_train = [int(item) for item in args.task_ids_train.split(',')]\n",
    "\n",
    "# if len(args.task_ids_val)==0:\n",
    "#     task_ids_val = None\n",
    "# else:\n",
    "#     task_ids_val = [int(item) for item in args.task_ids_val.split(',')]\n",
    "\n",
    "TF_type = args.TF_type\n",
    "variant = args.variant\n",
    "grad_clip = args.grad_clip\n",
    "\n",
    "input_names = args.input_names\n",
    "input_names = input_names.split('+')\n",
    "\n",
    "if grad_clip=='true':\n",
    "    grad_clip = True\n",
    "else:\n",
    "    grad_clip = False\n",
    "\n",
    "domain = inputdir.split('/')[-3]\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "# modality = training_params_file.split('_')[-1].split('.')[0]\n",
    "modality = '+'.join(input_names).replace('_', '-')\n",
    "\n",
    "# # outputdir += '{}/{}/{}/{}/'.format(domain, modality, inputdir.split('/')[-2], TF_type)\n",
    "    \n",
    "outputdir += '{}/{}/{}/{}/'.format(domain, modality, variant, TF_type)\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "print('will export data to', outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_dir(training_params):\n",
    "\n",
    "    if training_params['domain']=='CDC_dataset':\n",
    "        source_domain = 'GT_dataset'\n",
    "    else:\n",
    "        source_domain = 'CDC_dataset'\n",
    "        \n",
    "    variant = training_params['variant']\n",
    "\n",
    "    # by default, use this dir\n",
    "    # TODO: check if model channel dim is the same as data channel dim\n",
    "    pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECG-SR+ECG-SR/{}/prepare/'.format(source_domain, variant)\n",
    "\n",
    "    input_N_channel = len(training_params['input_names'])\n",
    "    if input_N_channel==1:\n",
    "        pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECG/{}/prepare/'.format(source_domain, variant)\n",
    "#       pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/SCG/win60_overlap95_seq20_norm/prepare/'.format(source_domain)\n",
    "\n",
    "    elif input_N_channel==2:\n",
    "        pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECG-SR+ECG-SR/{}/prepare/'.format(source_domain, variant)\n",
    "#         first_list = training_params['input_names']\n",
    "#         sec_list = ['ECG_SR', 'SCG_AMpt']\n",
    "\n",
    "#         # use ECGSCG if they are also 'ECG_SR' and 'SCG_AMpt'\n",
    "#         if all(map(lambda x, y: x == y, first_list, sec_list)):\n",
    "# #                     pretrain_dir = os.path.expanduser('~')+'/Estimation_RR/covid/results/stage4/unet_test/win60_overlap95_seq20_ECG_SR+SCG_AMpt_norm_script18/'\n",
    "#             pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECGSCG/win60_overlap95_seq20_norm/prepare/'.format(source_domain)\n",
    "\n",
    "#         else:\n",
    "#             pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECGdouble/win60_overlap95_seq20_norm/prepare/'.format(source_domain)\n",
    "    elif input_N_channel==3:\n",
    "        pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECGtriple/{}/prepare/'.format(source_domain, variant)\n",
    "\n",
    "    elif input_N_channel==4:\n",
    "        pretrain_dir = os.path.expanduser('~')+'/Estimation_EE/data/stage4_UNet/{}/ECGquadruple/{}/prepare/'.format(source_domain, variant)\n",
    "\n",
    "\n",
    "    return pretrain_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_params['model_name'] = 'variant'\n",
    "# xarr = np.array([1, 1, 2, 2.1, 3, 4, 5])\n",
    "# y = np.array([1,2])\n",
    "# mask_task = np.in1d(xarr, y)\n",
    "# # Out[25]: array([ True,  True, False, False, False], dtype=bool)\n",
    "\n",
    "# domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dimensions: torch.Size([2, 20, 58])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in training_params_list:\n",
    "    \n",
    "#     training_params['training_scheme'] = training_scheme\n",
    "    # include device in training_params\n",
    "    training_params['TF_type'] = TF_type\n",
    "    training_params['domain'] = domain\n",
    "    training_params['variant'] = variant\n",
    "    training_params['outputdir'] = outputdir\n",
    "    training_params['grad_clip'] = grad_clip\n",
    "    training_params['input_names'] = input_names\n",
    "    \n",
    "    \n",
    "    if domain=='CDC_dataset':\n",
    "        if len(training_params['task_ids_train'])==0:\n",
    "            training_params['task_ids_train'] = [0,1,2,3,4,5]\n",
    "        else:\n",
    "            training_params['task_ids_train'] = [int(item) for item in training_params['task_ids_train'].split(',')]\n",
    "\n",
    "        if len(training_params['task_ids_val'])==0:\n",
    "            training_params['task_ids_val'] = [0,1,2,3,4,5,6,10]\n",
    "        else:\n",
    "            training_params['task_ids_val'] = [int(item) for item in training_params['task_ids_val'].split(',')]\n",
    "    elif domain=='GT_dataset':\n",
    "        training_params['task_ids_train'] = [101]\n",
    "        training_params['task_ids_val'] = [101]\n",
    "\n",
    "\n",
    "    \n",
    "    training_params['select_channel'] = True\n",
    "    \n",
    "#     if 'variant' not in training_params:\n",
    "#         training_params['variant'] = 'baseline'\n",
    "\n",
    "    device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "#     device = torch.device('cpu')\n",
    "    training_params['device'] = device\n",
    "\n",
    "    stage3_dict = data_loader('stage3_dict', inputdir).item()\n",
    "    freq = stage3_dict['freq']\n",
    "    \n",
    "    training_params['xf_masked'] =  torch.from_numpy(freq)*60\n",
    "    \n",
    "    # include freq_dict in training_params\n",
    "#     freq = data_loader('freq', inputdir)\n",
    "    freq_dict = dict(zip(np.arange(freq.shape[0]), freq))\n",
    "    training_params['freq_dict'] = freq_dict\n",
    "    training_params['subject_ids'] = stage3_dict['subject_ids']    \n",
    "    \n",
    "#     if 'ordered_subject_ids' in training_params:\n",
    "#         training_params['subject_ids'] = np.asarray(training_params['ordered_subject_ids'])\n",
    "#       \"ordered_subject_ids\": [116, 118, 113, 105, 212, 117, 114, 110, 101, 103, 104, 106, 107, 108, 111, 115, 119, 120, 121],\n",
    "# 16, 10, 13, 9, 11,\n",
    "    if domain=='GT_dataset':\n",
    "        training_params['subject_ids'] = np.asarray([11, 10, 9,1, 18, 23, 13, 16, 5,  3, 4, 6, 7, 12, 15, 17, 22])\n",
    "    \n",
    "    training_params['surrogate_names'] = stage3_dict['surrogate_names']\n",
    "    \n",
    "    \n",
    "    training_params['meta_names'] = stage3_dict['meta_names']\n",
    "    \n",
    "    training_params['i_HR'] = training_params['meta_names'].index('HR_cosmed')\n",
    "    training_params['i_VT'] = training_params['meta_names'].index('VT_cosmed')\n",
    "    training_params['i_RR'] = training_params['meta_names'].index('RR_cosmed')\n",
    "    # include data_dimensions in training_params\n",
    "    \n",
    "    \n",
    "    if domain=='GT_dataset':\n",
    "        task_id = [101]\n",
    "    elif domain=='CDC_dataset':\n",
    "#         task_id = [0, 1, 2, 3, 4, 5]\n",
    "#         task_id = [0, 1, 2, 5]\n",
    "#         task_id = [0, 1, 2, 3, 5, 6]\n",
    "        task_id = [0, 1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
    "\n",
    "\n",
    "    if domain=='GT_dataset':\n",
    "        training_params['CV_config'] = {\n",
    "            'subject_id': 104,\n",
    "            'task_id': task_id,\n",
    "            'reject_subject_id': []\n",
    "        }\n",
    "    elif domain=='CDC_dataset':\n",
    "        training_params['CV_config'] = {\n",
    "            'subject_id': 104,\n",
    "            'task_id': task_id,\n",
    "            'reject_subject_id': [101, 102, 103]\n",
    "        }\n",
    "    \n",
    "    # reject bad subject\n",
    "    training_params['subject_ids'] = training_params['subject_ids'][~np.isin(training_params['subject_ids'], training_params['CV_config']['reject_subject_id'])]\n",
    "#     sys.exit()\n",
    "    \n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    print('data_dimensions:', data_dimensions)\n",
    "    training_params['data_dimensions'] = list(data_dimensions)\n",
    "    del dataloaders\n",
    "\n",
    "    \n",
    "    training_params['label_range'] = 'label+estimated'\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    pretrain_dir =  get_pretrain_dir(training_params)\n",
    "    training_params['pretrain_dir'] = pretrain_dir\n",
    "    \n",
    "    \n",
    "    if 'regressor' not in training_params:\n",
    "#     training_params['regressor'] = 'DominantFreq_regressor'\n",
    "        training_params['regressor'] = None\n",
    "\n",
    "    \n",
    "#     training_params['model_type'] = model_dict[training_params['model_name']]\n",
    "    training_params['model_type'] = model_dict[training_params['variant']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ECG_SR',\n",
       " 'SCGxyz_AMpt',\n",
       " 'SCG_AMpt',\n",
       " 'SCGx_AMpt',\n",
       " 'SCGy_AMpt',\n",
       " 'accelX_resp',\n",
       " 'accelY_resp',\n",
       " 'accelZ_resp',\n",
       " 'ppg_g_1_resp',\n",
       " 'ppg_g_2_resp',\n",
       " 'ppg_ir_1_resp',\n",
       " 'ppg_ir_2_resp']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage3_dict['surrogate_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104., 105., 106., 107., 108., 110., 111., 113., 114., 115., 116.,\n",
       "       117., 118., 119., 120., 121., 212.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(training_params['subject_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputdir += '{}/{}/{}/{}/'.format(domain, modality, inputdir.split('/')[-2], TF_type)\n",
    "# # training_params\n",
    "\n",
    "# df_ppg_hr = pd.read_feather('../../data/stage3/cardiac/df_ppg_hr.feather')\n",
    "# df_ppg_hr[(df_ppg_hr['subject_id']=='106') & (df_ppg_hr['task_name']=='6MWT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir_df_ppg_selected =  '../../data/stage3/cardiac/df_ppg_selected.feather'\n",
    "# df_ppg_selected = pd.read_feather(outputdir_df_ppg_selected)\n",
    "# df_ppg_selected[df_ppg_selected['task_name']=='6MWT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve signal and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_sAttentLayer = False\n",
    "\n",
    "if debug_sAttentLayer:\n",
    "\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data =dataloaders['train'].dataset.data\n",
    "    meta =dataloaders['train'].dataset.meta\n",
    "    \n",
    "#     mask = meta[:,0]==4\n",
    "#     data = data[mask,:,:,:]\n",
    "#     meta = meta[mask,:]\n",
    "\n",
    "    i_sample = -15\n",
    "    i_sample = -1\n",
    "\n",
    "    fig,axes=plt.subplots(3,1,figsize=(10,8))\n",
    "    axes[0].imshow(data[i_sample,0,:,:].T)\n",
    "    axes[1].imshow(data[i_sample,1,:,:].T)\n",
    "    axes[2].imshow(data[i_sample,:,:,:].mean(axis=0).T)\n",
    "    plt.show()\n",
    "\n",
    "    atten_block = sAttentLayer2(training_params, N_freq=58, channel=1, reduction=2).to(training_params['device']).float()\n",
    "    data = torch.from_numpy(data).to(training_params['device']).float()\n",
    "\n",
    "    x = {}\n",
    "    \n",
    "    x = data\n",
    "\n",
    "    \n",
    "#     x['ECG'] = torch.concat([data[:,[0],:,:], data[:,[0],:,:]],dim=1)\n",
    "#     x['SCG'] = torch.concat([data[:,[1],:,:], data[:,[1],:,:]],dim=1)\n",
    "\n",
    "#     out, x, weights, outputs = atten_block(x)\n",
    "    out, weights = atten_block(x)\n",
    "    print(weights[i_sample, :])\n",
    "\n",
    "    fig,ax=plt.subplots(figsize=(4,2))\n",
    "\n",
    "    ax.plot(weights.detach().cpu().numpy()[:,0])\n",
    "    # plt.plot(meta[:,0]/200+0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    fig,axes=plt.subplots(3,1,figsize=(10,8))\n",
    "    axes[0].imshow(x[i_sample,0,:,:].detach().cpu().numpy().T )\n",
    "    axes[1].imshow(x[i_sample,1,:,:].detach().cpu().numpy().T )\n",
    "\n",
    "#     out_sum = (out['ECG'][i_sample,1,:,:].detach().cpu().numpy() + out['SCG'][i_sample,1,:,:].detach().cpu().numpy() ) / 2\n",
    "#     axes[2].imshow(out_sum)\n",
    "    axes[2].imshow(out[i_sample,0,:,:].detach().cpu().numpy().T)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m(aaa).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERNEL_SIZE = 3\n",
    "# groups = 1\n",
    "# channel = 1\n",
    "# N_freq = 58\n",
    "\n",
    "# conv = nn.Sequential(\n",
    "#     nn.Conv2d(channel, channel, kernel_size=KERNEL_SIZE, stride=1, padding='same', bias=True, groups=groups),\n",
    "#     nn.BatchNorm2d(channel),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.AvgPool2d(kernel_size=KERNEL_SIZE, stride=(KERNEL_SIZE,1), padding=(0,1)),\n",
    "#     nn.Conv2d(channel, channel, kernel_size=KERNEL_SIZE, stride=1, padding='same', bias=True, groups=groups),\n",
    "#     nn.BatchNorm2d(channel),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.AvgPool2d(kernel_size=KERNEL_SIZE, stride=(KERNEL_SIZE,1), padding=(0,1)),\n",
    "# ).to(training_params['device']).float()\n",
    "\n",
    "# avg_pool = nn.AdaptiveAvgPool2d((1, N_freq))\n",
    "# max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "# softmax = nn.Softmax(dim=-1)\n",
    "# fc = nn.Linear(N_freq,1).to(training_params['device']).float()\n",
    "\n",
    "\n",
    "# aaa = avg_pool(conv(x[:,[0],:,:]))\n",
    "# bbb = avg_pool(conv(x[:,[1],:,:]))\n",
    "\n",
    "# aaa = aaa / aaa.sum(dim=-1,keepdim=True)\n",
    "# bbb = bbb / bbb.sum(dim=-1,keepdim=True)\n",
    "\n",
    "# aaa_logit = fc(aaa.squeeze())\n",
    "# bbb_logit = fc(bbb.squeeze())\n",
    "\n",
    "# # aaa_max = max_pool(aaa).squeeze()\n",
    "# # bbb_max = max_pool(bbb).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# aaa.size()\n",
    "\n",
    "# aaa_logit.size()\n",
    "\n",
    "# weights = torch.stack([aaa_logit, bbb_logit]).T\n",
    "# weights = softmax(weights)\n",
    "\n",
    "# weights.size()\n",
    "\n",
    "# weights\n",
    "\n",
    "# self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "# fig,axes=plt.subplots(2,1,figsize=(10,8))\n",
    "# # axes[0].imshow(aaa.detach().cpu().numpy()[i_sample,0,:,:].T)\n",
    "# # axes[1].imshow(bbb.detach().cpu().numpy()[i_sample,0,:,:].T)\n",
    "# axes[0].plot(aaa.detach().cpu().numpy()[i_sample,0,:,:].squeeze())\n",
    "# axes[0].plot(bbb.detach().cpu().numpy()[i_sample,0,:,:].squeeze())\n",
    "\n",
    "# print(aaa_max.squeeze()[i_sample], bbb_max.squeeze()[i_sample])\n",
    "\n",
    "# #     axes[2].imshow(data[i_sample,:,:,:].mean(axis=0).T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unet_extension.models.UNet"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (Maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv0): Sequential(\n",
      "    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "    (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (atten_block): sAttentLayer2(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 58))\n",
      "    (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (max_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "    (activation): Sigmoid()\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (Conv1): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv2): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv3): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up3): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up_conv3): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up2): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up_conv2): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv): Conv2d(4, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def test_model(training_params):\n",
    "    \n",
    "    model_def = training_params['model_type']\n",
    "    device = training_params['device']\n",
    "#     model= model_def(in_ch=1, out_ch=2, n1=training_params['N_channels']).to(device).float()\n",
    "#     if training_params['model_name']=='UNet':\n",
    "#         model= model_def(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "#     elif training_params['model_name']=='Late_UNet':\n",
    "#         model= model_def(training_params=training_params)\n",
    "    model= model_def(training_params=training_params)\n",
    "\n",
    "    data_dimensions = training_params['data_dimensions'].copy()\n",
    "    \n",
    "    if training_params['flipper']:\n",
    "        data_dimensions[-1] = data_dimensions[-1] * 2 \n",
    "\n",
    "    print(model)\n",
    "    \n",
    "#     summary(model, input_size=tuple(data_dimensions), batch_size=16, device='cpu')\n",
    "    \n",
    "test_model(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit test: check if model saved and loaded are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputdir: ../../data/stage4_UNet_test/GT_dataset/ECG-SR+ECG-SR/AT_block/prepare/\n",
      "Models match perfectly! :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.018s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "def compare_models(model_1, model_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismtach found at', key_item_1[0])\n",
    "            else:\n",
    "                raise Exception\n",
    "    if models_differ == 0:\n",
    "        print('Models match perfectly! :)')\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class TestModelFunctions(unittest.TestCase):\n",
    "    \"\"\"Tests for a pileline output\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print('outputdir:', outputdir)\n",
    "        \n",
    "    def test_save_load(self):\n",
    "        \n",
    "        model_def = training_params['model_type']\n",
    "\n",
    "#         if training_params['model_name']=='UNet':\n",
    "#             model1 = model_def(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "#         elif training_params['model_name']=='Late_UNet':\n",
    "#             model1 = model_def(training_params=training_params)\n",
    "        model1 = model_def(training_params=training_params)\n",
    "\n",
    "#         model1= training_params['model_type'](in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "\n",
    "        torch.save(model1.state_dict(), outputdir+'model_weights.pth')\n",
    "\n",
    "\n",
    "#         if training_params['model_name']=='UNet':\n",
    "#             model2 = model_def(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "#         elif training_params['model_name']=='Late_UNet':\n",
    "#             model2 = model_def(training_params=training_params)\n",
    "#         model2= training_params['model_type'](in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "        model2 = model_def(training_params=training_params)\n",
    "\n",
    "        model2.load_state_dict(torch.load(outputdir+'model_weights.pth'))\n",
    "\n",
    "        self.assertTrue(compare_models(model1, model2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputdirs(training_params):\n",
    "\n",
    "#     outputdir = training_params['outputdir']\n",
    "#     sweep_folder = get_sweep_folder(training_params)\n",
    "#     outputdir_sweep = outputdir+'{}/'.format(sweep_folder)\n",
    "\n",
    "    outputdir_sweep = outputdir\n",
    "    \n",
    "    outputdir_numeric = outputdir_sweep + 'numeric_results/'\n",
    "    if outputdir_numeric is not None:\n",
    "        if not os.path.exists(outputdir_numeric):\n",
    "            os.makedirs(outputdir_numeric)\n",
    "\n",
    "    outputdir_modelout = outputdir_sweep + 'model_output/'\n",
    "    if outputdir_modelout is not None:\n",
    "        if not os.path.exists(outputdir_modelout):\n",
    "            os.makedirs(outputdir_modelout)\n",
    "\n",
    "    outputdir_activation = outputdir_sweep + 'activation_layers/'\n",
    "    if outputdir_activation is not None:\n",
    "        if not os.path.exists(outputdir_activation):\n",
    "            os.makedirs(outputdir_activation)\n",
    "            \n",
    "    outputdir_featuremap = outputdir_sweep + 'feature_maps/'\n",
    "    if outputdir_featuremap is not None:\n",
    "        if not os.path.exists(outputdir_featuremap):\n",
    "            os.makedirs(outputdir_featuremap)\n",
    "\n",
    "#     outputdir_feature = outputdir_sweep + 'feature_visualization/'\n",
    "#     if outputdir_feature is not None:\n",
    "#         if not os.path.exists(outputdir_feature):\n",
    "#             os.makedirs(outputdir_feature)\n",
    "\n",
    "    training_params['outputdir_sweep'] = outputdir_sweep\n",
    "    training_params['outputdir_numeric'] = outputdir_numeric\n",
    "    training_params['outputdir_modelout'] = outputdir_modelout\n",
    "    training_params['outputdir_activation'] = outputdir_activation\n",
    "    training_params['outputdir_featuremap'] = outputdir_featuremap\n",
    "\n",
    "#     training_params['outputdir_feature'] = outputdir_feature\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'RR_cosmed'\n",
    "\n",
    "df_performance_train = {}\n",
    "df_performance_val = {}\n",
    "df_performance_val_input = {}\n",
    "df_performance_val_exp = {}\n",
    "\n",
    "df_outputlabel_train = {}\n",
    "df_outputlabel_val = {}\n",
    "df_outputlabel_val_input = {}\n",
    "\n",
    "df_performance_train[task] = pd.DataFrame()\n",
    "df_performance_val[task] = pd.DataFrame()\n",
    "df_performance_val_exp[task] = pd.DataFrame()\n",
    "df_performance_val_input[task] = pd.DataFrame()\n",
    "\n",
    "df_outputlabel_train[task] = pd.DataFrame()\n",
    "df_outputlabel_val[task] = pd.DataFrame()\n",
    "df_outputlabel_val_input[task] = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: need to change pretrain dir so GT_dataset load model from CDC_dataset dir, and CDC_dataset load model from GT_dataset dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================ TRAIN / TEST ================================\n",
      "begining soon...\n",
      "\tusing pretrain_dir: /home/mchan/Estimation_EE/data/stage4_UNet/CDC_dataset/ECG-SR+ECG-SR/AT_block/prepare/\n",
      "[sub -1] LOSO\n",
      "\tinitialize a new model, train from scratch\n",
      "\ttraining the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n================================ TRAIN / TEST ================================')\n",
    "print('begining soon...')\n",
    "start_time = time.time()\n",
    "\n",
    "# use_pretrained = True\n",
    "\n",
    "# Determining which pretrain model to use is a bit involved. Please read get_pretrain_dir carefully\n",
    "pretrain_dir = training_params['pretrain_dir']\n",
    "print('\\tusing pretrain_dir:', pretrain_dir)\n",
    "\n",
    "device = training_params['device']\n",
    "\n",
    "training_params = get_outputdirs(training_params) # could be tricky since it changes several keys\n",
    "model_def = training_params['model_type']\n",
    "\n",
    "\n",
    "\n",
    "vis_act = False\n",
    "\n",
    "\n",
    "vis_filt = True\n",
    "vis_feat = True\n",
    "\n",
    "# for i_CV, subject_id in enumerate(ordered_subject_ids):\n",
    "for subject_id in training_params['subject_ids']:\n",
    "    \n",
    "#     if subject_id!=22:\n",
    "#         continue\n",
    "\n",
    "    # stop wasting your time on these reject_subject_id!\n",
    "    if subject_id in training_params['CV_config']['reject_subject_id']:\n",
    "        continue\n",
    "\n",
    "    # subject_id = -1 implies using all of the data for training\n",
    "#     if (training_params['TF_type']=='pretrain') or (training_params['TF_type']=='prepare'):\n",
    "    if (training_params['TF_type']=='prepare'):\n",
    "        subject_id = -1\n",
    "\n",
    "    # set CV_config-subject_id to subject_id so get_loaders can get the right loader for ya\n",
    "    training_params['CV_config']['subject_id'] = subject_id\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    \n",
    "    # LOSO obviously\n",
    "    print('[sub {}] LOSO'.format( int(subject_id) ) )\n",
    "    \n",
    "    # just timing it, may remove it later\n",
    "    get_loaders_time = time.time()\n",
    "\n",
    "\n",
    "    # trained with the source (GT dataset), test on the target (CDC dataset)\n",
    "    # need to import the pretrained model if 'source' or 'FT_' are in training_params['TF_type']\n",
    "    if ('source' in training_params['TF_type']) or ('FT_' in training_params['TF_type']) or ('pretrain' in training_params['TF_type']):\n",
    "        print('\\tload a model trained using the GT dataset (dont initialize a model from scratch)')\n",
    "        total_loss_train, total_loss_val = 0, 0\n",
    "\n",
    "        # load the training_params for the pretrain model\n",
    "        # TODO: write some tests to make sure it is parsing the right model\n",
    "        # if pretrain model and input # don't match, maybe replace the first conv layer?\n",
    "        training_params_pretrain = data_loader('training_params', pretrain_dir).item()\n",
    "        pretrain_channel = len(training_params_pretrain['input_names'])\n",
    "        \n",
    "        # get the pretrained model\n",
    "#         model= U_Net(in_ch=pretrain_channel, out_ch=2, n1 = training_params['N_channels'], training_params=training_params)        \n",
    "        model= model_def(training_params=training_params)        \n",
    "        model.load_state_dict(torch.load(pretrain_dir+'model_weights.pth'))\n",
    "        \n",
    "        # TBD: make sure pretrain model has the same channel dimension as the data channel #\n",
    "        # for if pretrain model and input # don't match, replace the first conv layer\n",
    "#         if pretrain_channel!=training_params['data_dimensions'][0]:\n",
    "#             print('\\tpretrain model has different input dimension compared to data. Replace the first conv layer.')\n",
    "#             model_template = U_Net(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "#             model.Conv1.conv[0] = model_template.Conv1.conv[0]\n",
    "#             del model_template\n",
    "\n",
    "        # FT_top and FT_top2 may not be used anymore since it doesn't fit the research objective (6/29)\n",
    "        # for if FT_top, replace the first layer\n",
    "        if 'FT_top' in training_params['TF_type']:\n",
    "            print('\\tfine-tune the top layer')\n",
    "#             model_template = U_Net(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "            model_template= model_def(training_params=training_params)\n",
    "            model.Conv1.conv[0] = model_template.Conv1.conv[0]\n",
    "            del model_template\n",
    "\n",
    "        # for if FT_top2, replace the first two layers\n",
    "        elif 'FT_top2' in training_params['TF_type']:\n",
    "            print('\\tfine-tune the top 2 layers')\n",
    "#             model_template = U_Net(in_ch=training_params['data_dimensions'][0], out_ch=2, n1=training_params['N_channels'], training_params=training_params)\n",
    "            model_template= model_def(training_params=training_params)\n",
    "            model.Conv1.conv[0] = model_template.Conv1.conv[0]\n",
    "            model.Conv1.conv[3] = model_template.Conv1.conv[3]\n",
    "            del model_template\n",
    "\n",
    "    # if training_params['TF_type']=='target' or 'pretrain', will train from scratch\n",
    "    else:\n",
    "        print('\\tinitialize a new model, train from scratch')\n",
    "#         model= U_Net(in_ch=training_params['data_dimensions'][0], out_ch=2, n1 = training_params['N_channels'], training_params=training_params)\n",
    "        model= model_def(training_params=training_params)\n",
    "\n",
    "\n",
    "    \n",
    "    model= model.to(device).float()\n",
    "\n",
    "    # if training_params['TF_type']=='source', will not train (simply feed data into model trained using the GT dataset)\n",
    "    if training_params['TF_type']!='source':\n",
    "        print('\\ttraining the model...')\n",
    "        model, total_loss_train, total_loss_val = train_model(model, dataloaders, training_params)\n",
    "    else:\n",
    "        print('\\tdont train the model...')\n",
    "\n",
    "    \n",
    "#     sys.exit()\n",
    "\n",
    "    train_model_time = time.time()\n",
    "\n",
    "\n",
    "    plot_loss(total_loss_train, total_loss_val, outputdir=training_params['outputdir_sweep']+'TRAIN-TEST/')\n",
    "\n",
    "    # (~0.3sec)\n",
    "    print('\\tgetting performance for TRAIN and TEST...')\n",
    "    performance_dict_TRAIN, model_out_dict_TRAIN = get_performance(model, dataloaders['train_eval'], training_params)\n",
    "    performance_dict_TEST, model_out_dict_TEST = get_performance(model, dataloaders['val'], training_params)\n",
    "\n",
    "    \n",
    "    sys.exit()\n",
    "#     # model out is based on interpolated labels\n",
    "#     plot_unet_results(model_out_dict_TRAIN, training_params, title_str='TRAIN', outputdir=outputdir+'TRAIN-TEST/')\n",
    "    plot_unet_results(model_out_dict_TEST, training_params, title_str='TEST', outputdir=training_params['outputdir_sweep']+'TRAIN-TEST/')\n",
    "\n",
    "    #             print( CV_dict['performance_dict_val']['out_dict'].keys(), CV_dict['performance_dict_val']['label_dict'].keys(), task)\n",
    "    label_est_val = model_out_dict_TEST['RR_model'].squeeze()*60\n",
    "#     label_expectation_val = model_out_dict_TEST['RR_expectation'].squeeze()\n",
    "    label_val = model_out_dict_TEST['RR_label'].squeeze()*60\n",
    "    label_input_val = model_out_dict_TEST['RR_input_spectral'].squeeze()*60\n",
    "    \n",
    "    RQI_fft_val = get_RQI_fft(model_out_dict_TEST['out_concat'][:,0,:])\n",
    "    RQI_kurtosis_val = get_RQI_kurtosis(model_out_dict_TEST['out_concat'][:,0,:])\n",
    "\n",
    "    # WIP\n",
    "    VT_val = model_out_dict_TEST['meta'][:, training_params['i_VT']]\n",
    "    HR_val = model_out_dict_TEST['meta'][:, training_params['i_HR']]\n",
    "\n",
    "    label_est_train = model_out_dict_TRAIN['RR_model'].squeeze()*60\n",
    "#     label_expectation_train = model_out_dict_TRAIN['RR_expectation'].squeeze()\n",
    "    label_train = model_out_dict_TRAIN['RR_label'].squeeze()*60\n",
    "    \n",
    "    RQI_fft_train = get_RQI_fft(model_out_dict_TRAIN['out_concat'][:,0,:])\n",
    "    RQI_kurtosis_train = get_RQI_kurtosis(model_out_dict_TRAIN['out_concat'][:,0,:])\n",
    "    \n",
    "    ts_train = model_out_dict_TRAIN['ts']\n",
    "    ts_val = model_out_dict_TEST['ts']\n",
    "\n",
    "    # WIP\n",
    "    VT_train = model_out_dict_TRAIN['meta'][:, training_params['i_VT']]\n",
    "    HR_train = model_out_dict_TRAIN['meta'][:, training_params['i_HR']]\n",
    "\n",
    "#     kurtosis_train = get_kurtosis(model_out_dict_TRAIN['out_concat'][:,0,:].T)\n",
    "\n",
    "#     label_train = model_out_dict_TRAIN['RR_input_spectral'].squeeze()*60\n",
    "\n",
    "    # get performance df for training and testing dataset\n",
    "    df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, subject_id, task), ignore_index=True )\n",
    "    df_performance_train[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_train_{}.csv'.format(task), index=False)\n",
    "    \n",
    "\n",
    "    df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "        pd.DataFrame( {\n",
    "        'label_est': label_est_train,\n",
    "        'label': label_train,\n",
    "        'task': [task]*label_train.shape[0],\n",
    "        'CV': model_out_dict_TRAIN['meta'][:,0],\n",
    "        'activity': model_out_dict_TRAIN['meta'][:,1],\n",
    "        'RQI_fft': RQI_fft_train,\n",
    "        'RQI_kurtosis': RQI_kurtosis_train,\n",
    "\n",
    "        'VT_train': VT_train,\n",
    "        'HR_train': HR_train,\n",
    "            \n",
    "        'ts_train': ts_train,\n",
    "        }), ignore_index=True )\n",
    "\n",
    "    df_outputlabel_train[task].to_csv(training_params['outputdir_numeric'] + 'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "\n",
    "    # RR estimated from model for val dataset\n",
    "    df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, subject_id, task), ignore_index=True )\n",
    "    df_performance_val[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_val_{}.csv'.format(task), index=False)\n",
    "    # RR expectation from model for val dataset\n",
    "#     df_performance_val_exp[task] = df_performance_val_exp[task].append( get_df_performance(label_val, label_expectation_val, subject_id, task), ignore_index=True )\n",
    "#     df_performance_val_exp[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_val_exp_{}.csv'.format(task), index=False)\n",
    "\n",
    "    df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "        pd.DataFrame( {\n",
    "        'label_est': label_est_val,\n",
    "#         'label_expectation_val': label_expectation_val,\n",
    "        'label': label_val,\n",
    "        'task': [task]*label_val.shape[0],\n",
    "        'CV': model_out_dict_TEST['meta'][:,0],\n",
    "        'activity': model_out_dict_TEST['meta'][:,1],\n",
    "        'RQI_fft': RQI_fft_val,\n",
    "        'RQI_kurtosis': RQI_kurtosis_val,\n",
    "            \n",
    "        'VT_val': VT_val,\n",
    "        'HR_val': HR_val,\n",
    "            \n",
    "        'ts_val': ts_val,\n",
    "        }), ignore_index=True )\n",
    "\n",
    "    df_outputlabel_val[task].to_csv(training_params['outputdir_numeric'] + 'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "\n",
    "#     plot_regression(df_outputlabel_val[task], df_performance_val[task], task, training_params, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'] )\n",
    "    plot_regression(df_outputlabel_val[task], task, training_params, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'] )\n",
    "\n",
    "\n",
    "    \n",
    "    # RR estimated from input for val dataset\n",
    "    df_performance_val_input[task] = df_performance_val_input[task].append( get_df_performance(label_val, label_input_val, subject_id, task), ignore_index=True )\n",
    "    df_performance_val_input[task].to_csv(training_params['outputdir_numeric'] + 'df_performance_val_input_{}.csv'.format(task), index=False)\n",
    "\n",
    "    df_outputlabel_val_input[task] = df_outputlabel_val_input[task].append(\n",
    "        pd.DataFrame( {\n",
    "        'label_est': label_input_val,\n",
    "        'label': label_val,\n",
    "        'task': [task]*label_val.shape[0],\n",
    "        'CV': model_out_dict_TEST['meta'][:,0],\n",
    "        'activity': model_out_dict_TEST['meta'][:,1],\n",
    "        'RQI_fft': RQI_fft_val,\n",
    "        'RQI_kurtosis': RQI_kurtosis_val,\n",
    "            \n",
    "        'VT_val': VT_val,\n",
    "        'HR_val': HR_val,\n",
    "        \n",
    "        'ts_val': ts_val\n",
    "        }), ignore_index=True )\n",
    "\n",
    "    df_outputlabel_val_input[task].to_csv(training_params['outputdir_numeric'] + 'df_outputlabel_val_input_{}.csv'.format(task), index=False)\n",
    "\n",
    "    plot_regression(df_outputlabel_val_input[task], task, training_params, fig_name='regression_val_input_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'] )\n",
    "    plot_regression(df_outputlabel_val_input[task], task, training_params, fig_name='regression_val_input_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'] )\n",
    "\n",
    "    #             plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "    \n",
    "\n",
    "#     sys.exit()\n",
    "\n",
    "    print('\\tvisualizing results...')\n",
    "    # TODO: implement this\n",
    "    # plot the input\n",
    "    # plot ouput after each maxpool\n",
    "    # plot attention map (the last y in SE block)\n",
    "    if vis_act:\n",
    "        check_attention(model, dataloaders, training_params, mode='best', fig_name = 'IO_weights_{}'.format(int(subject_id)), outputdir=training_params['outputdir_activation']+'best/', show_plot=False)\n",
    "        check_attention(model, dataloaders, training_params, mode='worst', fig_name = 'IO_weights_{}'.format(int(subject_id)), outputdir=training_params['outputdir_activation']+'worst/', show_plot=False)\n",
    "        check_attention(model, dataloaders, training_params, mode='random', fig_name = 'IO_weights_{}'.format(int(subject_id)), outputdir=training_params['outputdir_activation']+'random/', show_plot=False)\n",
    "\n",
    "    if vis_filt:\n",
    "        \n",
    "#         if training_params['model_name'] == 'UNet':\n",
    "        if training_params['variant'] == 'baseline':\n",
    "            input_name = None\n",
    "            check_filters(model, training_params, input_choice=input_name, outputdir=training_params['outputdir_featuremap'])\n",
    "\n",
    "        elif training_params['model_name'] == 'Late_UNet':\n",
    "            for input_name in training_params['input_names']:\n",
    "                check_filters(model, training_params, input_choice=input_name, outputdir=training_params['outputdir_featuremap'])\n",
    "\n",
    "    if vis_feat:\n",
    "#         if training_params['model_name'] == 'UNet':\n",
    "        if training_params['variant'] == 'baseline':\n",
    "            input_name = None\n",
    "            check_featuremap(model, dataloaders, training_params, mode='best', input_choice=input_name, fig_name = 'featuremap_{}_{}'.format(input_name, int(subject_id)), outputdir=training_params['outputdir_featuremap']+'best/', show_plot=False)\n",
    "            check_featuremap(model, dataloaders, training_params, mode='worst', input_choice=input_name, fig_name = 'featuremap_{}_{}'.format(input_name, int(subject_id)), outputdir=training_params['outputdir_featuremap']+'worst/', show_plot=False)\n",
    "            check_featuremap(model, dataloaders, training_params, mode='random', input_choice=input_name, fig_name = 'featuremap_{}_{}'.format(input_name, int(subject_id)), outputdir=training_params['outputdir_featuremap']+'random/', show_plot=False)\n",
    "\n",
    "            if training_params['variant']=='AT_block':\n",
    "                check_weights(model, dataloaders, training_params, fig_name = 'weights_{}'.format(int(subject_id)), outputdir=training_params['outputdir_featuremap'], show_plot=False)\n",
    "\n",
    "#         elif training_params['model_name'] == 'Late_UNet':\n",
    "        elif training_params['variant'] == 'Late_UNet':\n",
    "            for input_name in training_params['input_names']:\n",
    "                check_featuremap(model, dataloaders, training_params, mode='best', input_choice=input_name, fig_name = 'featuremap_{}_{}'.format(input_name, int(subject_id)), outputdir=training_params['outputdir_featuremap']+'best/', show_plot=False)\n",
    "                check_featuremap(model, dataloaders, training_params, mode='worst', input_choice=input_name, fig_name = 'featuremap_{}_{}'.format(input_name, int(subject_id)), outputdir=training_params['outputdir_featuremap']+'worst/', show_plot=False)\n",
    "\n",
    "\n",
    "    \n",
    "    # TODO: implement this\n",
    "    # plot the conv folters\n",
    "#     plot_filters(model, fig_name = 'filters_{}'.format(subject_id), outputdir=training_params['outputdir_activation']+'filters/', show_plot=False)\n",
    "\n",
    "    torch.save(model.state_dict(), training_params['outputdir_sweep'] + 'model_weights.pth')\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "#     if (training_params['TF_type']=='pretrain') or (training_params['TF_type']=='prepare'):\n",
    "#         break\n",
    "    if (training_params['TF_type']=='prepare'):\n",
    "        break\n",
    "\n",
    "    \n",
    "# def plot_regression_all_agg(df_outputlabel, df_performance, training_params, fig_name=None, outputdir=None, show_plot=False, log_wandb=False):\n",
    "\n",
    "# plot_regression_all_agg(df_outputlabel_train[task], df_performance_train[task], training_params, fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "plot_regression_all_agg(df_outputlabel_train[task], training_params, fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "\n",
    "# plot_regression_all_agg(df_outputlabel_val[task], df_performance_val[task], training_params, fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "plot_regression_all_agg(df_outputlabel_val[task], training_params, fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=training_params['outputdir_modelout'], log_wandb=training_params['wandb'])\n",
    "\n",
    "plot_output(df_outputlabel_val[task], task, fig_name = 'outputINtime_val_{}'.format(task),  show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "\n",
    "    \n",
    "print('\\nDONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_out_dict_TRAIN['data_concat'].shape\n",
    "training_params['xf_masked'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params['xf_masked']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver(training_params, 'training_params', outputdir)\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_featuremap(model, dataloaders, training_params, mode='random', fig_name=None, show_plot=False, outputdir=None, log_wandb=False, verbose=False):\n",
    "#     # mode='worst'\n",
    "#     data, out, label, RR_model, RR_label, activation = get_data_activation(model, dataloaders, training_params, verbose=True)\n",
    "\n",
    "#     b, c, h, w = data.shape    \n",
    "#     error_abs = np.abs(RR_model - RR_label).squeeze()\n",
    "\n",
    "#     if mode=='worst':\n",
    "#         i_sample = np.argmax(error_abs)\n",
    "#     if mode=='best':\n",
    "#         i_sample = np.argmin(error_abs)\n",
    "#     if mode=='random':\n",
    "#         # check one sample only\n",
    "#         i_sample = np.random.randint(b)\n",
    "\n",
    "#     fig = plt.figure(figsize=(15,30), constrained_layout=True, dpi=100)\n",
    "#     gs = gridspec.GridSpec(16, 8)\n",
    "\n",
    "#     ts = np.arange(h)*3 # 1Hz\n",
    "#     RR_range = label_range_dict['RR']\n",
    "\n",
    "#     extent = [ts[0], ts[-1], RR_range[0], RR_range[-1]]\n",
    "\n",
    "#     # 1. plot the input\n",
    "#     vmin = data[i_sample,:,:,:].min()\n",
    "#     vmax = data[i_sample,:,:,:].max()\n",
    "#     for i_input, input_name in enumerate(training_params['input_names']):\n",
    "#         ax = fig.add_subplot(gs[i_input, 0])\n",
    "#         ax.imshow(data[i_sample,i_input,:,:].T,  cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title(input_name)\n",
    "\n",
    "#     #     'Conv1', 'Conv2', 'Conv3', 'Up_conv3', 'Up_conv2'\n",
    "#     # 2. plot the 1st average pooling output in SE block\n",
    "#     key = 'Conv1'\n",
    "#     vmin = activation[key][i_sample,:,:,:].min()\n",
    "#     vmax = activation[key][i_sample,:,:,:].max()\n",
    "#     for i_ch in range(activation[key].shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_ch, 1])\n",
    "#         ax.imshow(activation[key][i_sample,i_ch,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('i_ch: {}\\n{}'.format(i_ch, key))\n",
    "\n",
    "\n",
    "#     key = 'Conv2'\n",
    "#     vmin = activation[key][i_sample,:,:,:].min()\n",
    "#     vmax = activation[key][i_sample,:,:,:].max()\n",
    "#     for i_ch in range(activation[key].shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_ch, 2])\n",
    "#         ax.imshow(activation[key][i_sample,i_ch,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('i_ch: {}\\n{}'.format(i_ch, key))\n",
    "\n",
    "\n",
    "#     key = 'Conv3'\n",
    "#     vmin = activation[key][i_sample,:,:,:].min()\n",
    "#     vmax = activation[key][i_sample,:,:,:].max()\n",
    "#     for i_ch in range(activation[key].shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_ch, 3])\n",
    "#         ax.imshow(activation[key][i_sample,i_ch,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('i_ch: {}\\n{}'.format(i_ch, key))\n",
    "\n",
    "#     key = 'Up_conv3'\n",
    "#     vmin = activation[key][i_sample,:,:,:].min()\n",
    "#     vmax = activation[key][i_sample,:,:,:].max()\n",
    "#     for i_ch in range(activation[key].shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_ch, 4])\n",
    "#         ax.imshow(activation[key][i_sample,i_ch,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('i_ch: {}\\n{}'.format(i_ch, key))\n",
    "\n",
    "#     key = 'Up_conv2'\n",
    "#     vmin = activation[key][i_sample,:,:,:].min()\n",
    "#     vmax = activation[key][i_sample,:,:,:].max()\n",
    "#     for i_ch in range(activation[key].shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_ch, 5])\n",
    "#         ax.imshow(activation[key][i_sample,i_ch,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('i_ch: {}\\n{}'.format(i_ch, key))\n",
    "\n",
    "#     # 6. plot output\n",
    "#     vmin = out[i_sample,:,:,:].min()\n",
    "#     vmax = out[i_sample,:,:,:].max()\n",
    "#     #     print(vmin, vmax)\n",
    "#     for i_out in range(out.shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_out, 6])\n",
    "#         ax.imshow(out[i_sample, i_out,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('out ch: {}'.format(i_out) )\n",
    "\n",
    "#     vmin = label[i_sample,:2,:,:].min()\n",
    "#     vmax = label[i_sample,:2,:,:].max()\n",
    "#     for i_label in range(label.shape[1]-1):\n",
    "#         ax = fig.add_subplot(gs[i_label, 7])\n",
    "#         ax.imshow(label[i_sample, i_label,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('label ch: {}'.format(i_label) )\n",
    "\n",
    "#     fig.subplots_adjust(wspace=0.2, hspace=0.8)\n",
    "\n",
    "# #     if fig_name is None:\n",
    "# #         fig_name = 'featuremap'\n",
    "\n",
    "# #     fig.suptitle(fig_name+'\\ni_sample:{}'.format(i_sample), fontsize=20)\n",
    "\n",
    "#     # if log_wandb:\n",
    "#     #     wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "#     # if outputdir is not None:\n",
    "#     #     if not os.path.exists(outputdir):\n",
    "#     #         os.makedirs(outputdir)\n",
    "\n",
    "#     #     fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "#     # if show_plot == False:\n",
    "#     #     plt.close(fig)\n",
    "#     #     pyplot.close(fig)\n",
    "#     #     plt.close('all')\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "#     fig.subplots_adjust(wspace=0.2, hspace=0.8)\n",
    "\n",
    "#     if fig_name is None:\n",
    "#         fig_name = 'featuremap'\n",
    "\n",
    "#     fig.suptitle(fig_name+'\\ni_sample:{}'.format(i_sample) + '\\nRRout: {:.3f} RRlabel: {:.3f} AE: {:.3f} [BPM]'.format(\n",
    "#         RR_model[i_sample].squeeze(), RR_label[i_sample].squeeze(), error_abs[i_sample]), fontsize=15)\n",
    "    \n",
    "#     if log_wandb:\n",
    "#         wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "#     if outputdir is not None:\n",
    "#         if not os.path.exists(outputdir):\n",
    "#             os.makedirs(outputdir)\n",
    "\n",
    "#         fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "#     if show_plot == False:\n",
    "#         plt.close(fig)\n",
    "#         pyplot.close(fig)\n",
    "#         plt.close('all')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_activation(model, dataloaders, training_params, mode='random', fig_name=None, show_plot=False, outputdir=None, log_wandb=False, verbose=False):\n",
    "#     data, out, label, RR_model, RR_label, activation = get_data_activation(model, dataloaders, training_params)\n",
    "#     b, c, h, w = data.shape    \n",
    "#     error_abs = np.abs(RR_model - RR_label).squeeze()\n",
    "    \n",
    "    \n",
    "#     if mode=='worst':\n",
    "#         i_sample = np.argmax(error_abs)\n",
    "#     if mode=='best':\n",
    "#         i_sample = np.argmin(error_abs)\n",
    "#     if mode=='random':\n",
    "#         # check one sample only\n",
    "# #         np.random.seed(0)\n",
    "#         i_sample = np.random.randint(b)\n",
    "\n",
    "# #     print(error_abs)\n",
    "# #     print(error_abs[i_sample])\n",
    "# #     print(error_abs.min(), error_abs.max())\n",
    "\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(15,8), constrained_layout=True, dpi=100)\n",
    "#     gs = gridspec.GridSpec(4, 8)\n",
    "\n",
    "#     # gs.update(wspace = 0.5, hspace = 0.8)\n",
    "#     # i_sample = 50\n",
    "\n",
    "\n",
    "#     ts = np.arange(h)*3 # 1Hz\n",
    "#     RR_range = label_range_dict['RR']\n",
    "\n",
    "#     extent = [ts[0], ts[-1], RR_range[0], RR_range[-1]]\n",
    "\n",
    "#     # 1. plot the input\n",
    "#     vmin = data[i_sample,:,:,:].min()\n",
    "#     vmax = data[i_sample,:,:,:].max()\n",
    "#     for i_input, input_name in enumerate(training_params['input_names']):\n",
    "#         ax = fig.add_subplot(gs[i_input, 0])\n",
    "#         ax.imshow(data[i_sample,i_input,:,:].T,  cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title(input_name)\n",
    "\n",
    "#     # 2. plot the 1st average pooling output in SE block\n",
    "#     vmin = activation['SE1_conv1'][i_sample,:,:,:].min()\n",
    "#     vmax = activation['SE1_conv1'][i_sample,:,:,:].max()\n",
    "#     for i_input, input_name in enumerate(training_params['input_names']):\n",
    "#         ax = fig.add_subplot(gs[i_input, 1])\n",
    "#         ax.imshow(activation['SE1_conv1'][i_sample,i_input,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title(input_name+'\\nSE1_conv1')\n",
    "        \n",
    "#     # 2. plot the 1st average pooling output in SE block\n",
    "#     vmin = activation['SE1_conv2'][i_sample,:,:,:].min()\n",
    "#     vmax = activation['SE1_conv2'][i_sample,:,:,:].max()\n",
    "#     for i_input, input_name in enumerate(training_params['input_names']):\n",
    "#         ax = fig.add_subplot(gs[i_input, 2])\n",
    "#         ax.imshow(activation['SE1_conv2'][i_sample,i_input,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title(input_name+'\\nSE1_conv2')\n",
    "\n",
    "#     # 5. plot the weights for each input\n",
    "#     ax = fig.add_subplot(gs[:, 3])\n",
    "#     ax.barh(training_params['input_names'][::-1], activation['SE1_avg_pool'].squeeze()[i_sample,:][::-1], height=0.1)\n",
    "#     ax.set_title('SE1_avg_pool')\n",
    "#     ax_no_top_right(ax)\n",
    "    \n",
    "# #     5. plot the weights for each input\n",
    "#     ax = fig.add_subplot(gs[:, 4])\n",
    "# #     ax.barh(training_params['input_names'][::-1], activation['SE1_fc1'][i_sample,:][::-1], height=0.1)\n",
    "#     ax.barh(np.arange(activation['SE1_fc1'].shape[-1]), activation['SE1_fc1'][i_sample,:][::-1], height=0.1)\n",
    "#     ax.set_title('SE1_fc1')\n",
    "#     ax_no_top_right(ax)\n",
    "    \n",
    "#     # 5. plot the weights for each input\n",
    "#     ax = fig.add_subplot(gs[:, 5])\n",
    "#     ax.barh(training_params['input_names'][::-1], activation['SE1_fc2'][i_sample,:][::-1], height=0.1)\n",
    "#     ax.set_title('SE1_fc2')\n",
    "#     ax_no_top_right(ax)\n",
    "\n",
    "#     # 6. plot the weights for each input\n",
    "#     ax = fig.add_subplot(gs[:, 6])\n",
    "#     ax.barh(training_params['input_names'][::-1], activation['SE1_weight'][i_sample,:][::-1], height=0.1)\n",
    "#     ax.set_title('SE1_weight')\n",
    "#     ax_no_top_right(ax)\n",
    "    \n",
    "#     if verbose:\n",
    "# #         print('SE1_conv1', vmin, vmax)\n",
    "\n",
    "# #         print('input', vmin, vmax)\n",
    "\n",
    "# #         print('SE1_conv2', vmin, vmax)\n",
    "\n",
    "#         print('SE1_avg_pool', activation['SE1_avg_pool'][i_sample,:])\n",
    "\n",
    "#         print('SE1_fc1', activation['SE1_fc1'][i_sample,:])\n",
    "\n",
    "#         print('SE1_fc2', activation['SE1_fc2'][i_sample,:])\n",
    "\n",
    "#         print('SE1_weight', activation['SE1_weight'][i_sample,:])\n",
    "    \n",
    "# #     # 6. plot the weights for each input\n",
    "# #     ax = fig.add_subplot(gs[:, 6])\n",
    "# #     ax.barh(training_params['input_names'][::-1], activation['SE1_conv'][i_sample,:][::-1], height=0.1)\n",
    "# #     ax.set_title('SE1_conv')\n",
    "# #     ax_no_top_right(ax)\n",
    "# #     print('SE1_conv', activation['SE1_conv'][i_sample,:])\n",
    "\n",
    "\n",
    "#     # 6. plot output\n",
    "#     vmin = out[i_sample,:,:,:].min()\n",
    "#     vmax = out[i_sample,:,:,:].max()\n",
    "# #     print(vmin, vmax)\n",
    "#     for i_out in range(out.shape[1]):\n",
    "#         ax = fig.add_subplot(gs[i_out, -1])\n",
    "#         ax.imshow(out[i_sample, i_out,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#         ax.set_title('out ch: {}'.format(i_out) )\n",
    "        \n",
    "\n",
    "# #     print(label.shape)\n",
    "#     vmin = label[i_sample,-1,:,:].min()\n",
    "#     vmax = label[i_sample,-1,:,:].max()\n",
    "# #     for i_label in range(label.shape[1]):\n",
    "#     ax = fig.add_subplot(gs[i_out+2, -1])\n",
    "#     ax.imshow(label[i_sample, -1,:,:].T, cmap='viridis', aspect=\"auto\", origin='lower',  extent=extent, vmin=vmin, vmax=vmax)\n",
    "#     ax.set_title('label ch: {}'.format(-1) )\n",
    "        \n",
    "\n",
    "#     fig.subplots_adjust(wspace=0.2, hspace=0.8)\n",
    "\n",
    "\n",
    "#     if fig_name is None:\n",
    "#         fig_name = 'IO_weights'\n",
    "\n",
    "#     fig.suptitle(fig_name+'\\ni_sample:{}'.format(i_sample), fontsize=20)\n",
    "\n",
    "#     if log_wandb:\n",
    "#         wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "#     if outputdir is not None:\n",
    "#         if not os.path.exists(outputdir):\n",
    "#             os.makedirs(outputdir)\n",
    "\n",
    "#         fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "#     if show_plot == False:\n",
    "#         plt.close(fig)\n",
    "#         pyplot.close(fig)\n",
    "#         plt.close('all')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.ModuleDict()\n",
    "# model['i'] = U_Net(in_ch=1, out_ch=2, n1 = training_params['N_channels'], training_params=training_params)\n",
    "\n",
    "# U_Net(in_ch=1, out_ch=2, n1 = training_params['N_channels'], training_params=training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# for task in training_params['regressor_names']:\n",
    "#     if task!=main_task:\n",
    "#         continue\n",
    "\n",
    "#     plot_BA(df_outputlabel_val[main_task], main_task, fig_name='BA_val_{}'.format(main_task), show_plot=False, outputdir=outputdir+'model_output/', log_wandb=training_params['wandb'])\n",
    "#     plot_regression_all_agg(df_outputlabel_val[main_task], df_performance_val[main_task], outputdir=outputdir+'model_output/', show_plot=False, log_wandb=training_params['wandb'])\n",
    "\n",
    "# # log metrices on wnadb\n",
    "# if training_params['wandb']==True:\n",
    "#     # W&B\n",
    "#     label = df_outputlabel_val[main_task]['label'].values\n",
    "#     label_est = df_outputlabel_val[main_task]['label_est'].values\n",
    "# #         print(label.shape, label)\n",
    "# #         print(label_est.shape, label_est)\n",
    "\n",
    "#     PCC = get_PCC(label, label_est)\n",
    "#     Rsquared = get_CoeffDeterm(label, label_est)\n",
    "#     MAE, _ = get_MAE(label, label_est)\n",
    "#     RMSE = get_RMSE(label, label_est)\n",
    "#     MAPE, _ = get_MAPE(label, label_est)\n",
    "\n",
    "#     wandb.log(\n",
    "#         {\n",
    "#             'val_MAE': MAE,\n",
    "#             'val_RMSE': RMSE,\n",
    "#             'val_MAPE': MAPE,\n",
    "#             'val_PCC': PCC,\n",
    "#             'val_Rsquared': Rsquared,\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # performance_dict_TESTc, model_out_dict_TESTc = get_performance(model, dataloaders['valc'], training_params)\n",
    "\n",
    "# if plot_results:\n",
    "#     # model out is based on interpolated labels\n",
    "#     plot_unet_results(model_out_dict_TRAIN, training_params, title_str='TRAIN', outputdir=outputdir+'TRAIN-TEST/')\n",
    "#     plot_unet_results(model_out_dict_TEST, training_params, title_str='TEST', outputdir=outputdir+'TRAIN-TEST/')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_train, label_est_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_performance_val[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get_PCC(label_train.squeeze(), label_est_train.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print results for Reviewer's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_out_dict_TRAIN['RR_input']\n",
    "# model_out_dict_TRAIN['RR_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_out_dict_TRAIN['meta'][:,0]==subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_out_dict_TESTc['RR_model'].shape, model_out_dict_TESTc['ts'].shape\n",
    "# model_out_dict_TESTc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE_mean_model_TEST = performance_dict_TEST['MAE_mean_model']\n",
    "# MAPE_mean_model_TEST = performance_dict_TEST['MAPE_mean_model']\n",
    "# MAE_mean_input_TEST = performance_dict_TEST['MAE_mean_input']\n",
    "# MAPE_mean_input_TEST = performance_dict_TEST['MAPE_mean_input']\n",
    "# MAE_std_model_TEST = performance_dict_TEST['MAE_std_model']\n",
    "# MAPE_std_model_TEST = performance_dict_TEST['MAPE_std_model']\n",
    "# MAE_std_input_TEST = performance_dict_TEST['MAE_std_input']\n",
    "# MAPE_std_input_TEST = performance_dict_TEST['MAPE_std_input']\n",
    "\n",
    "# if plot_results:\n",
    "#     # scatter is based on cosmed datapoints\n",
    "#     plot_unet_scatter_compare(model_out_dict_TEST,  outputdir+'TRAIN-TEST/', show_plot=True)\n",
    "#     plot_unet_scatter_single(model_out_dict_TEST,  outputdir+'TRAIN-TEST/', show_plot=True)\n",
    "\n",
    "# plt.show()\n",
    "# # get_performance_model_time = time.time()\n",
    "# # time_elapsed = time.time() - start_time\n",
    "\n",
    "# # predictions_dict = {'MAE_mean_model_val': [MAE_mean_model_TEST*60],\n",
    "# #                     'MAE_std_model_val': [MAE_std_model_TEST*60],\n",
    "# #                     'MAPE_mean_model_val': [MAPE_mean_model_TEST*100],\n",
    "# #                     'MAPE_std_model_val': [MAPE_std_model_TEST*100],\n",
    "# #                     'MAE_mean_input_val': [MAE_mean_input_TEST*60],\n",
    "# #                     'MAE_std_input_val': [MAE_std_input_TEST*60],\n",
    "# #                     'MAPE_mean_input_val': [MAPE_mean_input_TEST*100],\n",
    "# #                     'MAPE_std_input_val': [MAPE_std_input_TEST*100],\n",
    "# #                     'time_elapsed': [time_elapsed]}\n",
    "\n",
    "# # df_performance_TEST = df_performance.append(pd.DataFrame.from_dict(predictions_dict), ignore_index=True)\n",
    "# # df_performance_TEST.to_csv(outputdir+'TRAIN-TEST/df_performance_TEST.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# # TEST_dict = {\n",
    "# #     'MAE input': ['{:.2f} ± {:.2f} bpm'.format(MAE_mean_input_TEST*60, MAE_std_input_TEST*60)],\n",
    "# #     'MAE model': ['{:.2f} ± {:.2f} bpm'.format(MAE_mean_model_TEST*60, MAE_std_model_TEST*60)],\n",
    "# #     'MAPE input': ['{:.2f} ± {:.2f} %'.format(MAPE_mean_input_TEST*100, MAPE_std_input_TEST*100)],\n",
    "# #     'MAPE model': ['{:.2f} ± {:.2f} %'.format(MAPE_mean_model_TEST*100, MAPE_std_model_TEST*100)]\n",
    "# # }\n",
    "# # df_pretty_performance = df_pretty_performance.append(pd.DataFrame.from_dict(TEST_dict), ignore_index=True)\n",
    "# # df_pretty_performance.to_csv(outputdir+'TRAIN-TEST/df_pretty_performance.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# # RR_label = model_out_dict_TESTc['RR_label']\n",
    "# # RR_input = model_out_dict_TESTc['RR_input']\n",
    "# # RR_model = model_out_dict_TESTc['RR_model']\n",
    "# # meta = model_out_dict_TESTc['meta']\n",
    "# # hr_TEST = dataloaders['valc'].dataset.hr\n",
    "# # ts = model_out_dict_TESTc['ts']\n",
    "\n",
    "# # i_shift = hr_TEST.shape[1]//2\n",
    "# # hr = hr_TEST[:,i_shift]\n",
    "\n",
    "\n",
    "# # eval_matrix = np.c_[RR_label, RR_input, RR_model, meta, hr, ts]\n",
    "\n",
    "# # # column_names = 'RR_label'+ 'RR_input'+ 'RR_model'+ 'meta'\n",
    "# # column_names = ['RR_label']\n",
    "\n",
    "# # for i in range(RR_input.shape[1]):\n",
    "# #     column_names.append('RR_input{}'.format(i))\n",
    "    \n",
    "# # column_names.append('RR_model')\n",
    "# # column_names.append('meta')\n",
    "# # column_names.append('hr')\n",
    "# # column_names.append('ts')\n",
    "\n",
    "\n",
    "# # df_RR_TEST = pd.DataFrame(eval_matrix, columns=column_names)\n",
    "# # df_RR_TEST.to_csv(outputdir+'TRAIN-TEST/df_RR_TEST.csv', index=False)\n",
    "\n",
    "# # print('\\nDONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(ts, hr)\n",
    "\n",
    "# plt.plot(ts, RR_label)\n",
    "# # plt.plot(ts, RR_input[:,1])\n",
    "# plt.plot(ts, RR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_label = model_out_dict_TEST['RR_label']\n",
    "RR_input = model_out_dict_TEST['RR_input']\n",
    "RR_model = model_out_dict_TEST['RR_model']\n",
    "meta = model_out_dict_TEST['meta']\n",
    "\n",
    "\n",
    "print('subject id\\tinput prediction\\t\\tmodel prediction')\n",
    "print('============================================================================')\n",
    "df_sub_performance = pd.DataFrame()\n",
    "\n",
    "print_performance = True\n",
    "\n",
    "for subject_id in np.unique(meta[:,0]):\n",
    "\n",
    "    indices_sub = np.where(meta==subject_id)[0]\n",
    "    MAE_mean_input, MAE_std_input = get_MAE(RR_label[indices_sub], RR_input[indices_sub].mean(axis=1, keepdims=True))\n",
    "    MAPE_mean_input, MAPE_std_input = get_MAPE(RR_label[indices_sub], RR_input[indices_sub].mean(axis=1, keepdims=True))\n",
    "\n",
    "    MAE_mean_model, MAE_std_model = get_MAE(RR_label[indices_sub], RR_model[indices_sub])\n",
    "    MAPE_mean_model, MAPE_std_model = get_MAPE(RR_label[indices_sub], RR_model[indices_sub])\n",
    "\n",
    "    if print_performance == True:\n",
    "\n",
    "        print('{}\\t\\tMAE: {:.2f} ± {:.2f} bpm\\t\\tMAE: {:.2f} ± {:.2f} bpm'.format(int(subject_id),MAE_mean_input*60, MAE_std_input*60, MAE_mean_model*60, MAE_std_model*60))\n",
    "\n",
    "\n",
    "    sub_dict = {\n",
    "        'subject_id': ['{}'.format(int(subject_id))],\n",
    "        'MAE input': ['{:.2f} ± {:.2f} bpm'.format(MAE_mean_input*60, MAE_std_input*60)],\n",
    "        'MAE model': ['{:.2f} ± {:.2f} bpm'.format(MAE_mean_model*60, MAE_std_model*60)],\n",
    "        'MAPE input': ['{:.2f} ± {:.2f} %'.format(MAPE_mean_input*100, MAPE_std_input*100)],\n",
    "        'MAPE model': ['{:.2f} ± {:.2f} %'.format(MAPE_mean_model*100, MAPE_std_model*100)]\n",
    "    }\n",
    "    df_sub_performance = df_sub_performance.append(pd.DataFrame.from_dict(sub_dict), ignore_index=True)\n",
    "\n",
    "df_sub_performance.to_csv(outputdir+'TRAIN-TEST/df_sub_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bland_altman_plot(m1, m2,\n",
    "#                       sd_limit=1.96,\n",
    "#                       ax=None,\n",
    "#                       scatter_kwds=None,\n",
    "#                       mean_line_kwds=None,\n",
    "#                       limit_lines_kwds=None):\n",
    "#     \"\"\"\n",
    "#     Bland-Altman Plot.\n",
    "#     A Bland-Altman plot is a graphical method to analyze the differences\n",
    "#     between two methods of measurement. The mean of the measures is plotted\n",
    "#     against their difference.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     m1, m2: pandas Series or array-like\n",
    "#     sd_limit : float, default 1.96\n",
    "#         The limit of agreements expressed in terms of the standard deviation of\n",
    "#         the differences. If `md` is the mean of the differences, and `sd` is\n",
    "#         the standard deviation of those differences, then the limits of\n",
    "#         agreement that will be plotted will be\n",
    "#                        md - sd_limit * sd, md + sd_limit * sd\n",
    "#         The default of 1.96 will produce 95% confidence intervals for the means\n",
    "#         of the differences.\n",
    "#         If sd_limit = 0, no limits will be plotted, and the ylimit of the plot\n",
    "#         defaults to 3 standard deviatons on either side of the mean.\n",
    "#     ax: matplotlib.axis, optional\n",
    "#         matplotlib axis object to plot on.\n",
    "#     scatter_kwargs: keywords\n",
    "#         Options to to style the scatter plot. Accepts any keywords for the\n",
    "#         matplotlib Axes.scatter plotting method\n",
    "#     mean_line_kwds: keywords\n",
    "#         Options to to style the scatter plot. Accepts any keywords for the\n",
    "#         matplotlib Axes.axhline plotting method\n",
    "#     limit_lines_kwds: keywords\n",
    "#         Options to to style the scatter plot. Accepts any keywords for the\n",
    "#         matplotlib Axes.axhline plotting method\n",
    "#    Returns\n",
    "#     -------\n",
    "#     ax: matplotlib Axis object\n",
    "#     \"\"\"\n",
    "\n",
    "#     if len(m1) != len(m2):\n",
    "#         raise ValueError('m1 does not have the same length as m2.')\n",
    "#     if sd_limit < 0:\n",
    "#         raise ValueError('sd_limit ({}) is less than 0.'.format(sd_limit))\n",
    "\n",
    "#     means = np.mean([m1, m2], axis=0)\n",
    "#     diffs = m1 - m2\n",
    "#     mean_diff = np.mean(diffs)\n",
    "#     std_diff = np.std(diffs, axis=0)\n",
    "\n",
    "#     if ax is None:\n",
    "#         ax = plt.gca()\n",
    "\n",
    "#     scatter_kwds = scatter_kwds or {}\n",
    "#     if 's' not in scatter_kwds:\n",
    "#         scatter_kwds['s'] = 20\n",
    "#     mean_line_kwds = mean_line_kwds or {}\n",
    "#     limit_lines_kwds = limit_lines_kwds or {}\n",
    "#     for kwds in [mean_line_kwds, limit_lines_kwds]:\n",
    "#         if 'color' not in kwds:\n",
    "#             kwds['color'] = 'gray'\n",
    "#         if 'linewidth' not in kwds:\n",
    "#             kwds['linewidth'] = 1\n",
    "#     if 'linestyle' not in mean_line_kwds:\n",
    "#         kwds['linestyle'] = '--'\n",
    "#     if 'linestyle' not in limit_lines_kwds:\n",
    "#         kwds['linestyle'] = ':'\n",
    "\n",
    "#     ax.scatter(means, diffs, **scatter_kwds)\n",
    "#     ax.axhline(mean_diff, **mean_line_kwds)  # draw mean line.\n",
    "\n",
    "#     # Annotate mean line with mean difference.\n",
    "#     ax.annotate('mean diff:\\n{}'.format(np.round(mean_diff, 2)),\n",
    "#                 xy=(0.99, 0.5),\n",
    "#                 horizontalalignment='right',\n",
    "#                 verticalalignment='center',\n",
    "#                 fontsize=14,\n",
    "#                 xycoords='axes fraction')\n",
    "\n",
    "#     if sd_limit > 0:\n",
    "#         half_ylim = (1.5 * sd_limit) * std_diff\n",
    "#         ax.set_ylim(mean_diff - half_ylim,\n",
    "#                     mean_diff + half_ylim)\n",
    "\n",
    "#         limit_of_agreement = sd_limit * std_diff\n",
    "#         lower = mean_diff - limit_of_agreement\n",
    "#         upper = mean_diff + limit_of_agreement\n",
    "#         for j, lim in enumerate([lower, upper]):\n",
    "#             ax.axhline(lim, **limit_lines_kwds)\n",
    "#         ax.annotate('-SD{}: {}'.format(sd_limit, np.round(lower, 2)),\n",
    "#                     xy=(0.99, 0.07),\n",
    "#                     horizontalalignment='right',\n",
    "#                     verticalalignment='bottom',\n",
    "#                     fontsize=14,\n",
    "#                     xycoords='axes fraction')\n",
    "#         ax.annotate('+SD{}: {}'.format(sd_limit, np.round(upper, 2)),\n",
    "#                     xy=(0.99, 0.92),\n",
    "#                     horizontalalignment='right',\n",
    "#                     fontsize=14,\n",
    "#                     xycoords='axes fraction')\n",
    "\n",
    "#     elif sd_limit == 0:\n",
    "#         half_ylim = 3 * std_diff\n",
    "#         ax.set_ylim(mean_diff - half_ylim,\n",
    "#                     mean_diff + half_ylim)\n",
    "\n",
    "#     ax.set_ylabel('Difference', fontsize=15)\n",
    "#     ax.set_xlabel('Means', fontsize=15)\n",
    "#     ax.tick_params(labelsize=13)\n",
    "#     plt.tight_layout()\n",
    "#     return ax\n",
    "\n",
    "# RR_label = model_out_dict_TEST['RR_label']*60\n",
    "# RR_model = model_out_dict_TEST['RR_model']*60\n",
    "\n",
    "# fig, ax = plt.subplots(1, figsize = (8,5), dpi=100)\n",
    "\n",
    "# bland_altman_plot(RR_label, RR_model, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD\n",
    "\n",
    "# def plot_unet_scatter_sub(model_out_dict_train, model_out_dict_val, training_params, outputdir=None, show_plot=False):\n",
    "\n",
    "#     fig = plt.figure(figsize=(21,7), dpi=100)\n",
    "\n",
    "# #     model_out_dict_train = get_model_out(model, dataloader['train_eval'], training_params)\n",
    "#     RR_label_train = model_out_dict_train['RR_label'][:,0]\n",
    "# #     RR_input_train = model_out_dict_train['RR_input']\n",
    "#     RR_model_train = model_out_dict_train['RR_model'][:,0]\n",
    "#     meta_train = model_out_dict_train['meta']\n",
    "#     N_subjects_train = np.unique(meta_train).shape[0]\n",
    "    \n",
    "# #     model_out_dict_val = get_model_out(model, dataloader['val'], training_params)\n",
    "#     RR_label_val = model_out_dict_val['RR_label'][:,0]\n",
    "#     RR_input_val = model_out_dict_val['RR_input'][:,0]\n",
    "#     RR_model_val = model_out_dict_val['RR_model'][:,0]\n",
    "#     meta_val = model_out_dict_val['meta']\n",
    "#     N_subjects_val = np.unique(meta_val).shape[0]\n",
    "    \n",
    "#     RR_arr = np.r_[RR_label_train, RR_model_train, RR_label_val, RR_model_val]*60\n",
    "# #     RR_range = [RR_arr.min(), RR_arr.max()]\n",
    "    \n",
    "#     RR_range = label_range_dict['br']\n",
    "    \n",
    "#     fontsize = 20\n",
    "#     ax1 = fig.add_subplot(1, 3, 1)\n",
    "#     colornames = list(color_dict.keys())\n",
    "#     markernames = list(marker_dict.keys())\n",
    "\n",
    "#     i = 0\n",
    "#     for subject_id in np.unique(model_out_dict_train['meta']):\n",
    "#         indices_sub = np.where(model_out_dict_train['meta']==subject_id)[0]\n",
    "#         ax1.scatter(RR_label_train[indices_sub]*60, RR_model_train[indices_sub]*60, label='sub {}'.format(int(subject_id)), color='steelblue', marker=marker_dict[markernames[i%len(markernames)]], s=80, alpha=0.3)\n",
    "#         i += 1\n",
    "# #     ax1.scatter(RR_label_train*60, RR_model_train*60, label='model RR', color='steelblue', s=80, alpha=0.3)\n",
    "\n",
    "#     ax1.plot( RR_range,RR_range , '--', color='gray', alpha=0.8)\n",
    "    \n",
    "#     # these are matplotlib.patch.Patch properties\n",
    "#     props = dict(boxstyle='round,pad=0.7', facecolor='none', edgecolor='black', alpha=0.5)\n",
    "# # bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'))\n",
    "    \n",
    "    \n",
    "#     textstr = r'$r^{2}$' + '= {:.2f}'.format(r2_score(RR_label_train, RR_model_train)) + '\\n' + \\\n",
    "#     'N = {} subejcts'.format(N_subjects_train) + '\\n' + \\\n",
    "#     'MAE = {:.2f} bpm'.format(get_MAE(RR_label_train, RR_model_train)[0]*60) + '\\n' + \\\n",
    "#     'MAPE = {:.2f} %'.format(get_MAPE(RR_label_train, RR_model_train)[0]*100)\n",
    "    \n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax1.text(0.7, 0.08, textstr, transform=ax1.transAxes, fontsize=fontsize-5,\n",
    "#     verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
    "#     ax1.legend(loc='upper right', frameon=True)\n",
    "\n",
    "#     ax1.set_title('Training RR', fontsize=fontsize*1.3)\n",
    "#     ax1.set_xlabel('RR reference (bpm)', fontsize=fontsize)\n",
    "#     ax1.set_ylabel('RR estimation (bpm)', fontsize=fontsize)\n",
    "    \n",
    "#     ax1.set_xlim(RR_range)\n",
    "#     ax1.set_ylim(RR_range)\n",
    "    \n",
    "\n",
    "\n",
    "#     ax2 = fig.add_subplot(1, 3, 2)\n",
    "#     for subject_id in np.unique(model_out_dict_val['meta']):\n",
    "#         indices_sub = np.where(model_out_dict_val['meta']==subject_id)[0]\n",
    "#         ax2.scatter(RR_label_val[indices_sub]*60, RR_model_val[indices_sub]*60, label='sub {}'.format(int(subject_id)), color='firebrick', marker=marker_dict[markernames[i%len(markernames)]], s=80, alpha=0.3)\n",
    "#         i += 1\n",
    "\n",
    "#     ax2.plot( RR_range,RR_range , '--', color='gray', alpha=0.8)\n",
    "\n",
    "#     ax2.set_title('Validation RR', fontsize=fontsize*1.3)\n",
    "#     ax2.set_xlabel('RR reference (bpm)', fontsize=fontsize)\n",
    "#     ax2.set_ylabel('RR estimation (bpm)', fontsize=fontsize)\n",
    "#     ax2.set_xlim(RR_range)\n",
    "#     ax2.set_ylim(RR_range)\n",
    "    \n",
    "    \n",
    "#     # these are matplotlib.patch.Patch properties\n",
    "#     textstr = r'$r^{2}$' + '= {:.2f}'.format(r2_score(RR_label_val, RR_model_val)) + '\\n' + \\\n",
    "#     'N = {} subejcts'.format(N_subjects_val) + '\\n' + \\\n",
    "#     'MAE = {:.2f} bpm'.format(get_MAE(RR_label_val, RR_model_val)[0]*60) + '\\n' + \\\n",
    "#     'MAPE = {:.2f} %'.format(get_MAPE(RR_label_val, RR_model_val)[0]*100)\n",
    "    \n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax2.text(0.7, 0.08, textstr, transform=ax2.transAxes, fontsize=fontsize-5,\n",
    "#     verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
    "#     ax2.legend(loc='upper right', frameon=True)\n",
    "\n",
    "    \n",
    "    \n",
    "#     ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "#     i -= np.unique(model_out_dict_val['meta']).shape[0]\n",
    "#     for subject_id in np.unique(model_out_dict_val['meta']):\n",
    "#         indices_sub = np.where(model_out_dict_val['meta']==subject_id)[0]\n",
    "#         ax3.scatter(RR_label_val[indices_sub]*60, RR_input_val[indices_sub]*60, label='sub {}'.format(int(subject_id)), color='seagreen', marker=marker_dict[markernames[i%len(markernames)]], s=80, alpha=0.3)\n",
    "#         i += 1\n",
    "    \n",
    "\n",
    "#     ax3.plot( RR_range,RR_range , '--', color='gray', alpha=0.8)\n",
    "\n",
    "#     ax3.set_title('Validation RR (DSP)', fontsize=fontsize*1.3)\n",
    "#     ax3.set_xlabel('RR reference (bpm)', fontsize=fontsize)\n",
    "#     ax3.set_ylabel('RR estimation (bpm)', fontsize=fontsize)\n",
    "#     ax3.set_xlim(RR_range)\n",
    "#     ax3.set_ylim(RR_range)\n",
    "#     ax3.legend(loc='upper right', frameon=True)\n",
    "    \n",
    "\n",
    "#     # these are matplotlib.patch.Patch properties\n",
    "#     textstr = r'$r^{2}$' + '= {:.2f}'.format(r2_score(RR_label_val, RR_input_val)) + '\\n' + \\\n",
    "#     'N = {} subejcts'.format(N_subjects_val) + '\\n' + \\\n",
    "#     'MAE = {:.2f} bpm'.format(get_MAE(RR_label_val, RR_input_val)[0]*60) + '\\n' + \\\n",
    "#     'MAPE = {:.2f} %'.format(get_MAPE(RR_label_val, RR_input_val)[0]*100)\n",
    "    \n",
    "#     # place a text box in upper left in axes coords\n",
    "#     ax3.text(0.7, 0.08, textstr, transform=ax3.transAxes, fontsize=fontsize-5,\n",
    "#     verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
    "    \n",
    "    \n",
    "#     fig.tight_layout()\n",
    "    \n",
    "#     if outputdir is not None:\n",
    "#         if not os.path.exists(outputdir):\n",
    "#             os.makedirs(outputdir)\n",
    "#         fig.savefig(outputdir + 'model_scatter.png', facecolor=fig.get_facecolor())\n",
    "#     if show_plot == False:\n",
    "#         plt.close(fig)\n",
    "#         pyplot.close(fig)\n",
    "#         plt.close('all')\n",
    "\n",
    "        \n",
    "# # plot_unet_scatter_sub(model_out_dict_TRAIN, model_out_dict_TEST, training_params, outputdir+'TRAIN-TEST/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "mienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
